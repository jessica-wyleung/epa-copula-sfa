{
 "cells": [
  {
   "cell_type": "raw",
   "id": "34512262",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  keep-ipynb: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a19514",
   "metadata": {},
   "source": [
    "# Chapter 2: Basics of SFA for cross-sectional data\n",
    "\n",
    "## Exercise 2.1 Data Import\n",
    "\n",
    "This exercise demonstrates importing the data file `cowing.xlsx` in Python using the Pandas `read_excel` function. After importing, we display the first two lines and the last line of the data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72af8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "data = pd.read_excel('cowing.xlsx', index_col=[0]) # use the first column of the excel file as the index\n",
    "data.columns = ['lnY','lnK','lnF', 'lnL', 'P_K', 'P_F', 'P_L']\n",
    "data.index.name = 'Utility' # The index column is the utility firm\n",
    "display(data.head(2))\n",
    "display(data.tail(1))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14429366",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Plotting a two-input production function surface\n",
    "\n",
    "In this exercise, we plot the surface for a two-input Cobb-Douglas production function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def CobbDouglas(beta, X, Y):\n",
    "    return 5 * X**beta * Y ** (1 - beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72633afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 3, 1000)\n",
    "y = np.linspace(0, 3, 1000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "output = CobbDouglas(beta=0.5, X=X, Y=Y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={\"projection\": \"3d\"})\n",
    "surf = ax.plot_surface(X, Y, output, cmap=cm.jet)\n",
    "ax.contour(X, Y, output, 8, cmap=cm.jet, linestyles=\"solid\", offset=-1)\n",
    "ax.contour(X, Y, output, 8, colors=\"k\", linestyles=\"solid\")\n",
    "ax.set_xlabel(\"Input 1\", fontsize=16)\n",
    "ax.set_ylabel(\"Input 2\", fontsize=16)\n",
    "ax.set_zlabel(\"Output\", fontsize=16)\n",
    "ax.view_init(10, -35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600524b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb659e94",
   "metadata": {},
   "source": [
    "## Exercise 2.3: Plotting a bivariate Gaussian Copula\n",
    "\n",
    "In this exercise, we plot the bivariate Gaussian copula density and distribution function with $\\rho = 0.5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be48e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = np.linspace(0.01, 0.99, 50)\n",
    "u2 = np.linspace(0.01, 0.99, 50)\n",
    "U1, U2 = np.meshgrid(u1, u2)\n",
    "U = np.concatenate([U1.flatten().reshape(-1, 1), U2.flatten().reshape(-1, 1)], axis=1)\n",
    "\n",
    "copula_pdf = stats.multivariate_normal.pdf(\n",
    "    x=stats.norm.ppf(U), mean=np.zeros(2), cov=np.array([[1, 0.5], [0.5, 1]])\n",
    ") / np.prod(stats.norm.pdf(stats.norm.ppf(U)), axis=1)\n",
    "copula_cdf = stats.multivariate_normal.cdf(\n",
    "    stats.norm.ppf(U), mean=np.zeros(2), cov=np.array([[1, 0.5], [0.5, 1]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={\"projection\": \"3d\"})\n",
    "surf = ax.plot_surface(U1, U2, copula_pdf.reshape(50, 50), cmap=cm.jet)\n",
    "ax.view_init(10, 10)\n",
    "ax.set_xlabel(r\"$w_{1}$\", fontsize=16)\n",
    "ax.set_ylabel(r\"$w_{2}$\", fontsize=16)\n",
    "ax.set_zlabel(r\"$c(w_{1}, w_{2}, \\rho)$\", fontsize=16)\n",
    "ax.set_title(r\"Gaussian copula density\", fontsize = 16)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={\"projection\": \"3d\"})\n",
    "surf = ax.plot_surface(U1, U2, copula_cdf.reshape(50, 50), cmap=cm.jet)\n",
    "ax.set_xlabel(r\"$w_{1}$\", fontsize=16)\n",
    "ax.set_ylabel(r\"$w_{2}$\", fontsize=16)\n",
    "ax.set_zlabel(r\"$c(w_{1}, w_{2}, \\rho)$\", fontsize=16)\n",
    "ax.set_title(r\"Gaussian copula function\", fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d3142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29db2b8",
   "metadata": {},
   "source": [
    "## Exercise 2.4: Corrected Ordinary Least Squared (COLS) for deterministic frontier of the U.S. utilities data\n",
    "\n",
    "In this exercise we implement a Corrected OLS (COLS) using a Cobb-Douglas production function.\n",
    "\n",
    "The coefficient estimates suggest an increasing return to scale in electricity generation. The sign on labour ($\\beta_{3}$) is surprising, but that coefficient is statistically insignificant. \n",
    "\n",
    "In order to obtain estimates of the inefficiencies, we generate the residuals and subtract their maximum from them. The result with a minus sign can be viewed as estimates of inefficiency $u_i$. Alternatively, if we calculate $\\exp(-u_i)$, this will have the interpretation of production efficiency estimates, i.e. the percentage of potential electricity output actually produced by utility $i$. Figure plots the histogram of technical efficiency estimates from the COLS model. The distribution appears to be right skewed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef014f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a0dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "production_data = pd.read_excel(\"cowing.xlsx\")  # Inputs and outputs are in Logarithms\n",
    "\n",
    "# Fit OLS model\n",
    "colsd = sm.ols(\n",
    "    formula=\"y ~ X1 + X2 + X3\", data=production_data[[\"y\", \"X1\", \"X2\", \"X3\"]]\n",
    ")\n",
    "colsd_fitted = colsd.fit()\n",
    "print(colsd_fitted.summary())  \n",
    "\n",
    "resids = colsd_fitted.resid  \n",
    "u_star = -(resids - np.max(resids))  \n",
    "eff_colsd = np.exp(-u_star)\n",
    "\n",
    "# Plot a histogram of the technical efficiency estimates\n",
    "figure = plt.figure(figsize=(10, 8))\n",
    "plt.hist(eff_colsd, 10, edgecolor=\"k\")\n",
    "plt.xlabel(\"$\\exp(-\\hat{u})$\", fontsize=16)\n",
    "plt.ylabel(\"Frequency\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a64ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355be692",
   "metadata": {},
   "source": [
    "## Exercise 2.5: MLE for the US utilities data\n",
    "\n",
    "In thi exercise, we perform Maximum Likelihood Estimation (MLE) for a Cobb-Douglas production function using a sample of 111 steam-electric power plants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def log_density(coefs, y, x1, x2, x3):\n",
    "\n",
    "    [alpha, beta1, beta2, beta3, \n",
    "     sigma2u, sigma2v] = coefs[:]\n",
    "\n",
    "    Lambda = np.sqrt(sigma2u / sigma2v)\n",
    "    sigma2 = sigma2u + sigma2v\n",
    "    sigma = np.sqrt(sigma2)\n",
    "\n",
    "    # Composed errors from the production function equation\n",
    "    eps = y - alpha - x1 * beta1 - x2 * beta2 - x3 * beta3\n",
    "\n",
    "    # Compute the log density\n",
    "    Den = (\n",
    "        (2 / sigma)\n",
    "        * stats.norm.pdf(eps / sigma)\n",
    "        * stats.norm.cdf(-Lambda * eps / sigma)\n",
    "    )\n",
    "    logDen = np.log(Den)\n",
    "\n",
    "    return logDen\n",
    "\n",
    "\n",
    "def loglikelihood(coefs, y, x1, x2, x3):\n",
    "\n",
    "    logDen = log_density(coefs, y, x1, x2, x3)\n",
    "    log_likelihood = -np.sum(logDen)\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def estimate(y, x1, x2, x3):\n",
    "\n",
    "    # Starting values for MLE\n",
    "    alpha = -11\n",
    "    beta1 = 0.03\n",
    "    beta2 = 1.1\n",
    "    beta3 = -0.01\n",
    "    sigma2u = 0.01\n",
    "    sigma2v = 0.0003\n",
    "\n",
    "    theta0 = np.array([alpha, beta1, beta2, beta3, \n",
    "                       sigma2u, sigma2v])\n",
    "\n",
    "    bounds = [(None, None) for x in range(len(theta0) - 2)] + [\n",
    "        (1e-6, np.inf),\n",
    "        (1e-6, np.inf),\n",
    "    ]\n",
    "\n",
    "    # Minimize the negative log-likelihood using numerical optimization.\n",
    "    MLE_results = minimize(\n",
    "        fun=loglikelihood,\n",
    "        x0=theta0,\n",
    "        method=\"L-BFGS-B\",\n",
    "        tol = 1e-6,\n",
    "        options={\"ftol\": 1e-6, \"maxiter\": 1000, \"maxfun\": 6*1000},\n",
    "        args=(y, x1, x2, x3),\n",
    "        bounds=bounds,\n",
    "    )\n",
    "\n",
    "    theta = MLE_results.x  \n",
    "    log_likelihood = MLE_results.fun  \n",
    "\n",
    "    # Estimate standard errors\n",
    "    delta = 1e-6\n",
    "    grad = np.zeros((len(y), len(theta)))\n",
    "    for i in range(len(theta)):\n",
    "        theta1 = np.copy(theta)\n",
    "        theta1[i] += delta\n",
    "        grad[:, i] = (\n",
    "            log_density(theta1, y, x1, x2, x3) - log_density(theta, y, x1, x2, x3)\n",
    "        ) / delta\n",
    "\n",
    "    OPG = grad.T @ grad  \n",
    "    ster = np.sqrt(np.diag(np.linalg.inv(OPG)))\n",
    "\n",
    "    return theta, ster, log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "production_data = pd.read_excel(\"cowing.xlsx\")  # Inputs and outputs are in Logarithms\n",
    "y = production_data[\"y\"]  # Output\n",
    "[x1, x2, x3] = [\n",
    "    production_data[\"X1\"],\n",
    "    production_data[\"X2\"],\n",
    "    production_data[\"X3\"],\n",
    "]  \n",
    "\n",
    "coefs, sterr, logMLE = estimate(y, x1, x2, x3)\n",
    "\n",
    "Zscores = coefs / sterr\n",
    "Pvalues = 2 * (1 - stats.norm.cdf(np.abs(Zscores)))\n",
    "lower_ConfInt95 = stats.norm.ppf(0.025, loc=coefs, scale=sterr)\n",
    "upper_ConfInt95 = stats.norm.ppf(0.975, loc=coefs, scale=sterr)\n",
    "\n",
    "# Display the results as a table\n",
    "results = pd.DataFrame(\n",
    "    data={\n",
    "        \"Est\": coefs,\n",
    "        \"StEr\": sterr,\n",
    "        \"z-stat\": Zscores,\n",
    "        \"p-val\": Pvalues,\n",
    "        \"Lower 95% Conf Int\": lower_ConfInt95,\n",
    "        \"Upper 95% Conf Int\": upper_ConfInt95,\n",
    "    },\n",
    "    index=[\n",
    "        r\"$$\\beta_{0}$$\",\n",
    "        r\"$$\\beta_{1}$$\",\n",
    "        r\"$$\\beta_{2}$$\",\n",
    "        r\"$$\\beta_{3}$$\",\n",
    "        r\"$$\\sigma^{2}_{u}$$\",\n",
    "        r\"$$\\sigma^{2}_{v}$$\",\n",
    "    ],\n",
    ")\n",
    "results = results.round(4)\n",
    "display(results)\n",
    "print(f\"Log-likelihood: {round(logMLE, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca88d74",
   "metadata": {},
   "source": [
    "### Prediction of Technical Inefficiencies, $\\hat{u}_{i}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4cc744",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    estimated_alpha,\n",
    "    estimated_beta1,\n",
    "    estimated_beta2,\n",
    "    estimated_beta3,\n",
    "    estimated_sigma2u,\n",
    "    estimated_sigma2v,\n",
    "] = coefs[:6]\n",
    "\n",
    "# Composed errors from the production function equation\n",
    "eps = (\n",
    "    y\n",
    "    - estimated_alpha\n",
    "    - x1 * estimated_beta1\n",
    "    - x2 * estimated_beta2\n",
    "    - x3 * estimated_beta3\n",
    ")\n",
    "\n",
    "sigma = np.sqrt(estimated_sigma2u + estimated_sigma2v)\n",
    "lam = np.sqrt(coefs[4] / estimated_sigma2v)\n",
    "\n",
    "bi = (eps * lam) / sigma\n",
    "hazard = stats.norm.pdf(bi) / (1 - stats.norm.cdf(bi))\n",
    "sigstar = np.sqrt(estimated_sigma2u * estimated_sigma2v) / sigma\n",
    "\n",
    "Eu = sigstar * (hazard - bi)  # Technical inefficiency estimates\n",
    "Vu = (sigstar**2) * (\n",
    "    1 + bi * hazard - hazard * hazard\n",
    ")  # Variance of technical inefficiency\n",
    "\n",
    "figure = plt.figure(figsize=(10, 8))\n",
    "plt.hist(Eu, edgecolor=\"k\", bins=10)\n",
    "plt.xlabel(\"$\\hat{u}$\", fontsize=16)\n",
    "plt.ylabel(\"Frequency\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8dd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5f3c0",
   "metadata": {},
   "source": [
    "## Exercise 2.6: MLE for models with heteroskedasticity\n",
    "\n",
    "In this exercise we perform Maximum Likelihood Estimation for a Cobb-Douglas production function with heteroskedasticity (non-constant variance) in $u$, that is $\\sigma^{2}_{ui}=\\exp(\\mathbf{z}_{i}'\\omega_u)$, for 196 dairy farms. The output is the log amount of milk production and the inputs are labor hours (`llabor`), feed (`lfeed`), the number of cows (`lcattle`) and the land size of the farm (`lland`) all in logarithms. The environmental variable comp is IT expenditure as a percentage of total expenditure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fe77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def log_density(coefs, y, x1, x2, x3, x4, z1):\n",
    "    # Get parameters\n",
    "    [alpha, beta1, beta2, beta3, beta4, \n",
    "     omega0, omega1, sigma2v] = coefs[:]\n",
    "\n",
    "    sigma2u = np.exp(omega0 + omega1 * z1)  # Heteroskedastic sigma2u\n",
    "    Lambda = np.sqrt(sigma2u / sigma2v)\n",
    "    sigma2 = sigma2u + sigma2v\n",
    "    sigma = np.sqrt(sigma2)\n",
    "\n",
    "    # Composed errors from the production function equation\n",
    "    eps = y - alpha - x1 * beta1 - x2 * beta2 - x3 * beta3 - x4 * beta4\n",
    "\n",
    "    # Compute log-density\n",
    "    Den = (\n",
    "        2 / sigma * stats.norm.pdf(eps / sigma) * stats.norm.cdf(-Lambda * eps / sigma)\n",
    "    )\n",
    "    Den = np.where(\n",
    "        np.abs(Den) < 1e-5, 1e-5, Den\n",
    "    )  # Adjust small densities for numerical precision\n",
    "    logDen = np.log(Den)\n",
    "\n",
    "    return logDen\n",
    "\n",
    "\n",
    "def loglikelihood(coefs, y, x1, x2, x3, x4, z1):\n",
    "    # Obtain the log likelihood\n",
    "    logDen = log_density(coefs, y, x1, x2, x3, x4, z1)\n",
    "    log_likelihood = -np.sum(logDen)\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def estimate(y, x1, x2, x3, x4, z1):\n",
    "    \"\"\"\n",
    "    Estimate model parameters by maximizing the log-likelihood function and compute standard errors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Starting values for MLE\n",
    "    alpha = 7.7\n",
    "    beta1 = 0.1\n",
    "    beta2 = 0.15\n",
    "    beta3 = 0.75\n",
    "    beta4 = 0.03\n",
    "    omega0 = -3\n",
    "    omega1 = -0.015  # Coefficient for z1\n",
    "    sigma2v = 0.005\n",
    "\n",
    "    # Initial parameter vector\n",
    "    theta0 = np.array([alpha, beta1, beta2, beta3, beta4, \n",
    "                       omega0, omega1, sigma2v])\n",
    "\n",
    "    bounds = [(None, None) for x in range(len(theta0) - 1)] + [(1e-6, np.inf)]\n",
    "\n",
    "    # Minimize the negative log-likelihood using numerical optimization.\n",
    "    MLE_results = minimize(\n",
    "        fun=loglikelihood,\n",
    "        x0=theta0,\n",
    "        method=\"L-BFGS-B\",\n",
    "        tol = 1e-6,\n",
    "        options={\"ftol\": 1e-6, \"maxiter\": 1000, \"maxfun\": 6*1000, \"maxcor\": 500},\n",
    "        args=(y, x1, x2, x3, x4, z1)\n",
    "    )\n",
    "\n",
    "    theta = MLE_results.x \n",
    "    log_likelihood = MLE_results.fun * -1\n",
    "\n",
    "    # Estimate standard errors\n",
    "    delta = 1e-6\n",
    "    grad = np.zeros((len(y), len(theta)))\n",
    "    for i in range(len(theta)):\n",
    "        theta1 = np.copy(theta)\n",
    "        theta1[i] += delta\n",
    "        grad[:, i] = (\n",
    "            log_density(theta1, y, x1, x2, x3, x4, z1)\n",
    "            - log_density(theta, y, x1, x2, x3, x4, z1)\n",
    "        ) / delta\n",
    "\n",
    "    OPG = grad.T @ grad  # Outer product of gradient\n",
    "    ster = np.sqrt(np.diag(np.linalg.inv(OPG)))\n",
    "\n",
    "    return theta, ster, log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ff6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "production_data = pd.read_csv(\"dairy.csv\")  # Inputs and outputs are in Logarithms\n",
    "y = production_data[\"ly\"].values\n",
    "[x1, x2, x3, x4] = [\n",
    "    production_data[\"llabor\"].values,\n",
    "    production_data[\"lfeed\"].values,\n",
    "    production_data[\"lcattle\"].values,\n",
    "    production_data[\"lland\"].values,\n",
    "]\n",
    "# Environmental variable\n",
    "z1 = production_data[\"comp\"]\n",
    "\n",
    "# Estimate coefficients via MLE and standard errors for SFA model\n",
    "coefs, sterr, logMLE = estimate(y, x1, x2, x3, x4, z1)\n",
    "\n",
    "Zscores = coefs / sterr\n",
    "Pvalues = 2 * (1 - stats.norm.cdf(np.abs(Zscores)))\n",
    "lower_ConfInt95 = stats.norm.ppf(0.025, loc=coefs, scale=sterr)\n",
    "upper_ConfInt95 = stats.norm.ppf(0.975, loc=coefs, scale=sterr)\n",
    "\n",
    "# Display the results as a table\n",
    "results = pd.DataFrame(\n",
    "    columns=[\"Est\", \"StEr\", \"z-stat\", \"p-val\", \"[95%Conf.\", \"Interv]\"],\n",
    "    index=[\n",
    "        r\"$$\\alpha$$\",\n",
    "        \"llabor\",\n",
    "        \"lfeed\",\n",
    "        \"lcattle\",\n",
    "        \"lland\",\n",
    "        \" \",\n",
    "        r\"$$\\omega_{0}$$\",\n",
    "        \"comp\",\n",
    "        r\"$$\\sigma^{2}_{v}$$\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "coefs = np.round(coefs.reshape(-1, 1), 3)\n",
    "sterr = np.round(sterr.reshape(-1, 1), 3)\n",
    "Zscores = np.round(Zscores.reshape(-1, 1), 3)\n",
    "Pvalues = np.round(Pvalues.reshape(-1, 1), 3)\n",
    "lower_ConfInt95 = np.round(lower_ConfInt95.reshape(-1, 1), 3)\n",
    "upper_ConfInt95 = np.round(upper_ConfInt95.reshape(-1, 1), 3)\n",
    "\n",
    "# Get the production frontier parameter estimates, standard errors and confidence intervals\n",
    "frontier = np.concatenate(\n",
    "    [\n",
    "        coefs[:5],\n",
    "        sterr[:5],\n",
    "        Zscores[:5],\n",
    "        Pvalues[:5],\n",
    "        lower_ConfInt95[:5],\n",
    "        upper_ConfInt95[:5],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Get the parameters for the heteroskedastic technical inefficiency parameterization\n",
    "Omegas = np.concatenate(\n",
    "    [\n",
    "        coefs[5:7],\n",
    "        sterr[5:7],\n",
    "        Zscores[5:7],\n",
    "        Pvalues[5:7],\n",
    "        lower_ConfInt95[5:7],\n",
    "        upper_ConfInt95[5:7],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "sigmas = np.array(\n",
    "    [coefs[7], sterr[7], Zscores[7], Pvalues[7], lower_ConfInt95[7], upper_ConfInt95[7]]\n",
    ").T\n",
    "\n",
    "print(\"\\nLog-Likelihood\", round(logMLE, 3))\n",
    "results.iloc[0:5, :] = frontier\n",
    "results.iloc[5, :] = np.full(shape=(1, 6), fill_value=\" \")\n",
    "results.iloc[6:8, :] = Omegas\n",
    "results.iloc[8:, :] = sigmas\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db0299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d8c197",
   "metadata": {},
   "source": [
    "## Exercise 2.7: MLE of truncated normal distribution with $\\mu \\neq 0$\n",
    "\n",
    "This exercise estimates a Cobb-Douglas production function with a truncated normal distribution for technical inefficiency with non-zero (constant) mean ($\\mu \\neq 0)$ via maximum likelihood using the the dairy farm production data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numdifftools as nd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def log_density(coefs, y, x1, x2, x3, x4):\n",
    "    # Get parameters\n",
    "    [alpha, beta1, beta2, beta3, beta4, \n",
    "     mu, sigma2u, sigma2v] = coefs[:]\n",
    "\n",
    "    # Composed errors from the production function equation\n",
    "    eps = y - alpha - x1 * beta1 - x2 * beta2 - x3 * beta3 - x4 * beta4\n",
    "    sigma2 = sigma2u + sigma2v\n",
    "    mu_star = (sigma2v * mu - sigma2u * eps) / sigma2\n",
    "    sigma2_star = (sigma2u * sigma2v) / sigma2\n",
    "\n",
    "    Den = stats.norm.pdf((eps + mu) / np.sqrt(sigma2)) / (\n",
    "        np.sqrt(sigma2)\n",
    "        * (\n",
    "            stats.norm.cdf(mu / np.sqrt(sigma2u))\n",
    "            / stats.norm.cdf(mu_star / np.sqrt(sigma2_star))\n",
    "        )\n",
    "    )\n",
    "    Den = np.where(\n",
    "        np.abs(Den) < 1e-5, 1e-5, Den\n",
    "    )  # Adjust small densities for numerical precision\n",
    "    logDen = np.log(Den)\n",
    "\n",
    "    return logDen\n",
    "\n",
    "\n",
    "def loglikelihood(coefs, y, x1, x2, x3, x4):\n",
    "    # Obtain the log likelihood\n",
    "    logDen = log_density(coefs, y, x1, x2, x3, x4)\n",
    "    log_likelihood = -np.sum(logDen)\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def estimate(y, x1, x2, x3, x4):\n",
    "    \"\"\"\n",
    "    Estimate model parameters by maximizing the log-likelihood function and compute standard errors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Starting values for MLE\n",
    "    alpha = 7.7\n",
    "    beta1 = 0.1\n",
    "    beta2 = 0.15\n",
    "    beta3 = 0.7\n",
    "    beta4 = 0.03\n",
    "    sigma2u = 0.17\n",
    "    sigma2v = 0.005\n",
    "    mu = -1.2\n",
    "\n",
    "    # Initial parameter vector\n",
    "    theta0 = np.array([alpha, beta1, beta2, beta3, beta4, \n",
    "                       mu, sigma2u, sigma2v])\n",
    "\n",
    "    # Bounds to ensure sigma2v and sigma2u are positive\n",
    "    bounds = [(None, None) for x in range(len(theta0) - 2)] + [\n",
    "        (1e-6, np.inf),\n",
    "        (1e-6, np.inf),\n",
    "    ]\n",
    "\n",
    "    # Minimize the negative log-likelihood using numerical optimization.\n",
    "    MLE_results = minimize(\n",
    "        fun=loglikelihood,\n",
    "        x0=theta0,\n",
    "        method=\"L-BFGS-B\",\n",
    "        tol = 1e-6,\n",
    "        options={\"ftol\": 1e-6, \"maxiter\": 1000, \"maxfun\": 6*1000, \"maxcor\": 500},\n",
    "        args=(y, x1, x2, x3, x4),\n",
    "        bounds=bounds,\n",
    "    )\n",
    "\n",
    "    theta = MLE_results.x  # Estimated parameter vector\n",
    "    logMLE = MLE_results.fun * -1  # Log-likelihood at the solution\n",
    "\n",
    "    # Estimate standard errors\n",
    "    # Inverse of the Hessian approach\n",
    "    hessian = nd.Hessian(f=loglikelihood, method=\"central\", step=1e-5)\n",
    "    ster = np.sqrt(np.diag(np.linalg.inv(hessian(theta, y, x1, x2, x3, x4))))\n",
    "\n",
    "    return theta, ster, logMLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "production_data = pd.read_csv(\"dairy.csv\")  # Inputs and outputs are in Logarithms\n",
    "y = production_data[\"ly\"].values\n",
    "[x1, x2, x3, x4] = [\n",
    "    production_data[\"llabor\"].values,\n",
    "    production_data[\"lfeed\"].values,\n",
    "    production_data[\"lcattle\"].values,\n",
    "    production_data[\"lland\"].values,\n",
    "]\n",
    "\n",
    "# Estimate coefficients via MLE and standard errors for SFA model\n",
    "coefs, sterr, logMLE = estimate(y, x1, x2, x3, x4)\n",
    "\n",
    "Zscores = coefs / sterr\n",
    "Pvalues = 2 * (1 - stats.norm.cdf(np.abs(Zscores)))\n",
    "lower_ConfInt95 = stats.norm.ppf(0.025, loc=coefs, scale=sterr)\n",
    "upper_ConfInt95 = stats.norm.ppf(0.975, loc=coefs, scale=sterr)\n",
    "\n",
    "# Display the results as a table\n",
    "results = pd.DataFrame(\n",
    "    columns=[\"Est\", \"StEr\", \"z-stat\", \"p-val\", \"[95%Conf.\", \"Interv]\"],\n",
    "    index=[\n",
    "        r\"$$\\alpha$$\",\n",
    "        \"llabor\",\n",
    "        \"lfeed\",\n",
    "        \"lcattle\",\n",
    "        \"lland\",\n",
    "        \" \",\n",
    "        r\"$$\\mu$$\",\n",
    "        r\"$$\\sigma^{2}_{u}$$\",\n",
    "        r\"$$\\sigma^{2}_{v}$$\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "coefs = np.round(coefs.reshape(-1, 1), 3)\n",
    "sterr = np.round(sterr.reshape(-1, 1), 3)\n",
    "Zscores = np.round(Zscores.reshape(-1, 1), 3)\n",
    "Pvalues = np.round(Pvalues.reshape(-1, 1), 3)\n",
    "lower_ConfInt95 = np.round(lower_ConfInt95.reshape(-1, 1), 3)\n",
    "upper_ConfInt95 = np.round(upper_ConfInt95.reshape(-1, 1), 3)\n",
    "\n",
    "# Get production function parameter estimates\n",
    "frontier = np.concatenate(\n",
    "    [\n",
    "        coefs[:5],\n",
    "        sterr[:5],\n",
    "        Zscores[:5],\n",
    "        Pvalues[:5],\n",
    "        lower_ConfInt95[:5],\n",
    "        upper_ConfInt95[:5],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Get sigma2u and sigma2v parameters\n",
    "mu = np.array(\n",
    "    [coefs[5], sterr[5], Zscores[5], Pvalues[5], lower_ConfInt95[5], upper_ConfInt95[5]]\n",
    ").T\n",
    "sigmas = np.concatenate(\n",
    "    [\n",
    "        coefs[6:],\n",
    "        sterr[6:],\n",
    "        Zscores[6:],\n",
    "        Pvalues[6:],\n",
    "        lower_ConfInt95[6:],\n",
    "        upper_ConfInt95[6:],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(\"\\nLog-Likelihood\", round(logMLE, 3))\n",
    "results.iloc[0:5, :] = frontier\n",
    "results.iloc[5, :] = np.full(shape=(1, 6), fill_value=\" \")\n",
    "results.iloc[6, :] = mu\n",
    "results.iloc[7:, :] = sigmas\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f5c704",
   "metadata": {},
   "source": [
    "## Exercise 2.8: MLE of truncated normal with $\\mu_{i} = z_{i}^{\\prime}\\delta$ and $\\sigma^{2}_{ui} = e^{({z_{i}^{\\prime}\\omega})}$\n",
    "\n",
    "This exercise estimates a Cobb-Douglas production function using a truncated normal distribution for technical inefficiency via maximum likelihood for 196 dairy farms. Marginal effects are also computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def log_density(coefs, y, x1, x2, x3, x4, z1):\n",
    "    \n",
    "    # get parameters\n",
    "    [alpha,beta1,beta2,beta3,beta4,\n",
    "     gamma0,gamma1,omega0,omega1,sigma2v,] = coefs[:10]\n",
    "\n",
    "    mu = gamma0 + gamma1 * z1\n",
    "    sigma2u = np.exp(omega0 + omega1 * z1)\n",
    "    # Composed errors from the production function equation\n",
    "    eps = y - alpha - x1 * beta1 - x2 * beta2 - x3 * beta3 - x4 * beta4\n",
    "    sigma2 = sigma2u + sigma2v\n",
    "    mu_star = (sigma2v * mu - sigma2u * eps) / sigma2\n",
    "    sigma2_star = (sigma2u * sigma2v) / sigma2\n",
    "\n",
    "    Den = stats.norm.pdf((eps + mu) / np.sqrt(sigma2)) / (\n",
    "        np.sqrt(sigma2)\n",
    "        * (\n",
    "            stats.norm.cdf(mu / np.sqrt(sigma2u))\n",
    "            / stats.norm.cdf(mu_star / np.sqrt(sigma2_star))\n",
    "        )\n",
    "    )\n",
    "    Den = np.where(\n",
    "        np.abs(Den) < 1e-6, 1e-6, Den\n",
    "    )  # Adjust small densities for numerical precision\n",
    "    logDen = np.log(Den)\n",
    "\n",
    "    return logDen\n",
    "\n",
    "\n",
    "def loglikelihood(coefs, y, x1, x2, x3, x4, z1):\n",
    "    # Obtain the log likelihood\n",
    "    logDen = log_density(coefs, y, x1, x2, x3, x4, z1)\n",
    "    log_likelihood = -np.sum(logDen)\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def estimate(y, x1, x2, x3, x4, z1):\n",
    "    \"\"\"\n",
    "    Estimate model parameters by maximizing the log-likelihood function and compute standard errors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Starting values for MLE\n",
    "    alpha = 7.5\n",
    "    beta1 = 0.1\n",
    "    beta2 = 0.15\n",
    "    beta3 = 0.7\n",
    "    beta4 = 0.03\n",
    "    gamma0 = 7\n",
    "    gamma1 = -1.5  # Coefficient for z1 in non-constant mu equation\n",
    "    omega0 = -3\n",
    "    omega1 = 0.3  # Coefficient for z1 in non-constant variance equation\n",
    "    sigma2v = 0.008\n",
    "\n",
    "    # Initial parameter vector\n",
    "    theta0 = np.array(\n",
    "        [alpha, beta1, beta2, beta3, beta4, gamma0, gamma1, omega0, omega1, sigma2v]\n",
    "    )\n",
    "\n",
    "    # Bounds to ensure sigma2v and sigma2u are positive\n",
    "    bounds = [(None, None) for x in range(len(theta0) - 1)] + [(1e-6, np.inf)]\n",
    "\n",
    "    MLE_results = minimize(\n",
    "        loglikelihood,\n",
    "        theta0,\n",
    "        method=\"L-BFGS-B\",\n",
    "        tol = 1e-6,\n",
    "        options={\"ftol\": 1e-6, \"maxiter\": 1000, \"maxfun\": 6*1000, \"maxcor\": 500},\n",
    "        args=(y, x1, x2, x3, x4, z1),\n",
    "        bounds=bounds,\n",
    "    )\n",
    "\n",
    "    theta = MLE_results.x\n",
    "    logMLE = MLE_results.fun * -1\n",
    "\n",
    "    # Standard errors\n",
    "    # Inverse of the Hessian approach\n",
    "    hessian = nd.Hessian(f=loglikelihood, method=\"central\", step=1e-6)\n",
    "    ster = np.sqrt(np.diag(np.linalg.inv(hessian(theta, y, x1, x2, x3, x4, z1))))\n",
    "    return theta, ster, logMLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f948578",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_data = pd.read_csv(\"dairy.csv\")  # Inputs and outputs are in Logarithms\n",
    "# output\n",
    "y = production_data[\"ly\"]\n",
    "# Inputs\n",
    "x1 = production_data[\"llabor\"]\n",
    "x2 = production_data[\"lfeed\"]\n",
    "x3 = production_data[\"lcattle\"]\n",
    "x4 = production_data[\"lland\"]\n",
    "# Environmental variable\n",
    "z1 = production_data[\"comp\"]\n",
    "\n",
    "# Estimate coefficients via MLE and standard errors for SFA model\n",
    "coefs, sterr, logMLE = estimate(y, x1, x2, x3, x4, z1)\n",
    "Zscores = coefs / sterr\n",
    "Pvalues = 2 * (1 - stats.norm.cdf(np.abs(Zscores)))\n",
    "lower_ConfInt95 = stats.norm.ppf(0.025, loc=coefs, scale=sterr)\n",
    "upper_ConfInt95 = stats.norm.ppf(0.975, loc=coefs, scale=sterr)\n",
    "\n",
    "# Display the results as a table\n",
    "results = pd.DataFrame(\n",
    "    columns=[\"Est\", \"StEr\", \"z-stat\", \"p-val\", \"[95%Conf.\", \"Interv]\"],\n",
    "    index=[\n",
    "        r\"$$\\alpha$$\",\n",
    "        \"llabor\",\n",
    "        \"lfeed\",\n",
    "        \"lcattle\",\n",
    "        \"lland\",\n",
    "        \" \",\n",
    "        r\"$$\\delta_{0}$$\",\n",
    "        r\"$$\\delta_{1}$$\",\n",
    "        r\"$$\\omega_{0}$$\",\n",
    "        r\"$$\\omega_{1}$$\",\n",
    "        r\"$$\\sigma^{2}_{v}$$\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "coefs = np.round(coefs.reshape(-1, 1), 3)\n",
    "sterr = np.round(sterr.reshape(-1, 1), 3)\n",
    "Zscores = np.round(Zscores.reshape(-1, 1), 3)\n",
    "Pvalues = np.round(Pvalues.reshape(-1, 1), 3)\n",
    "lower_ConfInt95 = np.round(lower_ConfInt95.reshape(-1, 1), 3)\n",
    "upper_ConfInt95 = np.round(upper_ConfInt95.reshape(-1, 1), 3)\n",
    "\n",
    "frontier = np.concatenate(\n",
    "    [\n",
    "        coefs[:5],\n",
    "        sterr[:5],\n",
    "        Zscores[:5],\n",
    "        Pvalues[:5],\n",
    "        lower_ConfInt95[:5],\n",
    "        upper_ConfInt95[:5],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "mu = np.concatenate(\n",
    "    [\n",
    "        coefs[5:7],\n",
    "        sterr[5:7],\n",
    "        Zscores[5:7],\n",
    "        Pvalues[5:7],\n",
    "        lower_ConfInt95[5:7],\n",
    "        upper_ConfInt95[5:7],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "usigma = np.concatenate(\n",
    "    [\n",
    "        coefs[7:9],\n",
    "        sterr[7:9],\n",
    "        Zscores[7:9],\n",
    "        Pvalues[7:9],\n",
    "        lower_ConfInt95[7:9],\n",
    "        upper_ConfInt95[7:9],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "vsigma = np.array(\n",
    "    [coefs[9], sterr[9], Zscores[9], Pvalues[9], lower_ConfInt95[9], upper_ConfInt95[9]]\n",
    ").T\n",
    "\n",
    "print(\"\\nLog-Likelihood\", round(logMLE, 3))\n",
    "results.iloc[0:5, :] = frontier\n",
    "results.iloc[5, :] = np.full(shape=(1, 6), fill_value=\" \")\n",
    "results.iloc[6:8, :] = mu\n",
    "results.iloc[8:10, :] = usigma\n",
    "results.iloc[10, :] = vsigma\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b1c3f",
   "metadata": {},
   "source": [
    "### Estimate Marginal Effects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal effect of comp on E(u)\n",
    "exogenous_data = np.column_stack([np.ones(len(z1)), z1])\n",
    "mu_vector = exogenous_data @ coefs[5:7]\n",
    "sigma_vector = np.sqrt(np.exp(exogenous_data @ coefs[7:9]))\n",
    "Lambda = mu_vector / sigma_vector\n",
    "cdf = stats.norm.cdf(Lambda)\n",
    "pdf = stats.norm.pdf(Lambda)\n",
    "ratio = pdf / cdf\n",
    "\n",
    "marginal_effects_Eu = coefs[6] * (1 - Lambda * ratio - (ratio**2)) + coefs[8] * (\n",
    "    sigma_vector / 2\n",
    ") * ((1 + (Lambda**2)) * ratio + (Lambda * ratio**2))\n",
    "average_marginal_effect_Eu = round(np.mean(marginal_effects_Eu), 5)\n",
    "print(\n",
    "    f'The average marginal effect of the variable \"comp\" on the unconditional E(u) is {average_marginal_effect_Eu}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546bc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba1890",
   "metadata": {},
   "source": [
    "## Exercise 2.9: An SF model with the scaling property\n",
    "\n",
    "This exercise estimates a Cobb-Douglas production function using a truncated normal distribution with the scaling property for technical inefficiency via maximum likelihood for 196 dairy farms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cfe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def log_density(coefs, y, x1, x2, x3, x4, z1):\n",
    "    \n",
    "    # get parameters\n",
    "    [alpha, beta1, beta2, beta3, beta4, \n",
    "     gamma1, tau, cu, sigma2v] = coefs[:9]\n",
    "\n",
    "    # Composed errors from the production function equation\n",
    "    eps = y - alpha - x1 * beta1 - x2 * beta2 - x3 * beta3 - x4 * beta4\n",
    "\n",
    "    sigma2u_tilde = cu * np.exp(2 * (gamma1 * z1))\n",
    "    u_tilde = tau * np.exp(gamma1 * z1)\n",
    "    sigma2 = sigma2v + sigma2u_tilde\n",
    "    u_star = (sigma2v * u_tilde - sigma2u_tilde * eps) / sigma2\n",
    "    sigma2_star = (sigma2v * sigma2u_tilde) / sigma2\n",
    "\n",
    "    Den = stats.norm.pdf((eps + u_tilde) / np.sqrt(sigma2)) / (\n",
    "        np.sqrt(sigma2)\n",
    "        * (\n",
    "            stats.norm.cdf(u_tilde / np.sqrt(sigma2u_tilde))\n",
    "            / stats.norm.cdf(u_star / np.sqrt(sigma2_star))\n",
    "        )\n",
    "    )\n",
    "    Den = np.where(\n",
    "        np.abs(Den) < 1e-5, 1e-5, Den\n",
    "    )  # Adjust small densities for numerical precision\n",
    "    logDen = np.log(Den)  # Log density\n",
    "\n",
    "    return logDen\n",
    "\n",
    "\n",
    "def loglikelihood(coefs, y, x1, x2, x3, x4, z1):\n",
    "    # Obtain the log likelihood\n",
    "    logDen = log_density(coefs, y, x1, x2, x3, x4, z1)\n",
    "    log_likelihood = -np.sum(logDen)\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def estimate(y, x1, x2, x3, x4, z1):\n",
    "    \n",
    "    # Starting values for MLE\n",
    "    alpha = 7.7\n",
    "    beta1 = 0.1\n",
    "    beta2 = 0.15\n",
    "    beta3 = 0.7\n",
    "    beta4 = 0.03\n",
    "    gamma1 = (-0.008)  # Coefficient for z1 in the scaling function h(.). No constant is included in the gamma parameter vector\n",
    "    tau = -1  # Mean of half-normal distribution common to all observations\n",
    "    cu = 0.15  # Variance of half-normal distribution common to all observations\n",
    "    sigma2v = 0.008\n",
    "\n",
    "    # Initial parameter vector\n",
    "    theta0 = [alpha, beta1, beta2, beta3, beta4, \n",
    "              gamma1, tau, cu, sigma2v]\n",
    "\n",
    "    bounds = [(None, None) for x in range(len(theta0) - 2)] + [\n",
    "        (1e-6, np.inf),\n",
    "        (1e-6, np.inf),\n",
    "    ]\n",
    "\n",
    "    MLE_results = minimize(\n",
    "        loglikelihood,\n",
    "        theta0,\n",
    "        method=\"L-BFGS-B\",\n",
    "        tol = 1e-6,\n",
    "        options={\"ftol\": 1e-6, \"maxiter\": 1000, \"maxfun\": 6*1000, \"maxcor\": 500},\n",
    "        args=(y, x1, x2, x3, x4, z1),\n",
    "        bounds=bounds,\n",
    "    )\n",
    "\n",
    "    theta = MLE_results.x\n",
    "    logMLE = MLE_results.fun * -1\n",
    "\n",
    "    # Standard errors\n",
    "    # Inverse of the Hessian approach\n",
    "    hessian = nd.Hessian(f=loglikelihood, method=\"central\", step=1e-6)\n",
    "    ster = np.sqrt(np.diag(np.linalg.inv(hessian(theta, y, x1, x2, x3, x4, z1))))\n",
    "\n",
    "    return theta, ster, logMLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe32001",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_data = pd.read_csv(\"dairy.csv\")  # Inputs and outputs are in Logarithms\n",
    "# Output\n",
    "y = production_data[\"ly\"]\n",
    "# Inputs\n",
    "x1 = production_data[\"llabor\"]\n",
    "x2 = production_data[\"lfeed\"]\n",
    "x3 = production_data[\"lcattle\"]\n",
    "x4 = production_data[\"lland\"]\n",
    "# Environmental variable\n",
    "z1 = production_data[\"comp\"]\n",
    "\n",
    "# Estimate coefficients via MLE and standard errors for SFA model\n",
    "coefs, sterr, logMLE = estimate(y, x1, x2, x3, x4, z1)\n",
    "\n",
    "Zscores = coefs / sterr\n",
    "Pvalues = 2 * (1 - stats.norm.cdf(np.abs(Zscores)))\n",
    "lower_ConfInt95 = stats.norm.ppf(0.025, loc=coefs, scale=sterr)\n",
    "upper_ConfInt95 = stats.norm.ppf(0.975, loc=coefs, scale=sterr)\n",
    "\n",
    "# Display the results as a table\n",
    "results = pd.DataFrame(\n",
    "    columns=[\"Est\", \"StEr\", \"z-stat\", \"p-val\", \"[95%Conf.\", \"Interv]\"],\n",
    "    index=[\n",
    "        r\"$$\\alpha$$\",\n",
    "        \"llabor\",\n",
    "        \"lfeed\",\n",
    "        \"lcattle\",\n",
    "        \"lland\",\n",
    "        \" \",\n",
    "        \"hscale\",\n",
    "        r\"$$\\tau$$\",\n",
    "        \"Cu\",\n",
    "        r\"$$\\sigma^{2}_{v}$$\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "coefs = np.round(coefs.reshape(-1, 1), 3)\n",
    "sterr = np.round(sterr.reshape(-1, 1), 3)\n",
    "Zscores = np.round(Zscores.reshape(-1, 1), 3)\n",
    "Pvalues = np.round(Pvalues.reshape(-1, 1), 3)\n",
    "lower_ConfInt95 = np.round(lower_ConfInt95.reshape(-1, 1), 3)\n",
    "upper_ConfInt95 = np.round(upper_ConfInt95.reshape(-1, 1), 3)\n",
    "\n",
    "frontier = np.concatenate(\n",
    "    [\n",
    "        coefs[:5],\n",
    "        sterr[:5],\n",
    "        Zscores[:5],\n",
    "        Pvalues[:5],\n",
    "        lower_ConfInt95[:5],\n",
    "        upper_ConfInt95[:5],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "scaling_ceofs = np.concatenate(\n",
    "    [\n",
    "        coefs[5:8],\n",
    "        sterr[5:8],\n",
    "        Zscores[5:8],\n",
    "        Pvalues[5:8],\n",
    "        lower_ConfInt95[5:8],\n",
    "        upper_ConfInt95[5:8],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "vsigma = np.array(\n",
    "    [coefs[8], sterr[8], Zscores[8], Pvalues[8], lower_ConfInt95[8], upper_ConfInt95[8]]\n",
    ").T\n",
    "\n",
    "print(\"\\nLog-Likelihood\", round(logMLE, 3))\n",
    "results.iloc[0:5, :] = frontier\n",
    "results.iloc[5, :] = np.full(shape=(1, 6), fill_value=\" \")\n",
    "results.iloc[6:9, :] = scaling_ceofs\n",
    "results.iloc[9, :] = vsigma\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf45d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b2f5a",
   "metadata": {},
   "source": [
    "## Exercise 2.10: An SF model with an exponential distribution for $\\mu_{i}$\n",
    "\n",
    "This exercise estimates a Cobb-Douglas production function with an exponential distribution for technical inefficiency via maximum likelihood for 196 dairy farms. To account for heteroskedasticity, the exponential distribution parameter is $\\eta^{2} = e^{({z}_{i}^{\\prime}{\\delta})}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e95858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def log_density(coefs, y, x1, x2, x3, x4, z1):\n",
    "    # Get parameters\n",
    "    [alpha, beta1, beta2, beta3, beta4, \n",
    "     omega0, omega1, sigma2v] = coefs[:9]\n",
    "\n",
    "    nu2 = np.exp(omega0 + omega1 * z1)\n",
    "\n",
    "    # Composed errors from the production function equation\n",
    "    eps = y - alpha - x1 * beta1 - x2 * beta2 - x3 * beta3 - x4 * beta4\n",
    "\n",
    "    cdf_term = stats.norm.cdf(\n",
    "        (-eps / np.sqrt(sigma2v)) - (np.sqrt(sigma2v) / np.sqrt(nu2))\n",
    "    )\n",
    "    Den = (\n",
    "        1\n",
    "        / np.sqrt(nu2)\n",
    "        * np.exp((eps / np.sqrt(nu2)) + (sigma2v / (2 * nu2)))\n",
    "        * cdf_term\n",
    "    )\n",
    "    logDen = np.log(Den)\n",
    "\n",
    "    return logDen\n",
    "\n",
    "\n",
    "def loglikelihood(coefs, y, x1, x2, x3, x4, z1):\n",
    "    \n",
    "    # Obtain the log likelihood\n",
    "    logDen = log_density(coefs, y, x1, x2, x3, x4, z1)\n",
    "    log_likelihood = -np.sum(logDen)\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def estimate(y, x1, x2, x3, x4, z1):\n",
    "    # Starting values for MLE\n",
    "    alpha = 7.5\n",
    "    beta1 = 0.1\n",
    "    beta2 = 0.15\n",
    "    beta3 = 0.7\n",
    "    beta4 = 0.03\n",
    "    omega0 = -4\n",
    "    omega1 = -0.005  # Coefficient for z1\n",
    "    sigma2v = 0.01\n",
    "\n",
    "    # Initial parameter vector\n",
    "    theta0 = [alpha, beta1, beta2, beta3, beta4, omega0, omega1, sigma2v]\n",
    "\n",
    "    # Bounds to ensure sigma2v and sigma2u are > 0\n",
    "    bounds = [(None, None) for x in range(len(theta0) - 1)] + [(1e-4, np.inf)]\n",
    "\n",
    "    MLE_results = minimize(\n",
    "        loglikelihood,\n",
    "        theta0,\n",
    "        method=\"L-BFGS-B\",\n",
    "        tol = 1e-6,\n",
    "        options={\"ftol\": 1e-6, \"maxiter\": 1000, \"maxfun\": 6*1000, \"maxcor\": 500},\n",
    "        args=(y, x1, x2, x3, x4, z1),\n",
    "        bounds=bounds,\n",
    "    )\n",
    "\n",
    "    theta = MLE_results.x\n",
    "    logMLE = MLE_results.fun * -1\n",
    "\n",
    "    # Standard errors\n",
    "    # Inverse of the Hessian approach\n",
    "    hessian = nd.Hessian(f=loglikelihood, method=\"central\", step=1e-6)\n",
    "    ster = np.sqrt(np.diag(np.linalg.inv(hessian(theta, y, x1, x2, x3, x4, z1))))\n",
    "\n",
    "    return theta, ster, logMLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e54227",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_data = pd.read_csv(\"dairy.csv\")  # Inputs and outputs are in Logarithms\n",
    "# Output\n",
    "y = production_data[\"ly\"]\n",
    "# Inputs\n",
    "x1 = production_data[\"llabor\"]\n",
    "x2 = production_data[\"lfeed\"]\n",
    "x3 = production_data[\"lcattle\"]\n",
    "x4 = production_data[\"lland\"]\n",
    "# Environmental variable\n",
    "z1 = production_data[\"comp\"]\n",
    "\n",
    "# Estimate coefficients via MLE and standard errors for SFA model\n",
    "coefs, sterr, logMLE = estimate(y, x1, x2, x3, x4, z1)\n",
    "\n",
    "Zscores = coefs / sterr\n",
    "Pvalues = 2 * (1 - stats.norm.cdf(np.abs(Zscores)))\n",
    "lower_ConfInt95 = stats.norm.ppf(0.025, loc=coefs, scale=sterr)\n",
    "upper_ConfInt95 = stats.norm.ppf(0.975, loc=coefs, scale=sterr)\n",
    "\n",
    "# Display the results as a table\n",
    "results = pd.DataFrame(\n",
    "    columns=[\"Est\", \"StEr\", \"z-stat\", \"p-val\", \"[95%Conf.\", \"Interv]\"],\n",
    "    index=[\n",
    "        r\"$$\\alpha$$\",\n",
    "        \"llabor\",\n",
    "        \"lfeed\",\n",
    "        \"lcattle\",\n",
    "        \"lland\",\n",
    "        \" \",\n",
    "        r\"$$\\omega_{0}$$\",\n",
    "        r\"$$\\omega_{1}$$\",\n",
    "        r\"$$\\sigma^{v}_{2}$$\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "coefs = np.round(coefs.reshape(-1, 1), 3)\n",
    "sterr = np.round(sterr.reshape(-1, 1), 3)\n",
    "Zscores = np.round(Zscores.reshape(-1, 1), 3)\n",
    "Pvalues = np.round(Pvalues.reshape(-1, 1), 3)\n",
    "lower_ConfInt95 = np.round(lower_ConfInt95.reshape(-1, 1), 3)\n",
    "upper_ConfInt95 = np.round(upper_ConfInt95.reshape(-1, 1), 3)\n",
    "\n",
    "frontier = np.concatenate(\n",
    "    [\n",
    "        coefs[:5],\n",
    "        sterr[:5],\n",
    "        Zscores[:5],\n",
    "        Pvalues[:5],\n",
    "        lower_ConfInt95[:5],\n",
    "        upper_ConfInt95[:5],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "etas = np.concatenate(\n",
    "    [\n",
    "        coefs[5:7],\n",
    "        sterr[5:7],\n",
    "        Zscores[5:7],\n",
    "        Pvalues[5:7],\n",
    "        lower_ConfInt95[5:7],\n",
    "        upper_ConfInt95[5:7],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "vsigma = np.array(\n",
    "    [coefs[7], sterr[7], Zscores[7], Pvalues[7], lower_ConfInt95[7], upper_ConfInt95[7]]\n",
    ").T\n",
    "\n",
    "print(\"\\nLog-Likelihood\", round(logMLE, 3))\n",
    "results.iloc[0:5, :] = frontier\n",
    "results.iloc[5, :] = np.full(shape=(1, 6), fill_value=\" \")\n",
    "results.iloc[6:8, :] = etas\n",
    "results.iloc[8, :] = vsigma\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1140c6",
   "metadata": {},
   "source": [
    "## Exercise 2.11: Non-independence of $u$ and $v$\n",
    "\n",
    "This exercise estimates the MSLE of the diary farm production model under the Gaussiann copulabetween $u$ and $v$. Unlike in the application of Smith (2008), in our application the estimate of copula paramete $\\rho$  is large an statistically significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb6dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def log_density(coefs, y, x1, x2, x3, x4, us):\n",
    "    \n",
    "    N = len(y)\n",
    "    S = len(us)\n",
    "\n",
    "    [alpha, beta1, beta2, beta3, beta4,\n",
    "    sigma2u, sigma2v, theta] = coefs[:]\n",
    "    \n",
    "    Rho = np.array([[1,theta], \n",
    "                    [theta,1]])\n",
    "    try:\n",
    "        R = np.linalg.cholesky(Rho)\n",
    "        eps = y - alpha - x1*beta1 - x2*beta2 - x3*beta3 - x4*beta4 #Composed errors from the production function equation. \n",
    "        eps_SxN = np.repeat(eps.values.reshape(-1,1), S, axis=1).T\n",
    "        us_SxN = np.repeat(np.sqrt(sigma2u)*us.reshape(-1,1), N, axis=1)\n",
    "        EpsPlusUs = eps_SxN + us_SxN\n",
    "        \n",
    "        DenEpsPlusUs = stats.norm.pdf(EpsPlusUs, loc=0, scale=np.sqrt(sigma2v))\n",
    "        CDFEpsPlusU = stats.norm.cdf(EpsPlusUs, loc=0, scale=np.sqrt(sigma2v))\n",
    "        CdfUs = 2*(stats.norm.cdf(np.sqrt(sigma2u)*us, loc=0, scale=np.sqrt(sigma2u)) -0.5)\n",
    "        \n",
    "        simulated_copula_pdfs = np.zeros(shape=(S,N))\n",
    "        for j in range(N):\n",
    "            U = np.concatenate([stats.norm.ppf(CDFEpsPlusU[:, j]).reshape(-1,1), \n",
    "                                stats.norm.ppf(CdfUs).reshape(-1,1)], axis=1)\n",
    "            copula_den = stats.multivariate_normal.pdf(U, \n",
    "                                                       mean=np.zeros(2),\n",
    "                                                       cov=Rho)/np.prod(stats.norm.pdf(U), \n",
    "                                                                        axis=1)\n",
    "            simulated_copula_pdfs[:,j] = copula_den\n",
    "\n",
    "        den = np.mean(simulated_copula_pdfs*DenEpsPlusUs, axis=0)\n",
    "\n",
    "        logDen = np.log(den)\n",
    "        \n",
    "    except np.linalg.LinAlgError:\n",
    "        logDen = np.ones(N)*-1e8 #Assign an arbitrarily large log density if R is not positve definite\n",
    "    \n",
    "    return logDen\n",
    "\n",
    "def loglikelihood(coefs, y, x1, x2, x3, x4, us):\n",
    "    # Obtain the log likelihood\n",
    "    logDen = log_density(coefs, y, x1, x2, x3, x4, us)\n",
    "    log_likelihood = -np.sum(logDen)\n",
    "\n",
    "    return log_likelihood\n",
    "    \n",
    "\n",
    "def estimate(y, x1, x2, x3, x4):\n",
    "\n",
    "    S = 100 # no. of simulated draws for the Integral \n",
    "    us = stats.norm.ppf((stats.uniform.rvs(size=S, \n",
    "                                           random_state=1234)+1)/2, 0, 1) # Simulated standard half-normal RVs\n",
    "    \n",
    "    alpha = 7.5\n",
    "    beta1 = 0.1\n",
    "    beta2 = 0.15\n",
    "    beta3 = 0.7\n",
    "    beta4 = 0.03\n",
    "    sigma2u = 0.2\n",
    "    sigma2v = 0.06\n",
    "    intiial_theta = 0.9\n",
    "\n",
    "    theta0 = np.array([alpha, beta1, beta2, beta3, beta4, \n",
    "                       sigma2u, sigma2v, intiial_theta])\n",
    "\n",
    "    # Bounds to ensure sigma2v and sigma2u are > 0\n",
    "    bounds = [(None, None) for x in range(len(theta0) - 3)] + [(1e-6, np.inf), (1e-6, np.inf), (1e-6, np.inf)]\n",
    "\n",
    "    MLE_results = minimize(\n",
    "        loglikelihood,\n",
    "        theta0,\n",
    "        method=\"Nelder-Mead\",\n",
    "        options={\"maxiter\": 1000},\n",
    "        args=(y, x1, x2, x3, x4, us),\n",
    "        bounds=bounds,\n",
    "    )\n",
    "    \n",
    "\n",
    "    theta = MLE_results.x\n",
    "    logMLE = MLE_results.fun * -1\n",
    "                                                               \n",
    "    # Standard errors\n",
    "    # Inverse of the Hessian approach\n",
    "    hessian = nd.Hessian(f=loglikelihood, method=\"central\", step=1e-6)\n",
    "    ster = np.sqrt(np.diag(np.linalg.inv(hessian(theta, y, x1, x2, x3, x4, us))))\n",
    "\n",
    "    return theta, ster, logMLE                                                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eddb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_data = pd.read_csv(\"dairy.csv\")  # Inputs and outputs are in Logarithms\n",
    "# Output\n",
    "y = production_data[\"ly\"]\n",
    "# Inputs\n",
    "x1 = production_data[\"llabor\"]\n",
    "x2 = production_data[\"lfeed\"]\n",
    "x3 = production_data[\"lcattle\"]\n",
    "x4 = production_data[\"lland\"]\n",
    "\n",
    "# Estimate coefficients via MLE and standard errors for SFA model\n",
    "coefs, sterr, logMLE = estimate(y, x1, x2, x3, x4)\n",
    "\n",
    "Zscores = coefs / sterr\n",
    "Pvalues = 2 * (1 - stats.norm.cdf(np.abs(Zscores)))\n",
    "lower_ConfInt95 = stats.norm.ppf(0.025, loc=coefs, scale=sterr)\n",
    "upper_ConfInt95 = stats.norm.ppf(0.975, loc=coefs, scale=sterr)\n",
    "\n",
    "# Display the results as a table\n",
    "results = pd.DataFrame(\n",
    "    columns=[\"Est\", \"StEr\", \"z-stat\", \"p-val\", \"[95%Conf.\", \"Interv]\"],\n",
    "    index=[\n",
    "        r\"$$\\alpha$$\",\n",
    "        r\"$$\\beta_{1} llabor$$\",\n",
    "        r\"$$\\beta_{2} lfeed$$\",\n",
    "        r\"$$\\beta_{3} lcattle$$\",\n",
    "        r\"$$\\beta_{4} lland$$\",\n",
    "        \" \",\n",
    "        r\"$$\\sigma^{u}_{2}$$\",\n",
    "        r\"$$\\sigma^{v}_{2}$$\", \n",
    "        r\"$$\\theta$$\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "coefs = np.round(coefs.reshape(-1, 1), 3)\n",
    "sterr = np.round(sterr.reshape(-1, 1), 3)\n",
    "Zscores = np.round(Zscores.reshape(-1, 1), 3)\n",
    "Pvalues = np.round(Pvalues.reshape(-1, 1), 3)\n",
    "lower_ConfInt95 = np.round(lower_ConfInt95.reshape(-1, 1), 3)\n",
    "upper_ConfInt95 = np.round(upper_ConfInt95.reshape(-1, 1), 3)\n",
    "\n",
    "frontier = np.concatenate(\n",
    "    [\n",
    "        coefs[:5],\n",
    "        sterr[:5],\n",
    "        Zscores[:5],\n",
    "        Pvalues[:5],\n",
    "        lower_ConfInt95[:5],\n",
    "        upper_ConfInt95[:5],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "sigmas = np.array(\n",
    "    [coefs[5:], sterr[5:], Zscores[5:], Pvalues[5:], lower_ConfInt95[5:], upper_ConfInt95[5:]]\n",
    ").T\n",
    "\n",
    "print(\"\\nLog-Likelihood\", round(logMLE, 3))\n",
    "results.iloc[0:5, :] = frontier\n",
    "results.iloc[5, :] = np.full(shape=(1, 6), fill_value=\" \")\n",
    "results.iloc[6:, :] = sigmas\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45ea72",
   "metadata": {},
   "source": [
    "# Appendices {.unnumbered}\n",
    "\n",
    "## Exercise 2.12: Ordinary Least Squares regression example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944dadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"auto.csv\")\n",
    "y = data[\"mpg\"]\n",
    "X = data[[\"headroom\", \"trunk\", \"weight\", \"foreign\"]]\n",
    "\n",
    "# Convert the foregin variable into a numeric dummy variable. Domestic = 1, Foreign = 0.\n",
    "X[\"foreign\"] = np.where(X[\"foreign\"] == \"Domestic\", 1, 0)\n",
    "\n",
    "# Add vector of 1s for the model intercept\n",
    "X[\"intercept\"] = np.ones(len(X))\n",
    "\n",
    "regression_model = sm.OLS(y, X)\n",
    "results = regression_model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4856340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d1d04",
   "metadata": {},
   "source": [
    "## Exercise 2.13: Maximum Likelihood Estimation example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def cb_func(xk):\n",
    "    \"\"\"\n",
    "    Callback function to record likelihood at each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    global obj_value, iterations\n",
    "    obj_value.append(log_likelihood(theta=xk, y=y))\n",
    "    iterations += 1\n",
    "\n",
    "\n",
    "def log_likelihood(theta, y):\n",
    "    mu = theta[0]\n",
    "    sigma = theta[1]\n",
    "    log_Den = np.log(stats.norm.pdf((y - mu) / sigma)) - np.log(sigma)\n",
    "\n",
    "    return -np.sum(log_Den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5feebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "global obj_value, iterations\n",
    "obj_value = []\n",
    "iterations = 0\n",
    "\n",
    "data = pd.read_csv(r\"auto.csv\")\n",
    "y = data[\"mpg\"]\n",
    "\n",
    "theta0 = np.array([20, 4.5])  # initial parameter vector\n",
    "\n",
    "# Minimize the negative log-likelihood function\n",
    "MLE_results = minimize(\n",
    "    log_likelihood,\n",
    "    theta0,\n",
    "    method=\"L-BFGS-B\",\n",
    "    options={\"disp\": False, \"maxiter\": 10000},\n",
    "    args=(y),\n",
    "    callback=cb_func,\n",
    "    bounds=[(None, None), (0.001, np.inf)],\n",
    ")\n",
    "\n",
    "theta = MLE_results[\"x\"]\n",
    "logMLE = MLE_results.fun\n",
    "\n",
    "# Tabulate the results\n",
    "results = pd.DataFrame(\n",
    "    data={\"$$\\mu$$\": theta[0], \"$$\\sigma$$\": theta[1]}, index=[\"Parameter Estimates\"]\n",
    ").T.round(4)\n",
    "\n",
    "print(\"\\nLog-Likelihood \", round(logMLE, 4))\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558b828",
   "metadata": {},
   "source": [
    "### Plot the objective function path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3faced",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(10, 8))\n",
    "plt.plot([x + 1 for x in range(iterations)], np.array(obj_value) * -1)\n",
    "plt.xlabel(\"Iteration number\", fontsize=14)\n",
    "plt.ylabel(\"Log-Likelihood\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c392396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions.\n",
    "%reset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
