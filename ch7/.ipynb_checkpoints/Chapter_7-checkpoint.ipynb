{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d18c50b8",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  keep-ipynb: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b645b",
   "metadata": {},
   "source": [
    "# Chapter 7: Predicting inefficiency\n",
    "\n",
    "## Exercise 7.1: JLMS estimator of inefficiency of rice production\n",
    "This exercise uses the procedure outlined in Section 7.2 to estimate technical inefficiencies of Indonesian rice farms. The details of the data are provided in Exercise 4.2. The table reports predicted technical inefficiencies in each time period for ten farms, namely those that are at the 0.05,0.15,...,0.95 fractiles of the sample of inefficiencies for $t = 1$. The first two columns report the rank and the quantile of the selected farms. The third and fourth columns contain the sample variation and sample mean over t, respectively, of the inefficiencies, which are reported in columns 5 through 10. The last row of the tables reports column-wise averages over the ten farms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55543b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def JLMS_panel_technical_inefficiency_scores(theta, y, X):\n",
    "    N = 171\n",
    "    T = 6\n",
    "\n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:14]\n",
    "    sigma = theta[-2]\n",
    "    _lambda = theta[-1]\n",
    "\n",
    "    u_hat = np.zeros((N, T))\n",
    "    for t in range(T):\n",
    "\n",
    "        eps_t = y[t] - alpha - X[t] @ beta\n",
    "        b = (eps_t * _lambda) / sigma\n",
    "        u_hat[:, t] = ((sigma * _lambda) / (1 + _lambda**2)) * (\n",
    "            stats.norm.pdf(b) / (1 - stats.norm.cdf(b)) - b\n",
    "        )\n",
    "\n",
    "    return u_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfccf002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.32398564193139884\n",
      "0 0.1344378962869995\n",
      "1 0.34236427934816605\n",
      "1 0.17368049476527608\n",
      "2 0.44728387759674604\n",
      "2 0.22617244206178974\n",
      "3 0.5131591829241486\n",
      "3 0.2022377435411547\n",
      "4 0.3124380164105146\n",
      "4 0.1278679397079935\n",
      "5 0.3211212850066159\n",
      "5 0.14298280637746294\n",
      "JLMS Scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py:343: FutureWarning:\n",
      "\n",
      "In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Fractile</th>\n",
       "      <th>Std(u)</th>\n",
       "      <th>Mean(u)</th>\n",
       "      <th>t=1</th>\n",
       "      <th>t=2</th>\n",
       "      <th>t=3</th>\n",
       "      <th>t=4</th>\n",
       "      <th>t=5</th>\n",
       "      <th>t=6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.199559</td>\n",
       "      <td>0.353626</td>\n",
       "      <td>0.155185</td>\n",
       "      <td>0.334495</td>\n",
       "      <td>0.751678</td>\n",
       "      <td>0.298441</td>\n",
       "      <td>0.168792</td>\n",
       "      <td>0.413164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.266284</td>\n",
       "      <td>0.389258</td>\n",
       "      <td>0.194313</td>\n",
       "      <td>0.247702</td>\n",
       "      <td>0.901049</td>\n",
       "      <td>0.577669</td>\n",
       "      <td>0.251172</td>\n",
       "      <td>0.163642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.089351</td>\n",
       "      <td>0.285298</td>\n",
       "      <td>0.227678</td>\n",
       "      <td>0.192656</td>\n",
       "      <td>0.460310</td>\n",
       "      <td>0.332479</td>\n",
       "      <td>0.232645</td>\n",
       "      <td>0.266019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.139124</td>\n",
       "      <td>0.389688</td>\n",
       "      <td>0.258642</td>\n",
       "      <td>0.210409</td>\n",
       "      <td>0.295907</td>\n",
       "      <td>0.535117</td>\n",
       "      <td>0.477681</td>\n",
       "      <td>0.560369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.200043</td>\n",
       "      <td>0.338445</td>\n",
       "      <td>0.283757</td>\n",
       "      <td>0.201320</td>\n",
       "      <td>0.559250</td>\n",
       "      <td>0.664111</td>\n",
       "      <td>0.178528</td>\n",
       "      <td>0.143705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>94</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.065599</td>\n",
       "      <td>0.362450</td>\n",
       "      <td>0.307076</td>\n",
       "      <td>0.346067</td>\n",
       "      <td>0.296354</td>\n",
       "      <td>0.380834</td>\n",
       "      <td>0.349237</td>\n",
       "      <td>0.495129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>111</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.108994</td>\n",
       "      <td>0.455132</td>\n",
       "      <td>0.336327</td>\n",
       "      <td>0.376101</td>\n",
       "      <td>0.393122</td>\n",
       "      <td>0.559206</td>\n",
       "      <td>0.642810</td>\n",
       "      <td>0.423228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.139395</td>\n",
       "      <td>0.307457</td>\n",
       "      <td>0.393067</td>\n",
       "      <td>0.551203</td>\n",
       "      <td>0.307025</td>\n",
       "      <td>0.291704</td>\n",
       "      <td>0.134145</td>\n",
       "      <td>0.167600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>145</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.293376</td>\n",
       "      <td>0.600202</td>\n",
       "      <td>0.466352</td>\n",
       "      <td>0.382522</td>\n",
       "      <td>1.180340</td>\n",
       "      <td>0.717295</td>\n",
       "      <td>0.284017</td>\n",
       "      <td>0.570689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>162</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.138623</td>\n",
       "      <td>0.552315</td>\n",
       "      <td>0.568924</td>\n",
       "      <td>0.719727</td>\n",
       "      <td>0.540897</td>\n",
       "      <td>0.716119</td>\n",
       "      <td>0.425638</td>\n",
       "      <td>0.342586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrr}\n",
       "\\toprule\n",
       "{} &  Rank &  Fractile &    Std(u) &   Mean(u) &       t=1 &       t=2 &       t=3 &       t=4 &       t=5 &       t=6 \\\\\n",
       "\\midrule\n",
       "0 &     9 &      0.05 &  0.199559 &  0.353626 &  0.155185 &  0.334495 &  0.751678 &  0.298441 &  0.168792 &  0.413164 \\\\\n",
       "1 &    26 &      0.15 &  0.266284 &  0.389258 &  0.194313 &  0.247702 &  0.901049 &  0.577669 &  0.251172 &  0.163642 \\\\\n",
       "2 &    43 &      0.25 &  0.089351 &  0.285298 &  0.227678 &  0.192656 &  0.460310 &  0.332479 &  0.232645 &  0.266019 \\\\\n",
       "3 &    60 &      0.35 &  0.139124 &  0.389688 &  0.258642 &  0.210409 &  0.295907 &  0.535117 &  0.477681 &  0.560369 \\\\\n",
       "4 &    77 &      0.45 &  0.200043 &  0.338445 &  0.283757 &  0.201320 &  0.559250 &  0.664111 &  0.178528 &  0.143705 \\\\\n",
       "5 &    94 &      0.55 &  0.065599 &  0.362450 &  0.307076 &  0.346067 &  0.296354 &  0.380834 &  0.349237 &  0.495129 \\\\\n",
       "6 &   111 &      0.65 &  0.108994 &  0.455132 &  0.336327 &  0.376101 &  0.393122 &  0.559206 &  0.642810 &  0.423228 \\\\\n",
       "7 &   128 &      0.75 &  0.139395 &  0.307457 &  0.393067 &  0.551203 &  0.307025 &  0.291704 &  0.134145 &  0.167600 \\\\\n",
       "8 &   145 &      0.85 &  0.293376 &  0.600202 &  0.466352 &  0.382522 &  1.180340 &  0.717295 &  0.284017 &  0.570689 \\\\\n",
       "9 &   162 &      0.95 &  0.138623 &  0.552315 &  0.568924 &  0.719727 &  0.540897 &  0.716119 &  0.425638 &  0.342586 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "   Rank  Fractile    Std(u)   Mean(u)       t=1       t=2       t=3       t=4  \\\n",
       "0     9      0.05  0.199559  0.353626  0.155185  0.334495  0.751678  0.298441   \n",
       "1    26      0.15  0.266284  0.389258  0.194313  0.247702  0.901049  0.577669   \n",
       "2    43      0.25  0.089351  0.285298  0.227678  0.192656  0.460310  0.332479   \n",
       "3    60      0.35  0.139124  0.389688  0.258642  0.210409  0.295907  0.535117   \n",
       "4    77      0.45  0.200043  0.338445  0.283757  0.201320  0.559250  0.664111   \n",
       "5    94      0.55  0.065599  0.362450  0.307076  0.346067  0.296354  0.380834   \n",
       "6   111      0.65  0.108994  0.455132  0.336327  0.376101  0.393122  0.559206   \n",
       "7   128      0.75  0.139395  0.307457  0.393067  0.551203  0.307025  0.291704   \n",
       "8   145      0.85  0.293376  0.600202  0.466352  0.382522  1.180340  0.717295   \n",
       "9   162      0.95  0.138623  0.552315  0.568924  0.719727  0.540897  0.716119   \n",
       "\n",
       "        t=5       t=6  \n",
       "0  0.168792  0.413164  \n",
       "1  0.251172  0.163642  \n",
       "2  0.232645  0.266019  \n",
       "3  0.477681  0.560369  \n",
       "4  0.178528  0.143705  \n",
       "5  0.349237  0.495129  \n",
       "6  0.642810  0.423228  \n",
       "7  0.134145  0.167600  \n",
       "8  0.284017  0.570689  \n",
       "9  0.425638  0.342586  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py:343: FutureWarning:\n",
      "\n",
      "In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Std(u)</th>\n",
       "      <th>Mean(u)</th>\n",
       "      <th>t=1</th>\n",
       "      <th>t=2</th>\n",
       "      <th>t=3</th>\n",
       "      <th>t=4</th>\n",
       "      <th>t=5</th>\n",
       "      <th>t=6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.164035</td>\n",
       "      <td>0.403387</td>\n",
       "      <td>0.319132</td>\n",
       "      <td>0.35622</td>\n",
       "      <td>0.568593</td>\n",
       "      <td>0.507298</td>\n",
       "      <td>0.314467</td>\n",
       "      <td>0.354613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrr}\n",
       "\\toprule\n",
       "{} &    Std(u) &   Mean(u) &       t=1 &      t=2 &       t=3 &       t=4 &       t=5 &       t=6 \\\\\n",
       "\\midrule\n",
       "Average &  0.164035 &  0.403387 &  0.319132 &  0.35622 &  0.568593 &  0.507298 &  0.314467 &  0.354613 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "           Std(u)   Mean(u)       t=1      t=2       t=3       t=4       t=5  \\\n",
       "Average  0.164035  0.403387  0.319132  0.35622  0.568593  0.507298  0.314467   \n",
       "\n",
       "              t=6  \n",
       "Average  0.354613  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ricefarm = pd.read_csv(r\"ricefarm.csv\")\n",
    "\n",
    "ricefarm = ricefarm.iloc[:, [0, 1, 3, 5, 6, 7, 8, 14, 16]]\n",
    "\n",
    "# Create ID dummies\n",
    "dar = np.round(ricefarm[\"ID\"] / 100000).astype(int)\n",
    "id_dummies = pd.get_dummies(dar, prefix=\"DR\").astype(int)\n",
    "ricefarm = pd.concat([ricefarm, id_dummies.iloc[:, :-1]], axis=1)\n",
    "\n",
    "# Create rice variety dummies\n",
    "rice_dummies = pd.get_dummies(ricefarm.iloc[:, 2], prefix=\"VAR\").astype(int)\n",
    "ricefarm = pd.concat([ricefarm, rice_dummies.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Recode TSP as logs and zeros\n",
    "ricefarm[ricefarm.columns[5]] = np.where(\n",
    "    ricefarm.iloc[:, 5] > 0, np.log(ricefarm.iloc[:, 5]), 0\n",
    ")\n",
    "\n",
    "# Convert pesticide usage to a dummy\n",
    "ricefarm[ricefarm.columns[6]] = (ricefarm.iloc[:, 6] != 0).astype(int)\n",
    "\n",
    "# Reorder the data\n",
    "ricefarm = ricefarm.iloc[:, [8, 3, 4, 5, 7, 1, 6, 14, 15, 9, 10, 11, 12, 13]]\n",
    "\n",
    "y = np.log(ricefarm.iloc[:, 0].values)\n",
    "X = ricefarm.iloc[:, 1:].values\n",
    "\n",
    "# Log covariates\n",
    "X[:, 0:2] = np.log(X[:, 0:2])\n",
    "X[:, 3] = np.log(X[:, 3])\n",
    "X[:, 4] = np.log(X[:, 4])\n",
    "\n",
    "# Cross sections of  y and X\n",
    "N = 171\n",
    "T = 6\n",
    "y_t = []\n",
    "X_t = []\n",
    "for t in range(T):\n",
    "    if t == 0:\n",
    "        y_t.append(y[:N])\n",
    "        X_t.append(X[:N, :])\n",
    "    else:\n",
    "        y_t.append(y[t * N : (t + 1) * N])\n",
    "        X_t.append(X[t * N : (t + 1) * N, :])\n",
    "\n",
    "# Import the model parameters from exercise 5.8\n",
    "theta = np.loadtxt(\"exercise_4_8_theta_python.txt\", delimiter=\",\")\n",
    "\n",
    "# Estimation of technical inefficiencies\n",
    "# Get idx of percentile firms\n",
    "percetile_firm_idx = np.round(np.arange(0.05, 1, 0.1) * N).astype(int)\n",
    "\n",
    "# JLMS formula technical inefficiencies\n",
    "JLMS_u_hat = JLMS_panel_technical_inefficiency_scores(theta=theta, y=y_t, X=X_t)\n",
    "# Sort technical inefficiency scores by t=1 values\n",
    "sort_idx = JLMS_u_hat[:, 0].argsort()\n",
    "sorted_t1_JLMS_u_hat = JLMS_u_hat[sort_idx]\n",
    "# Mean technical inefficiency for each firms over all t\n",
    "JLMS_mean = np.mean(JLMS_u_hat, axis=1)\n",
    "sorted_t1_JLMS_mean = JLMS_mean[sort_idx]\n",
    "# Standard deviation of technical inefficiency for each firm over all t\n",
    "JLMS_std = np.std(JLMS_u_hat, axis=1)\n",
    "sorted_t1_JLMS_std = JLMS_std[sort_idx]\n",
    "\n",
    "JLMS_u_hat_for_table = sorted_t1_JLMS_u_hat[percetile_firm_idx-1, :]\n",
    "JLMS_std_u_hat_for_table = sorted_t1_JLMS_std[percetile_firm_idx-1]\n",
    "JLMS_mean_u_hat_for_table = sorted_t1_JLMS_mean[percetile_firm_idx-1]\n",
    "\n",
    "# Tabulate the technical inefficiency scores\n",
    "JLMS_technical_inefficiency_results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Rank\",\n",
    "        \"Fractile\",\n",
    "        \"Std(u)\",\n",
    "        \"Mean(u)\",\n",
    "        \"t=1\",\n",
    "        \"t=2\",\n",
    "        \"t=3\",\n",
    "        \"t=4\",\n",
    "        \"t=5\",\n",
    "        \"t=6\",\n",
    "    ]\n",
    ")\n",
    "JLMS_technical_inefficiency_results[\"Rank\"] = percetile_firm_idx\n",
    "JLMS_technical_inefficiency_results[\"Fractile\"] = np.arange(0.05, 1, 0.1)\n",
    "JLMS_technical_inefficiency_results[\"Std(u)\"] = JLMS_std_u_hat_for_table\n",
    "JLMS_technical_inefficiency_results[\"Mean(u)\"] = JLMS_mean_u_hat_for_table\n",
    "JLMS_technical_inefficiency_results.iloc[:, -6:] = JLMS_u_hat_for_table\n",
    "\n",
    "# Compute averages\n",
    "average_JLMS = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            np.mean(JLMS_std_u_hat_for_table).reshape(-1, 1).T,\n",
    "            np.mean(JLMS_mean_u_hat_for_table).reshape(-1, 1).T,\n",
    "            np.mean(JLMS_u_hat_for_table, axis=0).reshape(-1, 1).T,\n",
    "        ],\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"Std(u)\", \"Mean(u)\", \"t=1\", \"t=2\", \"t=3\", \"t=4\", \"t=5\", \"t=6\"],\n",
    "    index=[\"Average\"],\n",
    ")\n",
    "\n",
    "print(\"JLMS Scores\")\n",
    "display(JLMS_technical_inefficiency_results)\n",
    "display(average_JLMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37fdd4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83e14f",
   "metadata": {},
   "source": [
    "## Exercise 7.2: The APS estimator of inefficiency of rice production\n",
    "This exercise uses the procedure outlined in Section 7.4.2 to estimate technical inefficiencies of Indonesian rice farms. The predictions employ the Gaussian copula to draw $S = 10,000$ observations from the joint distribution of $u$ and $\\epsilon$. For the simulated sample, the predictions are based on the Nadaraya-Watson estimator which uses the Gaussian kernel with a common bandwidth parameter chosen by cross-validation.\n",
    "\n",
    "The table reports predicted inefficiencies in each time period for ten farms, namely those that are at the 0.05, 0.15, . . . , 0.95 fractiles of the sample of inefficiencies at $t = 1$. The first two columns report the rank and the quantile of the selected farms. The third and fourth columns contain the sample variation and sample mean over $t$, respectively, of the inefficiency estimates, which are reported in columns 5 through 10. The last row of the tables reports column-wise averages over the ten farms. (The results is somewhat different from the original results by Amsler et al. (2014), who used GAUSS and obtained a smaller average $\\hat{\\sigma}$ compared to JLMS.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fdcb99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.linalg import expm, logm, norm\n",
    "\n",
    "\n",
    "def JLMS_panel_technical_inefficiency_scores(theta, y, X):\n",
    "    N = 171\n",
    "    T = 6\n",
    "\n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:14]\n",
    "    sigma = theta[-2]\n",
    "    _lambda = theta[-1]\n",
    "\n",
    "    u_hat = np.zeros((N, T))\n",
    "    for t in range(T):\n",
    "\n",
    "        eps_t = y[t] - alpha - X[t] @ beta\n",
    "        b = (eps_t * _lambda) / sigma\n",
    "        u_hat[:, t] = ((sigma * _lambda) / (1 + _lambda**2)) * (\n",
    "            stats.norm.pdf(b) / (1 - stats.norm.cdf(b)) - b\n",
    "        )\n",
    "\n",
    "\n",
    "    return u_hat\n",
    "\n",
    "\n",
    "def inverse_mapping_vec(gamma, tol_value=1e-8):\n",
    "    C = []\n",
    "    iter_number = -1\n",
    "\n",
    "    # Check if input is of proper format: gamma is of suitable length\n",
    "    if not isinstance(gamma, (np.ndarray, list)):\n",
    "        raise ValueError\n",
    "    if isinstance(gamma, np.ndarray):\n",
    "        if gamma.ndim != 1:\n",
    "            raise ValueError\n",
    "    n = 0.5 * (1 + np.sqrt(1 + 8 * len(gamma)))\n",
    "    if not all([n.is_integer(), n > 1]):\n",
    "        raise ValueError\n",
    "\n",
    "    # Check if tolerance value belongs to a proper interval\n",
    "    # and change it to the default value otherwise\n",
    "    if not (0 < tol_value <= 1e-4):\n",
    "        tol_value = 1e-8\n",
    "        print(\"Warning: tolerance value has been changed to default\")\n",
    "\n",
    "    # Place elements from gamma into off-diagonal parts\n",
    "    # and put zeros on the main diagonal of [n x n] symmetric matrix A\n",
    "    n = int(n)\n",
    "    A = np.zeros(shape=(n, n))\n",
    "    A[np.triu_indices(n, 1)] = gamma\n",
    "    A = A + A.T\n",
    "\n",
    "    # Read properties of the input matrix\n",
    "    diag_vec = np.diag(A)  # get current diagonal\n",
    "    diag_ind = np.diag_indices_from(\n",
    "        A\n",
    "    )  # get row and column indices of diagonal elements\n",
    "\n",
    "    # Iterative algorithm to get the proper diagonal vector\n",
    "    dist = np.sqrt(n)\n",
    "    while dist > np.sqrt(n) * tol_value:\n",
    "        diag_delta = np.log(np.diag(expm(A)))\n",
    "        diag_vec = diag_vec - diag_delta\n",
    "        A[diag_ind] = diag_vec\n",
    "        dist = norm(diag_delta)\n",
    "        iter_number += 1\n",
    "\n",
    "    # Get a unique reciprocal correlation matrix\n",
    "    C = expm(A)\n",
    "    np.fill_diagonal(C, 1)\n",
    "\n",
    "    return C\n",
    "\n",
    "def NW_panel_technical_inefficiency_scores(theta, y, X, U_hat):\n",
    "    N = 171\n",
    "    T = 6\n",
    "\n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:14]\n",
    "    sigma = theta[-2]\n",
    "    _lambda = theta[-1]\n",
    "\n",
    "    sigma2u = ((_lambda * sigma) / (1 + _lambda)) ** 2\n",
    "    sigma2v = ((sigma) / (1 + _lambda)) ** 2\n",
    "\n",
    "    obs_eps = np.zeros((N, T))\n",
    "    for t in range(T):\n",
    "        obs_eps[:, t] = y[t] - alpha - X[t] @ beta\n",
    "\n",
    "    # Simulated half normal RVs from the estimated Gaussian copula\n",
    "    simulated_u = stats.halfnorm.ppf(\n",
    "        U_hat, loc=np.zeros(T), scale=np.array([np.sqrt(sigma2u) for x in range(T)])\n",
    "    )\n",
    "    # Simulated random noise for all T\n",
    "    simulated_v = stats.multivariate_normal.rvs(\n",
    "        size=10000, mean=np.zeros(T), cov=np.eye(T) * sigma2v, random_state=123\n",
    "    )\n",
    "    simulated_eps = simulated_v - simulated_u\n",
    "\n",
    "    # Rule of thumb bandwidth\n",
    "    h_eps = (\n",
    "        1.06\n",
    "        * 10000 ** (-1 / 5)\n",
    "        * (max(np.std(simulated_eps), stats.iqr(simulated_eps) / 1.34))\n",
    "    )\n",
    "\n",
    "    u_hat = np.zeros((N, T))\n",
    "    for i in range(N):\n",
    "        panel_i_kernel_regression_results = np.zeros(T)\n",
    "        eps_kernel = np.zeros((10000, T))\n",
    "        # Construct the kernel distances for all T time periods\n",
    "        for t in range(T):\n",
    "            eps_kernel[:, t] = stats.norm.pdf(\n",
    "                (simulated_eps[:, t] - obs_eps[i, t]) / h_eps\n",
    "            )\n",
    "        kernel_product = np.prod(eps_kernel, 1)\n",
    "        for j in range(T):\n",
    "            panel_i_kernel_regression_results[j] = np.sum(\n",
    "                kernel_product * simulated_u[:, j]\n",
    "            ) / np.sum(kernel_product)\n",
    "        u_hat[i, :] = panel_i_kernel_regression_results\n",
    "\n",
    "    return u_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba9ae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nadaraya Watson Scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py:343: FutureWarning:\n",
      "\n",
      "In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Fractile</th>\n",
       "      <th>Std(u)</th>\n",
       "      <th>Mean(u)</th>\n",
       "      <th>t=1</th>\n",
       "      <th>t=2</th>\n",
       "      <th>t=3</th>\n",
       "      <th>t=4</th>\n",
       "      <th>t=5</th>\n",
       "      <th>t=6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.245873</td>\n",
       "      <td>0.403594</td>\n",
       "      <td>0.024135</td>\n",
       "      <td>0.318582</td>\n",
       "      <td>0.738054</td>\n",
       "      <td>0.688861</td>\n",
       "      <td>0.270232</td>\n",
       "      <td>0.381701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.169122</td>\n",
       "      <td>0.239246</td>\n",
       "      <td>0.052832</td>\n",
       "      <td>0.124861</td>\n",
       "      <td>0.509873</td>\n",
       "      <td>0.430818</td>\n",
       "      <td>0.138350</td>\n",
       "      <td>0.178745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.172628</td>\n",
       "      <td>0.251123</td>\n",
       "      <td>0.096750</td>\n",
       "      <td>0.066012</td>\n",
       "      <td>0.585722</td>\n",
       "      <td>0.327563</td>\n",
       "      <td>0.202194</td>\n",
       "      <td>0.228496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.127765</td>\n",
       "      <td>0.243374</td>\n",
       "      <td>0.158353</td>\n",
       "      <td>0.491428</td>\n",
       "      <td>0.309240</td>\n",
       "      <td>0.098721</td>\n",
       "      <td>0.214911</td>\n",
       "      <td>0.187592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.187637</td>\n",
       "      <td>0.358608</td>\n",
       "      <td>0.195284</td>\n",
       "      <td>0.089106</td>\n",
       "      <td>0.250239</td>\n",
       "      <td>0.535277</td>\n",
       "      <td>0.505213</td>\n",
       "      <td>0.576530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>94</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.145152</td>\n",
       "      <td>0.196747</td>\n",
       "      <td>0.227607</td>\n",
       "      <td>0.125899</td>\n",
       "      <td>0.506787</td>\n",
       "      <td>0.104959</td>\n",
       "      <td>0.108918</td>\n",
       "      <td>0.106310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>111</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.100962</td>\n",
       "      <td>0.287877</td>\n",
       "      <td>0.290810</td>\n",
       "      <td>0.273255</td>\n",
       "      <td>0.249796</td>\n",
       "      <td>0.501644</td>\n",
       "      <td>0.218975</td>\n",
       "      <td>0.192783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.235708</td>\n",
       "      <td>0.332172</td>\n",
       "      <td>0.339868</td>\n",
       "      <td>0.141119</td>\n",
       "      <td>0.475475</td>\n",
       "      <td>0.772116</td>\n",
       "      <td>0.095174</td>\n",
       "      <td>0.169282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>145</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.225826</td>\n",
       "      <td>0.392722</td>\n",
       "      <td>0.466092</td>\n",
       "      <td>0.489747</td>\n",
       "      <td>0.082886</td>\n",
       "      <td>0.791562</td>\n",
       "      <td>0.298176</td>\n",
       "      <td>0.227871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>162</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.222817</td>\n",
       "      <td>0.544130</td>\n",
       "      <td>0.667975</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.315315</td>\n",
       "      <td>0.917145</td>\n",
       "      <td>0.307852</td>\n",
       "      <td>0.396496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrr}\n",
       "\\toprule\n",
       "{} &  Rank &  Fractile &    Std(u) &   Mean(u) &       t=1 &       t=2 &       t=3 &       t=4 &       t=5 &       t=6 \\\\\n",
       "\\midrule\n",
       "0 &     9 &      0.05 &  0.245873 &  0.403594 &  0.024135 &  0.318582 &  0.738054 &  0.688861 &  0.270232 &  0.381701 \\\\\n",
       "1 &    26 &      0.15 &  0.169122 &  0.239246 &  0.052832 &  0.124861 &  0.509873 &  0.430818 &  0.138350 &  0.178745 \\\\\n",
       "2 &    43 &      0.25 &  0.172628 &  0.251123 &  0.096750 &  0.066012 &  0.585722 &  0.327563 &  0.202194 &  0.228496 \\\\\n",
       "3 &    60 &      0.35 &  0.127765 &  0.243374 &  0.158353 &  0.491428 &  0.309240 &  0.098721 &  0.214911 &  0.187592 \\\\\n",
       "4 &    77 &      0.45 &  0.187637 &  0.358608 &  0.195284 &  0.089106 &  0.250239 &  0.535277 &  0.505213 &  0.576530 \\\\\n",
       "5 &    94 &      0.55 &  0.145152 &  0.196747 &  0.227607 &  0.125899 &  0.506787 &  0.104959 &  0.108918 &  0.106310 \\\\\n",
       "6 &   111 &      0.65 &  0.100962 &  0.287877 &  0.290810 &  0.273255 &  0.249796 &  0.501644 &  0.218975 &  0.192783 \\\\\n",
       "7 &   128 &      0.75 &  0.235708 &  0.332172 &  0.339868 &  0.141119 &  0.475475 &  0.772116 &  0.095174 &  0.169282 \\\\\n",
       "8 &   145 &      0.85 &  0.225826 &  0.392722 &  0.466092 &  0.489747 &  0.082886 &  0.791562 &  0.298176 &  0.227871 \\\\\n",
       "9 &   162 &      0.95 &  0.222817 &  0.544130 &  0.667975 &  0.660000 &  0.315315 &  0.917145 &  0.307852 &  0.396496 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "   Rank  Fractile    Std(u)   Mean(u)       t=1       t=2       t=3       t=4  \\\n",
       "0     9      0.05  0.245873  0.403594  0.024135  0.318582  0.738054  0.688861   \n",
       "1    26      0.15  0.169122  0.239246  0.052832  0.124861  0.509873  0.430818   \n",
       "2    43      0.25  0.172628  0.251123  0.096750  0.066012  0.585722  0.327563   \n",
       "3    60      0.35  0.127765  0.243374  0.158353  0.491428  0.309240  0.098721   \n",
       "4    77      0.45  0.187637  0.358608  0.195284  0.089106  0.250239  0.535277   \n",
       "5    94      0.55  0.145152  0.196747  0.227607  0.125899  0.506787  0.104959   \n",
       "6   111      0.65  0.100962  0.287877  0.290810  0.273255  0.249796  0.501644   \n",
       "7   128      0.75  0.235708  0.332172  0.339868  0.141119  0.475475  0.772116   \n",
       "8   145      0.85  0.225826  0.392722  0.466092  0.489747  0.082886  0.791562   \n",
       "9   162      0.95  0.222817  0.544130  0.667975  0.660000  0.315315  0.917145   \n",
       "\n",
       "        t=5       t=6  \n",
       "0  0.270232  0.381701  \n",
       "1  0.138350  0.178745  \n",
       "2  0.202194  0.228496  \n",
       "3  0.214911  0.187592  \n",
       "4  0.505213  0.576530  \n",
       "5  0.108918  0.106310  \n",
       "6  0.218975  0.192783  \n",
       "7  0.095174  0.169282  \n",
       "8  0.298176  0.227871  \n",
       "9  0.307852  0.396496  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py:343: FutureWarning:\n",
      "\n",
      "In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Std(u)</th>\n",
       "      <th>Mean(u)</th>\n",
       "      <th>t=1</th>\n",
       "      <th>t=2</th>\n",
       "      <th>t=3</th>\n",
       "      <th>t=4</th>\n",
       "      <th>t=5</th>\n",
       "      <th>t=6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.183349</td>\n",
       "      <td>0.324959</td>\n",
       "      <td>0.251971</td>\n",
       "      <td>0.278001</td>\n",
       "      <td>0.402339</td>\n",
       "      <td>0.516867</td>\n",
       "      <td>0.235999</td>\n",
       "      <td>0.264581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrr}\n",
       "\\toprule\n",
       "{} &    Std(u) &   Mean(u) &       t=1 &       t=2 &       t=3 &       t=4 &       t=5 &       t=6 \\\\\n",
       "\\midrule\n",
       "Average &  0.183349 &  0.324959 &  0.251971 &  0.278001 &  0.402339 &  0.516867 &  0.235999 &  0.264581 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "           Std(u)   Mean(u)       t=1       t=2       t=3       t=4       t=5  \\\n",
       "Average  0.183349  0.324959  0.251971  0.278001  0.402339  0.516867  0.235999   \n",
       "\n",
       "              t=6  \n",
       "Average  0.264581  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ricefarm = pd.read_csv(r\"ricefarm.csv\")\n",
    "\n",
    "ricefarm = ricefarm.iloc[:, [0, 1, 3, 5, 6, 7, 8, 14, 16]]\n",
    "\n",
    "# Create ID dummies\n",
    "dar = np.round(ricefarm[\"ID\"] / 100000).astype(int)\n",
    "id_dummies = pd.get_dummies(dar, prefix=\"DR\").astype(int)\n",
    "ricefarm = pd.concat([ricefarm, id_dummies.iloc[:, :-1]], axis=1)\n",
    "\n",
    "# Create rice variety dummies\n",
    "rice_dummies = pd.get_dummies(ricefarm.iloc[:, 2], prefix=\"VAR\").astype(int)\n",
    "ricefarm = pd.concat([ricefarm, rice_dummies.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Recode TSP as logs and zeros\n",
    "ricefarm[ricefarm.columns[5]] = np.where(\n",
    "    ricefarm.iloc[:, 5] > 0, np.log(ricefarm.iloc[:, 5]), 0\n",
    ")\n",
    "\n",
    "# Convert pesticide usage to a dummy\n",
    "ricefarm[ricefarm.columns[6]] = (ricefarm.iloc[:, 6] != 0).astype(int)\n",
    "\n",
    "# Reorder the data\n",
    "ricefarm = ricefarm.iloc[:, [8, 3, 4, 5, 7, 1, 6, 14, 15, 9, 10, 11, 12, 13]]\n",
    "\n",
    "y = np.log(ricefarm.iloc[:, 0].values)\n",
    "X = ricefarm.iloc[:, 1:].values\n",
    "\n",
    "# Log covariates\n",
    "X[:, 0:2] = np.log(X[:, 0:2])\n",
    "X[:, 3] = np.log(X[:, 3])\n",
    "X[:, 4] = np.log(X[:, 4])\n",
    "\n",
    "# Cross sections of  y and X\n",
    "N = 171\n",
    "T = 6\n",
    "y_t = []\n",
    "X_t = []\n",
    "for t in range(T):\n",
    "    if t == 0:\n",
    "        y_t.append(y[:N])\n",
    "        X_t.append(X[:N, :])\n",
    "    else:\n",
    "        y_t.append(y[t * N : (t + 1) * N])\n",
    "        X_t.append(X[t * N : (t + 1) * N, :])\n",
    "        \n",
    "# Import the model parameters from exercise 5.8\n",
    "theta = np.loadtxt(\"exercise_4_8_theta_python.txt\", delimiter=\",\")\n",
    "gamma = np.loadtxt(\"exercise_4_8_gamma_python.txt\", delimiter=\",\")\n",
    "\n",
    "# Estimation of technical inefficiencies\n",
    "# Get idx of percentile firms\n",
    "percetile_firm_idx = np.round(np.arange(0.05, 1, 0.1) * N).astype(int)\n",
    "\n",
    "# NW technical inefficiency estimates\n",
    "Rho_hat = inverse_mapping_vec(gamma)\n",
    "# Draw T dependent unfirm RV's for each s_{1}, ... 10000\n",
    "Z = stats.multivariate_normal.rvs(\n",
    "    size=10000, mean=np.zeros(T), cov=np.eye(T), random_state=1234\n",
    ")\n",
    "A = np.linalg.cholesky(Rho_hat)\n",
    "U_hat = stats.norm.cdf(\n",
    "    Z @ A, loc=np.zeros(T), scale=np.ones(T)\n",
    ")  # Dependent uniform RVs from a gaussian copula\n",
    "NW_u_hat = NW_panel_technical_inefficiency_scores(\n",
    "    theta=theta, y=y_t, X=X_t, U_hat=U_hat\n",
    ")\n",
    "\n",
    "sort_idx = NW_u_hat[:, 0].argsort()\n",
    "# Sort technical inefficiency scores by t=1 values\n",
    "sorted_t1_NW_u_hat = NW_u_hat[sort_idx]\n",
    "# Mean technical inefficiency for each firms over all t\n",
    "NW_mean = np.mean(NW_u_hat, axis=1)\n",
    "sorted_t1_NW_mean = NW_mean[sort_idx]\n",
    "# Standard deviation of technical inefficiency for each firm over all t\n",
    "NW_std = np.std(NW_u_hat, axis=1)\n",
    "sorted_t1_NW_std = NW_std[sort_idx]\n",
    "\n",
    "NW_u_hat_for_table = sorted_t1_NW_u_hat[percetile_firm_idx-1, :]\n",
    "NW_std_u_hat_for_table = sorted_t1_NW_std[percetile_firm_idx-1]\n",
    "NW_mean_u_hat_for_table = sorted_t1_NW_mean[percetile_firm_idx-1]\n",
    "\n",
    "NW_technical_inefficiency_results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Rank\",\n",
    "        \"Fractile\",\n",
    "        \"Std(u)\",\n",
    "        \"Mean(u)\",\n",
    "        \"t=1\",\n",
    "        \"t=2\",\n",
    "        \"t=3\",\n",
    "        \"t=4\",\n",
    "        \"t=5\",\n",
    "        \"t=6\",\n",
    "    ]\n",
    ")\n",
    "NW_technical_inefficiency_results[\"Rank\"] = percetile_firm_idx\n",
    "NW_technical_inefficiency_results[\"Fractile\"] = np.arange(0.05, 1, 0.1)\n",
    "NW_technical_inefficiency_results[\"Std(u)\"] = NW_std_u_hat_for_table\n",
    "NW_technical_inefficiency_results[\"Mean(u)\"] = NW_mean_u_hat_for_table\n",
    "NW_technical_inefficiency_results.iloc[:, -6:] = NW_u_hat_for_table\n",
    "\n",
    "# Compute averages\n",
    "average_NW = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            np.mean(NW_std_u_hat_for_table).reshape(-1, 1).T,\n",
    "            np.mean(NW_mean_u_hat_for_table).reshape(-1, 1).T,\n",
    "            np.mean(NW_u_hat_for_table, axis=0).reshape(-1, 1).T,\n",
    "        ],\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"Std(u)\", \"Mean(u)\", \"t=1\", \"t=2\", \"t=3\", \"t=4\", \"t=5\", \"t=6\"],\n",
    "    index=[\"Average\"],\n",
    ")\n",
    "\n",
    "print(\"Nadaraya Watson Scores\")\n",
    "display(NW_technical_inefficiency_results)\n",
    "display(average_NW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df564111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0303b",
   "metadata": {},
   "source": [
    "## Exercise 7.3: Improved JLMS estimator for the 72 US utilities\n",
    "This exercise applies the predictors from Sections 7.4.2 and 7.6.1 to the US electricity generation dataset comprised of 72 firms over the period 1986-1999. The data file is steamelectric.xlsx. The output variable is net steam electric power in megawatt-hours and the values of the three inputs (fuel, labour and maintenance, and capital) are obtained by dividing respective expenses by relevant input prices; the data are cleaned following Amsler et al. (2021). The price of fuel aggregate is a Tornqvist price index of fuels. The aggregate price of labor and maintenance is a cost-share weighted price, where the price of labor is a company-wide average wage rate and the price of maintenance and other supplies is a price index of electrical supplies from the U.S. Bureau of Labor Statistics. The price of capital is the yield of the firm’s latest issue of long term debt adjusted for appreciation and depreciation. This data has recently been used by Amsler et al. (2021) and Lai and Kumbhakar (2019).\n",
    "\n",
    "The first table reports the average estimated technical inefficiencies by the year as well as the average of their estimated standard deviations, which can be viewed as measures of the unexplained variation in $u$. (We use a different random seed to Amsler et al. (2023) so our estimates are different from theirs.) Table 7.3 contains three predictors based on the panel model where the model parameters are estimated by MSLE. The JLMS estimator uses only the contemporaneous $\\epsilon_{it}$ in the conditioning set. The Nadaraya-Watson [NW] and Local Linear Forest [LLF] estimators use the expanded conditioning set and the APS14 approach to estimating the conditional expectation, described in Section 7.4.2. The second table contains three estimators based on the model with endogeneity where the parameters of the model are estimated by MSLE. For this part, we ignore the panel nature of the dataset (by viewing it as a pooled cross-section, i.e. by assuming independence over t as well as over i) and focus on the presence of the two types of inefficiency. In this case, JLMS is the estimator that does not use the $\\omega_{j}’s while NW and LLF include them in the conditioning set using the APS16 approach described in Section 7.6.1.\n",
    "\n",
    "In the second table the average technical inefficiency score over all firms and years is similar across estimators, regardless of whether or not we condition on the allocative inefficiency terms. Over all years, the average standard deviation of technical inefficiency scores from the Local Linear Forest is considerably smaller than those from the JLMS or Nadaraya-Watson estimators. In Table 7.3, the mean technical inefficiency scores are always larger for all estimators and years in the data. Again, we find that the average technical inefficiency score over all firms and years is similar for all estimators. However, there are noticeable differences in the mean technical inefficiency score of estimators between years, for example in years 86, 87, 91 and 98. Finally, we note that the mean standard deviations for both non-parametric estimators are remarkably smaller than those associated with the JLMS estimator, suggesting that we explain a much larger portion of the unconditional variation in u. The technical inefficiency scores computed from the Local Linear Forest estimator are always associated with the smallest standard deviations.\n",
    "\n",
    "Note: Users should uncomment the line calling the subprocess and change the file path to their own R executable location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee1dbb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.linalg import expm, logm, norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def direct_mapping_mat(C):\n",
    "    gamma = []\n",
    "\n",
    "    try:\n",
    "        # Check if input is of proper format: C is 2D np.array of suitable dimensions\n",
    "        # and is positive-definite correlation matrix\n",
    "        if not isinstance(C, np.ndarray):\n",
    "            raise ValueError\n",
    "        if C.ndim != 2:\n",
    "            raise ValueError\n",
    "        if C.shape[0] != C.shape[1]:\n",
    "            raise ValueError\n",
    "        if not all(\n",
    "            [\n",
    "                np.all(np.abs(np.diag(C) - np.ones(C.shape[0])) < 1e-8),\n",
    "                np.all(np.linalg.eigvals(C) > 0),\n",
    "                np.all(np.abs(C - C.T) < 1e-8),\n",
    "            ]\n",
    "        ):\n",
    "            raise ValueError\n",
    "\n",
    "        # Apply matrix log-transformation to C and get off-diagonal elements\n",
    "        A = logm(C)\n",
    "        gamma = A[np.triu_indices(C.shape[0], 1)]\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Error: input is of a wrong format\")\n",
    "\n",
    "    return gamma\n",
    "\n",
    "\n",
    "def inverse_mapping_vec(gamma, tol_value=1e-8):\n",
    "    C = []\n",
    "    iter_number = -1\n",
    "\n",
    "    # Check if input is of proper format: gamma is of suitable length\n",
    "    if not isinstance(gamma, (np.ndarray, list)):\n",
    "        raise ValueError\n",
    "    if isinstance(gamma, np.ndarray):\n",
    "        if gamma.ndim != 1:\n",
    "            raise ValueError\n",
    "    n = 0.5 * (1 + np.sqrt(1 + 8 * len(gamma)))\n",
    "    if not all([n.is_integer(), n > 1]):\n",
    "        raise ValueError\n",
    "\n",
    "    # Check if tolerance value belongs to a proper interval\n",
    "    # and change it to the default value otherwise\n",
    "    if not (0 < tol_value <= 1e-4):\n",
    "        tol_value = 1e-8\n",
    "        print(\"Warning: tolerance value has been changed to default\")\n",
    "\n",
    "    # Place elements from gamma into off-diagonal parts\n",
    "    # and put zeros on the main diagonal of [n x n] symmetric matrix A\n",
    "    n = int(n)\n",
    "    A = np.zeros(shape=(n, n))\n",
    "    A[np.triu_indices(n, 1)] = gamma\n",
    "    A = A + A.T\n",
    "\n",
    "    # Read properties of the input matrix\n",
    "    diag_vec = np.diag(A)  # get current diagonal\n",
    "    diag_ind = np.diag_indices_from(\n",
    "        A\n",
    "    )  # get row and column indices of diagonal elements\n",
    "\n",
    "    # Iterative algorithm to get the proper diagonal vector\n",
    "    dist = np.sqrt(n)\n",
    "    while dist > np.sqrt(n) * tol_value:\n",
    "        diag_delta = np.log(np.diag(expm(A)))\n",
    "        diag_vec = diag_vec - diag_delta\n",
    "        A[diag_ind] = diag_vec\n",
    "        dist = norm(diag_delta)\n",
    "        iter_number += 1\n",
    "\n",
    "    # Get a unique reciprocal correlation matrix\n",
    "    C = expm(A)\n",
    "    np.fill_diagonal(C, 1)\n",
    "\n",
    "    return C\n",
    "\n",
    "def Loglikelihood_Gaussian_copula_cross_sectional_application_SFA(theta, y, X, P, us_Sxn, n_inputs, S):\n",
    "    \n",
    "    n = len(y)\n",
    "    \n",
    "    alpha = np.exp(theta[0])\n",
    "    beta = np.exp(theta[1:n_inputs+1])\n",
    "    sigma2_v = theta[1+n_inputs]\n",
    "    sigma2_u = theta[2+n_inputs]\n",
    "    sigma2_w = np.exp(theta[3+n_inputs:(3+n_inputs)+(n_inputs-1)])\n",
    "    mu_W = theta[(3+n_inputs)+(n_inputs-2)+1:(3+n_inputs)+(n_inputs-2)+(n_inputs)]\n",
    "\n",
    "    rhos_log_form = theta[-3:]\n",
    "    \n",
    "    Rho_hat = inverse_mapping_vec(rhos_log_form)\n",
    "\n",
    "    # Cobb-Douglas production function\n",
    "    eps = y - np.log(alpha) - X@beta \n",
    "    W = (np.tile(X[:, 0].reshape(-1, 1), (1, n_inputs-1)) - X[:,1:]) - (P[:,1:] - np.tile(P[:, 0].reshape(-1, 1), (1, n_inputs-1)) +(np.log(beta[0]) - np.log(beta[1:])))\n",
    "    # Marginal density of allocative inefficiency terms\n",
    "    Den_W = stats.norm.pdf(W, np.tile(mu_W, (n, 1)), np.tile(np.sqrt(sigma2_w), (n, 1)))\n",
    "    CDF_W = stats.norm.cdf(W, np.tile(mu_W, (n, 1)), np.tile(np.sqrt(sigma2_w), (n, 1)))\n",
    "\n",
    "    eps_Sxn = np.repeat(eps.reshape(-1,1), S, axis=1).T        \n",
    "    us_Sxn_scaled = np.sqrt(sigma2_u)*us_Sxn\n",
    "    CdfUs = 2*(stats.norm.cdf(np.sqrt(sigma2_u)*us_Sxn, 0, np.sqrt(sigma2_u)) -0.5)\n",
    "    eps_plus_us = eps_Sxn + us_Sxn_scaled\n",
    "    den_eps_plus_us = stats.norm.pdf(eps_plus_us, 0, np.sqrt(sigma2_v))\n",
    "\n",
    "    #Evaluate the integral via simulation (to integrate out u from eps+u)\n",
    "    simulated_copula_pdfs = np.zeros((S,n))\n",
    "    CDF_W_rep = {}\n",
    "    for i in range(n_inputs-1):\n",
    "        CDF_W_rep[i] = np.repeat(CDF_W[:, i].reshape(-1,1), S, axis=1).T        \n",
    "        \n",
    "    for j in range(n):\n",
    "        CDF_w_j = np.zeros((S, n_inputs-1))\n",
    "        for i in range(n_inputs-1):\n",
    "            CDF_w_j[:,i] = CDF_W_rep[i][:,j]\n",
    "        U = np.concatenate([stats.norm.ppf(CdfUs[:, j]).reshape(-1,1),\n",
    "                            stats.norm.ppf(CDF_w_j)], \n",
    "                           axis=1)\n",
    "        c123 = stats.multivariate_normal.pdf(U, \n",
    "                                             mean = np.array([0, 0, 0]), \n",
    "                                             cov=Rho_hat)/ np.prod(stats.norm.pdf(U), axis=1)\n",
    "        simulated_copula_pdfs[:,j] = c123\n",
    "\n",
    "    Integral = np.mean(simulated_copula_pdfs*den_eps_plus_us, \n",
    "                       axis=0) #Evaluation of the integral over S simulated samples. Column-wise mean.\n",
    "    # Joint desnity. product of marginal density of w_{i}, i = 1, ..., n_inputs and the joint density f(\\epsilon, w)\n",
    "    prod_Den_W = np.prod(Den_W, 1)\n",
    "    DenAll = prod_Den_W*Integral;\n",
    "    DenAll[DenAll < 1e-6] = 1e-6\n",
    "    r = np.log(np.sum(beta))\n",
    "    logDen = np.log(DenAll) + r\n",
    "    logL = -np.sum(logDen)\n",
    "    \n",
    "    return logL\n",
    "    \n",
    "def estimate_Jondrow1982_u_hat(theta, n_inputs, n_corr_terms, y, X):\n",
    "    \n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:n_inputs+1]\n",
    "    sigma2_v = theta[1+n_inputs]\n",
    "    sigma2_u = theta[2+n_inputs]\n",
    "    \n",
    "    obs_eps = y - np.log(alpha) - X@beta\n",
    "    _lambda = np.sqrt(sigma2_u/sigma2_v)\n",
    "    sigma = np.sqrt(sigma2_u+sigma2_v)\n",
    "    sig_star = np.sqrt(sigma2_u*sigma2_v/(sigma**2))\n",
    "    u_hat = sig_star*(((stats.norm.pdf(_lambda*obs_eps/sigma))/(1-stats.norm.cdf(_lambda*obs_eps/sigma))) - ((_lambda*obs_eps)/sigma))\n",
    "    V_u_hat = sig_star**2*(1+stats.norm.pdf(_lambda*obs_eps/sigma)/(1-stats.norm.cdf(_lambda*obs_eps/sigma))*_lambda*obs_eps/sigma-(stats.norm.pdf(_lambda*obs_eps/sigma)/(1-stats.norm.cdf(_lambda*obs_eps/sigma)))**2)\n",
    "    \n",
    "    return u_hat, V_u_hat\n",
    "\n",
    "def Estimate_Jondrow1982_u_hat_panel_SFA_application_RS2007(alpha, beta, delta, sigma2_v, y, X, T, N):\n",
    "    \n",
    "    obs_eps = {}\n",
    "    u_hat = np.zeros((N, T))\n",
    "    V_u_hat = np.zeros((N, T))\n",
    "    for t in range(T):\n",
    "        sigma2_u = np.exp(delta[0] + delta[1]*t)\n",
    "        _lambda = np.sqrt(sigma2_u)/np.sqrt(sigma2_v)\n",
    "        sigma = np.sqrt(sigma2_u+sigma2_v)\n",
    "        sig_star = np.sqrt(sigma2_u*sigma2_v/(sigma**2))\n",
    "    \n",
    "        u_hat_ = np.full(N, np.nan)\n",
    "        V_u_hat_ = np.full(N, np.nan)\n",
    "        obs_eps[t] = y[t] - np.log(alpha) - X[t]@beta # composed errors from the production function equation (i.e residuals from the production function)\n",
    "        b = (obs_eps[t]*_lambda)/sigma\n",
    "        u_hat_tmp = ((sigma*_lambda)/(1 + _lambda**2))*(stats.norm.pdf(b)/(1 - stats.norm.cdf(b)) - b)\n",
    "        V_u_hat_tmp = sig_star**2*(1+stats.norm.pdf(b)/(1-stats.norm.cdf(b))*b-(stats.norm.pdf(b)/(1-stats.norm.cdf(b)))**2)\n",
    "\n",
    "        u_hat_[:len(u_hat_tmp)] = u_hat_tmp\n",
    "        V_u_hat_[:len(V_u_hat_tmp)] = V_u_hat_tmp\n",
    "        u_hat[:, t] = u_hat_\n",
    "        V_u_hat[:, t] = V_u_hat_\n",
    "        \n",
    "    return u_hat, V_u_hat\n",
    "\n",
    "def simulate_error_components(Rho, n_inputs, S_kernel, seed):\n",
    "    \n",
    "    chol_of_rho = np.linalg.cholesky(Rho)\n",
    "    Z = stats.multivariate_normal.rvs(mean=np.zeros(n_inputs), \n",
    "                                      cov=np.eye(n_inputs), \n",
    "                                      size=S_kernel, \n",
    "                                      random_state=seed)\n",
    "    X = chol_of_rho@Z.T\n",
    "    U = stats.norm.cdf(X)\n",
    "     \n",
    "    return U.T\n",
    "\n",
    "def Estimate_NW_u_hat_conditional_eps_panel_SFA_RS2007(theta, y, X, N, T, k, U_hat, S_kernel):\n",
    "    \n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:n_inputs+1]\n",
    "    delta = theta[4:6]\n",
    "    sigma2_v = theta[6]\n",
    "  \n",
    "    # Observed variables\n",
    "    obs_eps = {}\n",
    "    for t in range(T):\n",
    "        eps__ = np.full(N, np.nan)\n",
    "        tmp_eps = y[t] - np.log(alpha) - X[t]@beta\n",
    "        eps__[:len(tmp_eps)] = tmp_eps\n",
    "        obs_eps[t] = eps__\n",
    "\n",
    "    # Simulated variables\n",
    "    simulated_v = stats.multivariate_normal.rvs(mean=np.zeros(T), \n",
    "                                                cov=np.eye(T)*sigma2_v, \n",
    "                                                size=S_kernel) #simulate random noise for all T panels \n",
    "    simulated_u = np.zeros((S_kernel, T))\n",
    "    simulated_eps = np.zeros((S_kernel, T))\n",
    "    for t in range(T):\n",
    "        sigma2_u = np.exp(delta[0] + delta[1]*t)\n",
    "        simulated_u[:, t] = np.sqrt(sigma2_u)*stats.norm.ppf((U_hat[:,t]+1)/2)\n",
    "        simulated_eps[:,t] = simulated_v[:,t] - simulated_u[:,t]\n",
    "\n",
    "    # Bandwidth information for each conditioning variable\n",
    "    h_eps = np.zeros(T)\n",
    "    for t in range(T):\n",
    "        h_eps[t] = 1.06*S_kernel**(-1/5)*(max(np.std(simulated_eps[:,t]), stats.iqr(simulated_eps[:,t])/1.34))\n",
    "\n",
    "    # kernel estimates for E[u_{t}|eps_{1}, ..., eps_{T}]\n",
    "    # V[u|eps, w1, w2] = E[u^2|w2, w3, eps] - (E[u|w2, w3, eps])^2\n",
    "    u_hat = np.zeros((N, T))\n",
    "    u_hat2 = np.zeros((N, T))\n",
    "    all_eps = np.concatenate([x.reshape(-1,1) for x in obs_eps.values()], axis=1)\n",
    "    for i in range(N): \n",
    "        obs_eps_i = all_eps[i, :]\n",
    "\n",
    "        panel_i_kernel_regression_results = np.zeros(T)\n",
    "        panel_i_kernel_regression_results2 = np.zeros(T)\n",
    "        eps_kernel = np.zeros((S_kernel, T))\n",
    "        # Construct the kernel distances for all T time periods\n",
    "        for t in range(T):\n",
    "            eps_kernel[:,t] = stats.norm.pdf((simulated_eps[:,t] - obs_eps[t][i])/h_eps[t])\n",
    "\n",
    "        out = eps_kernel[:,np.all(~np.isnan(eps_kernel), axis=0)]\n",
    "        kernel_product = np.prod(out, 1)\n",
    "        for j in range(T): # NW for each t = 1, .., T observation in each panel i\n",
    "            if not np.isnan(obs_eps_i[j]):\n",
    "                panel_i_kernel_regression_results[j] = np.sum(kernel_product*simulated_u[:, j])/np.sum(kernel_product)\n",
    "                panel_i_kernel_regression_results2[j] = np.sum(kernel_product*simulated_u[:,j]**2)/np.sum(kernel_product)\n",
    "            else:\n",
    "                panel_i_kernel_regression_results[j] = np.nan\n",
    "                panel_i_kernel_regression_results2[j] = np.nan\n",
    "\n",
    "        u_hat[i, :] = panel_i_kernel_regression_results\n",
    "        u_hat2[i,  :] = panel_i_kernel_regression_results2\n",
    "\n",
    "    V_u_hat = u_hat2 - (u_hat**2)\n",
    "    \n",
    "    return u_hat, V_u_hat\n",
    "\n",
    "def Estimate_NW_u_hat_conditional_W_cross_sectional_application(theta, n_inputs, n_corr_terms, y, X, P, U_hat, S_kernel):\n",
    "    \n",
    "    n = len(y)\n",
    "\n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:n_inputs+1]\n",
    "    sigma2_v = theta[1+n_inputs]\n",
    "    sigma2_u = theta[2+n_inputs]\n",
    "    sigma2_w = np.exp(theta[3+n_inputs:(3+n_inputs)+(n_inputs-1)])\n",
    "    mu_W = theta[(3+n_inputs)+(n_inputs-2)+1:(3+n_inputs)+(n_inputs-2)+(n_inputs)]\n",
    "\n",
    "    # Observed variables\n",
    "    obs_eps = y - np.log(alpha) - X@beta\n",
    "    W = (np.tile(X[:, 0].reshape(-1, 1), (1, n_inputs-1)) - X[:,1:]) - (P[:,1:] - np.tile(P[:, 0].reshape(-1, 1), (1, n_inputs-1)) +(np.log(beta[0]) - np.log(beta[1:])))\n",
    "    rep_obs_eps = np.repeat(obs_eps.reshape(-1,1), S_kernel, axis=1).T        \n",
    "    rep_obs_W = {}\n",
    "    for i in range(n_inputs-1):\n",
    "        rep_obs_W[i] = np.repeat(W[:, i].reshape(-1,1), S_kernel, axis=1).T        \n",
    "\n",
    "    # Simulated variables\n",
    "    simulated_v = stats.norm.rvs(loc=0, scale=np.sqrt(sigma2_v), size=S_kernel)\n",
    "    simulated_u = np.sqrt(sigma2_u)*stats.norm.ppf((U_hat[:, 0]+1)/2) # Simulated half normal rvs\n",
    "    simulated_W = np.zeros((S_kernel, n_inputs-1))\n",
    "    for i in range(n_inputs-1):\n",
    "        simulated_W[:,i] = stats.norm.ppf(U_hat[:,i+1], mu_W[i], np.sqrt(sigma2_w[i]))\n",
    "    simulated_eps = simulated_v - simulated_u\n",
    "\n",
    "    # Bandwidth information for each conditioning variable\n",
    "    h_eps = 1.06*S_kernel**(-1/5)*(max(np.std(simulated_eps), stats.iqr(simulated_eps)/1.34))\n",
    "    h_W = np.zeros(n_inputs-1)\n",
    "    for i in range(n_inputs-1):\n",
    "        h_W[i] = 1.06*S_kernel**(-1/5)*(max(np.std(simulated_W[:,i]), stats.iqr(simulated_W[:,i])/1.34))\n",
    "    h = np.concatenate([np.array([h_eps]), h_W])\n",
    "\n",
    "    # Kernel estimates for E[u|eps, w1, w2]\n",
    "    # V[u|eps, w1, w2] = E[u^2|w2, w3, eps] - (E[u|w2, w3, eps])^2\n",
    "    kernel_regression_results1 = np.zeros(n)\n",
    "    kernel_regression_results2 = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        eps_kernel = stats.norm.pdf((simulated_eps - rep_obs_eps[:,i])/h[0])\n",
    "        W_kernel = np.zeros((S_kernel,n_inputs-1))\n",
    "        for j in range(n_inputs-1):\n",
    "            W_kernel[:,j] = stats.norm.pdf((simulated_W[:,j] - rep_obs_W[j][:,i])/h[j+1])\n",
    "\n",
    "        W_kernel_prod = np.prod(W_kernel, 1)\n",
    "        kernel_product = eps_kernel*W_kernel_prod\n",
    "        kernel_regression_results1[i] = np.sum(kernel_product*simulated_u)/np.sum(kernel_product)\n",
    "        kernel_regression_results2[i] = np.sum(kernel_product*(simulated_u**2))/np.sum(kernel_product)\n",
    "\n",
    "    u_hat = kernel_regression_results1\n",
    "    V_u_hat = kernel_regression_results2 - (u_hat**2)\n",
    "    \n",
    "    return u_hat, V_u_hat\n",
    "\n",
    "def Loglikelihood_APS14_dynamic_panel_SFA_u_RS2007(theta, y, X, N, T, k, S, FMSLE_us):\n",
    "\n",
    "    if np.any(np.isnan(theta)):\n",
    "        logDen = np.ones((n, 1))*-1e8\n",
    "        logL = -np.sum(logDen)\n",
    "    else:\n",
    "        rhos = theta[7:]\n",
    "        Rho = inverse_mapping_vec(rhos) # Gaussian copula correlation matrix \n",
    "       \n",
    "        alpha = np.exp(theta[0])\n",
    "        beta = 1/(1+np.exp(-theta[1:4])) # Inverse logit transform of betas\n",
    "        delta = theta[4:6]\n",
    "        sigma2_v = theta[6]\n",
    "      \n",
    "        eps = {}\n",
    "        for t in range(T):\n",
    "            eps__ = np.full(N, np.nan)\n",
    "            tmp_eps = y[t] - np.log(alpha) - X[t]@beta\n",
    "            eps__[:len(tmp_eps)] = tmp_eps\n",
    "            eps[t] = eps__\n",
    "            \n",
    "        if not np.all(np.linalg.eigvals(Rho) > 0):\n",
    "            logDen = np.ones((n, 1))*-1e8\n",
    "            logL = -np.sum(logDen)\n",
    "        else:\n",
    "            # Evaluate the integral via simulated MLE (FMSLE)\n",
    "            A = np.linalg.cholesky(Rho)\n",
    "            all_eps = np.concatenate([_eps.reshape(-1,1) for _eps in eps.values()], axis=1)\n",
    "            FMSLE_densities = np.zeros(N)\n",
    "            for i in range(N):\n",
    "                eps_i = all_eps[i, :]\n",
    "                n_NaNs = len(eps_i[np.isnan(eps_i)])\n",
    "                eps_i = eps_i[~np.isnan(eps_i)] # remove any NaN from an unbalanced panel\n",
    "                rep_eps_i = np.tile(eps_i, (S, 1))\n",
    "                simulated_us_i = np.zeros((S, T))\n",
    "                for t in range(T):\n",
    "                   simulated_us_i[:, t] = FMSLE_us[t][:, i]\n",
    "        \n",
    "                # Transform simulated values to half-normal RV's\n",
    "                CDF_u_i = stats.norm.cdf(simulated_us_i@A, np.zeros((S, T)), np.ones((S, T)))\n",
    "                sigma2_u_hat = np.exp(delta[0] + delta[1]*np.arange(1, T+1, 1))\n",
    "                u_i = stats.halfnorm.ppf(CDF_u_i, \n",
    "                                         np.zeros((S, T)), \n",
    "                                         np.tile(np.ones((1, T)) * np.sqrt(sigma2_u_hat), (S, 1)))\n",
    "                \n",
    "                # adjust for possible unbalanced panel\n",
    "                u_i = u_i[:,:T-n_NaNs]\n",
    "                # Joint density\n",
    "                den_i = np.mean(stats.multivariate_normal.pdf(rep_eps_i + u_i, \n",
    "                                                              mean=np.zeros(T-n_NaNs), \n",
    "                                                          cov=np.eye(T-n_NaNs)*sigma2_v)) # eq 1 pg. 510 section 5.1 APS14\n",
    "                if den_i < 1e-10:\n",
    "                    den_i = 1e-10\n",
    "                FMSLE_densities[i] = den_i\n",
    "        \n",
    "            logL = -np.sum(np.log(FMSLE_densities))\n",
    "        \n",
    "    return logL\n",
    "\n",
    "def export_simulation_data_RS2007_electricity_application(theta, n_inputs, y, X, P, U_hat, S_kernel):\n",
    "    \n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:n_inputs+1]\n",
    "    sigma2_v = theta[1+n_inputs]\n",
    "    sigma2_u = theta[2+n_inputs]\n",
    "    sigma2_w = np.exp(theta[3+n_inputs:(3+n_inputs)+(n_inputs-1)])\n",
    "    mu_W = theta[(3+n_inputs)+(n_inputs-2)+1:(3+n_inputs)+(n_inputs-2)+(n_inputs)]\n",
    "\n",
    "    # Observed variables\n",
    "    obs_eps = y - np.log(alpha) - X@beta\n",
    "    W = (np.tile(X[:, 0].reshape(-1, 1), (1, n_inputs-1)) - X[:,1:]) - (P[:,1:] - np.tile(P[:, 0].reshape(-1, 1), (1, n_inputs-1)) +(np.log(beta[0]) - np.log(beta[1:])))\n",
    "    rep_obs_W = {}\n",
    "    for i in range(n_inputs-1):\n",
    "        rep_obs_W[i] = np.repeat(W[:, i].reshape(-1,1), S_kernel, axis=1).T        \n",
    "\n",
    "    # Simulated variables\n",
    "    simulated_v = stats.norm.rvs(loc=0, scale=np.sqrt(sigma2_v), size=S_kernel)\n",
    "    simulated_u = np.sqrt(sigma2_u)*stats.norm.ppf((U_hat[:, 0]+1)/2) # Simulated half normal rvs\n",
    "    simulated_W = np.zeros((S_kernel, n_inputs-1))\n",
    "    for i in range(n_inputs-1):\n",
    "        simulated_W[:,i] = stats.norm.ppf(U_hat[:,i+1], mu_W[i], np.sqrt(sigma2_w[i]))\n",
    "    simulated_eps = simulated_v - simulated_u\n",
    "    \n",
    "    # Export\n",
    "    NN_train_eps_W = pd.DataFrame(np.concatenate([simulated_eps.reshape(-1,1), simulated_W], \n",
    "                                                 axis=1), \n",
    "                                  columns=['train_simulated_eps']+[f'train_simulated_w{i+1}' for i in range(n_inputs-1)])\n",
    "    NN_train_u = pd.DataFrame(simulated_u, \n",
    "                              columns=['train_simulated_u'])\n",
    "    NN_test_eps_W = pd.DataFrame(np.concatenate([obs_eps.reshape(-1,1), W], \n",
    "                                                 axis=1), \n",
    "                                  columns=['test_simulated_eps']+[f'test_simulated_w{i+1}' for i in range(n_inputs-1)])\n",
    "\n",
    "    NN_train_eps_W.to_csv(r'./cross_sectional_SFA_RS2007_electricity_application_NN_train_eps_W.csv', \n",
    "                          index=False)\n",
    "    NN_train_u.to_csv(r'./cross_sectional_SFA_RS2007_electricity_application_NN_train_u.csv', \n",
    "                          index=False)\n",
    "    NN_test_eps_W.to_csv(r'./cross_sectional_SFA_RS2007_electricity_application_NN_test_eps_W.csv', \n",
    "                          index=False)\n",
    "\n",
    "def export_RS2007_electricity_SFA_panel_data(theta, y, X, N, T, U_hat, S_kernel):\n",
    "    \n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:4]\n",
    "    delta = theta[4:6]\n",
    "    sigma2_v = theta[6]\n",
    "  \n",
    "    #Observed variables\n",
    "    obs_eps = {}\n",
    "    for t in range(T):\n",
    "        eps__ = np.full(N, np.nan)\n",
    "        tmp_eps = y[t] - np.log(alpha) - X[t]@beta\n",
    "        eps__[:len(tmp_eps)] = tmp_eps\n",
    "        obs_eps[t] = eps__\n",
    "\n",
    "    obs_eps = np.concatenate([x.reshape(-1, 1) for x in obs_eps.values()], axis=1)\n",
    "    \n",
    "    # Simulated variables\n",
    "    simulated_v = stats.multivariate_normal.rvs(np.zeros(T), np.eye(T)*sigma2_v, S_kernel) # simulate random noise for all T panels \n",
    "    simulated_u = np.zeros((S_kernel, T))\n",
    "    simulated_eps = np.zeros((S_kernel, T))\n",
    "    for t in range(T):\n",
    "        sigma2_u = np.exp(delta[0] + delta[1]*t)\n",
    "        simulated_u[:, t] = np.sqrt(sigma2_u)*stats.norm.ppf((U_hat[:, t]+1)/2) # simulated half normal rvs\n",
    "        simulated_eps[:, t] = simulated_v[:, t] - simulated_u[:, t]\n",
    "\n",
    "    NN_train_eps = pd.DataFrame(simulated_eps, \n",
    "                                columns=[f'train_eps_{t}' for t in range(T)])\n",
    "\n",
    "    NN_train_u = pd.DataFrame(simulated_u, \n",
    "                              columns=[f'train_u_{t}' for t in range(T)])\n",
    "\n",
    "    NN_test_eps = pd.DataFrame(obs_eps, \n",
    "                               columns=[f'test_eps_{t}' for t in range(T)])\n",
    "    \n",
    "    NN_train_eps.to_csv(r'./panel_SFA_RS2007_electricty_application_NN_train_eps.csv', \n",
    "                          index=False)\n",
    "    NN_train_u.to_csv(r'./panel_SFA_RS2007_electricty_application_NN_train_u.csv', \n",
    "                          index=False)\n",
    "    NN_test_eps.to_csv(r'./panel_SFA_RS2007_electricty_application_NN_test_eps.csv', \n",
    "                          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a28c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Sectional Models (APS16 Estimator)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py:343: FutureWarning:\n",
      "\n",
      "In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>JLMS Est.</th>\n",
       "      <th>JLMS Std Dev</th>\n",
       "      <th>NW Est.</th>\n",
       "      <th>NW Std Dev</th>\n",
       "      <th>LLF Est.</th>\n",
       "      <th>LLF Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.0</td>\n",
       "      <td>0.450176</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.447624</td>\n",
       "      <td>0.112284</td>\n",
       "      <td>0.456667</td>\n",
       "      <td>0.006001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.0</td>\n",
       "      <td>0.387181</td>\n",
       "      <td>0.107435</td>\n",
       "      <td>0.390395</td>\n",
       "      <td>0.107120</td>\n",
       "      <td>0.396716</td>\n",
       "      <td>0.005560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.0</td>\n",
       "      <td>0.378763</td>\n",
       "      <td>0.107390</td>\n",
       "      <td>0.380626</td>\n",
       "      <td>0.109197</td>\n",
       "      <td>0.393009</td>\n",
       "      <td>0.005495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.0</td>\n",
       "      <td>0.331737</td>\n",
       "      <td>0.104838</td>\n",
       "      <td>0.336866</td>\n",
       "      <td>0.107073</td>\n",
       "      <td>0.346487</td>\n",
       "      <td>0.005466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.367386</td>\n",
       "      <td>0.105288</td>\n",
       "      <td>0.370492</td>\n",
       "      <td>0.108393</td>\n",
       "      <td>0.380370</td>\n",
       "      <td>0.005408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>91.0</td>\n",
       "      <td>0.373536</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.375362</td>\n",
       "      <td>0.107794</td>\n",
       "      <td>0.386282</td>\n",
       "      <td>0.005562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>92.0</td>\n",
       "      <td>0.396527</td>\n",
       "      <td>0.104903</td>\n",
       "      <td>0.392474</td>\n",
       "      <td>0.109243</td>\n",
       "      <td>0.406717</td>\n",
       "      <td>0.005638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93.0</td>\n",
       "      <td>0.406144</td>\n",
       "      <td>0.105184</td>\n",
       "      <td>0.399113</td>\n",
       "      <td>0.106681</td>\n",
       "      <td>0.412800</td>\n",
       "      <td>0.005704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>94.0</td>\n",
       "      <td>0.384264</td>\n",
       "      <td>0.102811</td>\n",
       "      <td>0.385815</td>\n",
       "      <td>0.104459</td>\n",
       "      <td>0.396918</td>\n",
       "      <td>0.005171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>95.0</td>\n",
       "      <td>0.398865</td>\n",
       "      <td>0.103161</td>\n",
       "      <td>0.401284</td>\n",
       "      <td>0.103416</td>\n",
       "      <td>0.412667</td>\n",
       "      <td>0.005573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>96.0</td>\n",
       "      <td>0.372470</td>\n",
       "      <td>0.099701</td>\n",
       "      <td>0.371540</td>\n",
       "      <td>0.100064</td>\n",
       "      <td>0.384600</td>\n",
       "      <td>0.005650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>97.0</td>\n",
       "      <td>0.341549</td>\n",
       "      <td>0.096496</td>\n",
       "      <td>0.346991</td>\n",
       "      <td>0.099792</td>\n",
       "      <td>0.354684</td>\n",
       "      <td>0.005437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>98.0</td>\n",
       "      <td>0.349710</td>\n",
       "      <td>0.096732</td>\n",
       "      <td>0.353238</td>\n",
       "      <td>0.096877</td>\n",
       "      <td>0.361774</td>\n",
       "      <td>0.005206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrr}\n",
       "\\toprule\n",
       "{} &  Year &  JLMS Est. &  JLMS Std Dev &   NW Est. &  NW Std Dev &  LLF Est. &  LLF Std Dev \\\\\n",
       "\\midrule\n",
       "0  &  86.0 &   0.450176 &      0.110600 &  0.447624 &    0.112284 &  0.456667 &     0.006001 \\\\\n",
       "1  &  87.0 &   0.387181 &      0.107435 &  0.390395 &    0.107120 &  0.396716 &     0.005560 \\\\\n",
       "2  &  88.0 &   0.378763 &      0.107390 &  0.380626 &    0.109197 &  0.393009 &     0.005495 \\\\\n",
       "3  &  89.0 &   0.331737 &      0.104838 &  0.336866 &    0.107073 &  0.346487 &     0.005466 \\\\\n",
       "4  &  90.0 &   0.367386 &      0.105288 &  0.370492 &    0.108393 &  0.380370 &     0.005408 \\\\\n",
       "5  &  91.0 &   0.373536 &      0.105500 &  0.375362 &    0.107794 &  0.386282 &     0.005562 \\\\\n",
       "6  &  92.0 &   0.396527 &      0.104903 &  0.392474 &    0.109243 &  0.406717 &     0.005638 \\\\\n",
       "7  &  93.0 &   0.406144 &      0.105184 &  0.399113 &    0.106681 &  0.412800 &     0.005704 \\\\\n",
       "8  &  94.0 &   0.384264 &      0.102811 &  0.385815 &    0.104459 &  0.396918 &     0.005171 \\\\\n",
       "9  &  95.0 &   0.398865 &      0.103161 &  0.401284 &    0.103416 &  0.412667 &     0.005573 \\\\\n",
       "10 &  96.0 &   0.372470 &      0.099701 &  0.371540 &    0.100064 &  0.384600 &     0.005650 \\\\\n",
       "11 &  97.0 &   0.341549 &      0.096496 &  0.346991 &    0.099792 &  0.354684 &     0.005437 \\\\\n",
       "12 &  98.0 &   0.349710 &      0.096732 &  0.353238 &    0.096877 &  0.361774 &     0.005206 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "    Year  JLMS Est.  JLMS Std Dev   NW Est.  NW Std Dev  LLF Est.  LLF Std Dev\n",
       "0   86.0   0.450176      0.110600  0.447624    0.112284  0.456667     0.006001\n",
       "1   87.0   0.387181      0.107435  0.390395    0.107120  0.396716     0.005560\n",
       "2   88.0   0.378763      0.107390  0.380626    0.109197  0.393009     0.005495\n",
       "3   89.0   0.331737      0.104838  0.336866    0.107073  0.346487     0.005466\n",
       "4   90.0   0.367386      0.105288  0.370492    0.108393  0.380370     0.005408\n",
       "5   91.0   0.373536      0.105500  0.375362    0.107794  0.386282     0.005562\n",
       "6   92.0   0.396527      0.104903  0.392474    0.109243  0.406717     0.005638\n",
       "7   93.0   0.406144      0.105184  0.399113    0.106681  0.412800     0.005704\n",
       "8   94.0   0.384264      0.102811  0.385815    0.104459  0.396918     0.005171\n",
       "9   95.0   0.398865      0.103161  0.401284    0.103416  0.412667     0.005573\n",
       "10  96.0   0.372470      0.099701  0.371540    0.100064  0.384600     0.005650\n",
       "11  97.0   0.341549      0.096496  0.346991    0.099792  0.354684     0.005437\n",
       "12  98.0   0.349710      0.096732  0.353238    0.096877  0.361774     0.005206"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py:343: FutureWarning:\n",
      "\n",
      "In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JLMS Est.</th>\n",
       "      <th>JLMS Std Dev</th>\n",
       "      <th>NW Est.</th>\n",
       "      <th>NW Std Dev</th>\n",
       "      <th>LLF Est.</th>\n",
       "      <th>LLF Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.379943</td>\n",
       "      <td>0.103849</td>\n",
       "      <td>0.380975</td>\n",
       "      <td>0.105569</td>\n",
       "      <td>0.391586</td>\n",
       "      <td>0.005529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrr}\n",
       "\\toprule\n",
       "{} &  JLMS Est. &  JLMS Std Dev &   NW Est. &  NW Std Dev &  LLF Est. &  LLF Std Dev \\\\\n",
       "\\midrule\n",
       "Average &   0.379943 &      0.103849 &  0.380975 &    0.105569 &  0.391586 &     0.005529 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "         JLMS Est.  JLMS Std Dev   NW Est.  NW Std Dev  LLF Est.  LLF Std Dev\n",
       "Average   0.379943      0.103849  0.380975    0.105569  0.391586     0.005529"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_excel('steamelectric.xlsx')\n",
    "\n",
    "data['fuel_price_index'] = np.nan\n",
    "data.iloc[:-1, -1] = data.iloc[:-1, 2].values/data.iloc[1:,2].values\n",
    "data['LM_price_index'] = np.nan\n",
    "data.iloc[:-1, -1] = data.iloc[:-1, 3].values/data.iloc[1:,3].values\n",
    "\n",
    "data.iloc[13:1007:14, :] = np.nan #drop last fuel price index for each plant\n",
    "data = data.dropna()\n",
    "\n",
    "X1 = (data['Fuel Costs ($1000)']/data['fuel_price_index'])*1000*1e-6; #fuel costs over price index\n",
    "X2 = data['Operating Costs ($1000)']/data['LM_price_index']*1000*1e-6; #LM costs over price index\n",
    "X3 = data['Capital ($1000)']/1000; # capital\n",
    "\n",
    "X = np.log(pd.concat([X1, X2, X3], axis=1))\n",
    "P = np.log(data[['fuel_price_index', 'LM_price_index', 'User Cost of Capital']])\n",
    "y = np.log(data['Output (MWhr)']*1e-6) # output ml MWpH \n",
    "\n",
    "# Estimate cross sectional models\n",
    "n = len(y)\n",
    "S = 500 # Number of Halton draws used in maximum simulated likelihood\n",
    "S_kernel = 10000; # Number of simulated draws for evaluation of the conditional expectation\n",
    "n_inputs = 3\n",
    "n_corr_terms = ((n_inputs)^2-(n_inputs))/2 # Number of off diagonal lower triangular correlation/covariance terms for Gaussian copula correlation matrix. \n",
    "\n",
    "initial_lalpha = -2.6\n",
    "initial_beta = np.array([0.5, 0.3, 0.3])\n",
    "initial_lbeta = np.log(initial_beta)\n",
    "initial_logit_beta = np.log(initial_beta/(1-initial_beta))\n",
    "initial_sigma2_v = 0.015\n",
    "initial_lsigma2_v = np.log(initial_sigma2_v)\n",
    "initial_sigma2_u = 0.15\n",
    "initial_lsigma2_u = np.log(initial_sigma2_u)\n",
    "initial_mu_W = np.array([0.5, 0])\n",
    "\n",
    "sampler = stats.qmc.Halton(d=1, \n",
    "                           scramble=True, \n",
    "                           seed=123)\n",
    "sample = sampler.random(n=n)\n",
    "us_ = stats.norm.ppf((sample+1)/2, 0, 1)\n",
    "us_Sxn = np.reshape(np.repeat(us_[:, np.newaxis], S, axis=1), (S, n))\n",
    "\n",
    "# Gaussian Copula\n",
    "initial_Sigma = np.array([[0.2, 0.09], \n",
    "                          [0.09, 0.2]])\n",
    "initial_sigma2_w = np.diag(initial_Sigma)\n",
    "initial_lsigma2_w = np.log(initial_sigma2_w)\n",
    "\n",
    "eps_ = y - initial_lalpha - X@initial_beta\n",
    "W_ = (np.tile(X.iloc[:, 0].values.reshape(-1, 1), (1, n_inputs-1)) - X.iloc[:,1:].values) - (P.iloc[:,1:].values - np.tile(P.iloc[:, 0].values.reshape(-1, 1), (1, n_inputs-1)) +(np.log(initial_beta[0]) - np.log(initial_beta[1:])))\n",
    "initial_Rho = np.corrcoef(np.concatenate([eps_.values.reshape(-1,1), W_], axis=1).T)\n",
    "initial_lRho = direct_mapping_mat(initial_Rho)\n",
    "\n",
    "Gaussian_copula_theta0 = np.concatenate([np.array([initial_lalpha]), initial_lbeta, np.array([initial_sigma2_v, initial_sigma2_u]), \n",
    "                                         initial_lsigma2_w, initial_mu_W, initial_lRho])\n",
    "\n",
    "# Bounds to ensure sigma2v and sigma2u are >= 0\n",
    "bounds = [(None, None) for x in range(4)] + [\n",
    "    (1e-5, np.inf),\n",
    "    (1e-5, np.inf),\n",
    "] + [(None, None) for x in range(7)]\n",
    "\n",
    "# Minimize the negative log-likelihood using numerical optimization.\n",
    "MLE_results = minimize(\n",
    "    fun=Loglikelihood_Gaussian_copula_cross_sectional_application_SFA,\n",
    "    x0=Gaussian_copula_theta0,\n",
    "    method=\"L-BFGS-B\",\n",
    "    tol = 1e-6,\n",
    "    options={\"ftol\": 1e-6, \"maxiter\": 1000, \"maxfun\": 6*1000},\n",
    "    args=(y.values, X.values, P.values, us_Sxn, n_inputs, S),\n",
    "    bounds=bounds,\n",
    ")\n",
    "\n",
    "Gaussian_copula_theta = MLE_results.x\n",
    "logMLE = MLE_results.fun * -1  # Log-likelihood at the solution\n",
    "\n",
    "# Transform parameters\n",
    "Gaussian_copula_theta[:4] = np.exp(Gaussian_copula_theta[:4]) # Transform production system coefficients\n",
    "Gaussian_copula_theta[6:8] = np.exp(Gaussian_copula_theta[6:8])\n",
    "rhos_log_form = Gaussian_copula_theta[-3:]\n",
    "Rho = inverse_mapping_vec(rhos_log_form)\n",
    "Gaussian_copula_theta[-3:] = direct_mapping_mat(initial_Rho)\n",
    "Rho_lower_trianglular = Rho[np.tril_indices(Rho.shape[1], 1)]\n",
    "\n",
    "np.savetxt(r'./cross_sectional_SFA_RS2007_electricity_application_gaussian_copula_correlation_matrix.csv', \n",
    "              Rho, \n",
    "              delimiter=',')\n",
    "\n",
    "# JLMS technical inefficiency scores\n",
    "(Gaussian_copula_JLMS_u_hat, \n",
    " Gaussian_copula_JLMS_V_u_hat) = estimate_Jondrow1982_u_hat(theta=Gaussian_copula_theta, \n",
    "                                                            n_inputs=n_inputs, \n",
    "                           n_corr_terms=n_corr_terms, \n",
    "                           y=y.values, \n",
    "                           X=X.values)\n",
    "JLMS_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_JLMS_u_hat.reshape(-1,1)], axis=1)\n",
    "JLMS_V_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_JLMS_V_u_hat.reshape(-1,1)], axis=1)\n",
    "JLMS_u_hat_year_mean = np.zeros((13, 2))\n",
    "JLMS_V_u_hat_year_mean = np.zeros((13, 2))\n",
    "JLMS_std_u_hat_year_mean = np.zeros((13, 2))\n",
    "for i, t in enumerate(range(86, 99)):\n",
    "    JLMS_u_hat_year_mean[i, 0] = t\n",
    "    JLMS_V_u_hat_year_mean[i, 0] = t\n",
    "    JLMS_std_u_hat_year_mean[i, 0] = t\n",
    "    JLMS_u_hat_year_mean[i, 1] = np.mean(JLMS_u_hat_matrix[JLMS_u_hat_matrix[:,0] == t, 1])\n",
    "    JLMS_V_u_hat_year_mean[i, 1] = np.mean(JLMS_V_u_hat_matrix[JLMS_V_u_hat_matrix[:,0] == t, 1])\n",
    "    JLMS_std_u_hat_year_mean[i, 1:] = np.mean(np.sqrt(JLMS_V_u_hat_matrix[JLMS_V_u_hat_matrix[:,0] == t, 1]))\n",
    "\n",
    "# Copula Nadaraya Watson Scores\n",
    "Gaussian_copula_U_hat = simulate_error_components(Rho=Rho,\n",
    "                                                                      n_inputs=n_inputs, \n",
    "                                                                      S_kernel=S_kernel, \n",
    "                                                                      seed=1234)\n",
    "(Gaussian_copula_NW_conditional_W_u_hat, \n",
    " Gaussian_copula_NW_conditional_W_V_u_hat) = Estimate_NW_u_hat_conditional_W_cross_sectional_application(theta=Gaussian_copula_theta, \n",
    "                                                                                                         n_inputs=n_inputs, \n",
    "                                                                                                         n_corr_terms=n_corr_terms, \n",
    "                                                                                                         y=y.values, \n",
    "                                                                                                         X=X.values, \n",
    "                                                                                                         P=P.values, \n",
    "                                                                                                 U_hat=Gaussian_copula_U_hat, \n",
    "                                                                                                         S_kernel=S_kernel)\n",
    "NW_conditional_W_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_NW_conditional_W_u_hat.reshape(-1,1)], axis=1)\n",
    "NW_conditional_W_V_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_NW_conditional_W_V_u_hat.reshape(-1,1)], axis=1)\n",
    "NW_conditional_W_u_hat_year_mean = np.zeros((13, 2))\n",
    "NW_conditional_W_V_u_hat_year_mean = np.zeros((13, 2))\n",
    "NW_conditional_W_std_u_hat_year_mean = np.zeros((13, 2))\n",
    "for i, t in enumerate(range(86, 99)):\n",
    "    NW_conditional_W_u_hat_year_mean[i, 0] = t\n",
    "    NW_conditional_W_V_u_hat_year_mean[i, 0] = t\n",
    "    NW_conditional_W_std_u_hat_year_mean[i, 0] = t\n",
    "    NW_conditional_W_u_hat_year_mean[i, 1] = np.mean(NW_conditional_W_u_hat_matrix[NW_conditional_W_u_hat_matrix[:,0] == t, 1])\n",
    "    NW_conditional_W_V_u_hat_year_mean[i, 1] = np.mean(NW_conditional_W_V_u_hat_matrix[NW_conditional_W_V_u_hat_matrix[:,0] == t, 1])\n",
    "    NW_conditional_W_std_u_hat_year_mean[i, 1] = np.mean(np.sqrt(NW_conditional_W_V_u_hat_matrix[NW_conditional_W_V_u_hat_matrix[:,0] == t, 1]))\n",
    "\n",
    "# Export simulated training data and compute LLF u hat\n",
    "export_simulation_data_RS2007_electricity_application(theta=Gaussian_copula_theta, \n",
    "                                                      n_inputs=n_inputs, \n",
    "                                                      y=y.values, \n",
    "                                                      X=X.values, \n",
    "                                                      P=P.values, \n",
    "                                                      U_hat=Gaussian_copula_U_hat, \n",
    "                                                      S_kernel=S_kernel)\n",
    "\n",
    "# Users should change the first directory to the path where R is installed - use the code depending on your OS\n",
    "# Windows\n",
    "# subprocess.call([r'C:\\Program Files\\R\\R-4.2.2\\bin\\Rscript.exe ./train_LocalLinear_forest_cross_sectional_RS2007_electricty_application.R'], \n",
    "#                 shell=True)\n",
    "# Mac\n",
    "# subprocess.call([r'/usr/local/bin/Rscript', './train_LocalLinear_forest_cross_sectional_RS2007_electricty_application.R'])\n",
    "Gaussian_copula_LLF_results = pd.read_csv(r'./LLF_Gaussian_copula_u_hat.csv')\n",
    "Gaussian_copula_LLF_conditional_W_u_hat = Gaussian_copula_LLF_results.iloc[:,0].values\n",
    "Gaussian_copula_LLF_conditional_W_V_u_hat = Gaussian_copula_LLF_results.iloc[:,1].values\n",
    "\n",
    "LLF_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_LLF_conditional_W_u_hat.reshape(-1,1)], axis=1)\n",
    "LLF_V_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_LLF_conditional_W_V_u_hat.reshape(-1,1)], axis=1)\n",
    "LLF_u_hat_year_mean = np.zeros((13, 2))\n",
    "LLF_V_u_hat_year_mean = np.zeros((13, 2))\n",
    "LLF_std_u_hat_year_mean = np.zeros((13, 2))\n",
    "for i, t in enumerate(range(86, 99)):\n",
    "    LLF_u_hat_year_mean[i, 0] = t\n",
    "    LLF_V_u_hat_year_mean[i, 0] = t\n",
    "    LLF_std_u_hat_year_mean[i, 0] = t\n",
    "    LLF_u_hat_year_mean[i, 1] = np.mean(LLF_u_hat_matrix[LLF_u_hat_matrix[:,0] == t, 1])\n",
    "    LLF_V_u_hat_year_mean[i, 1] = np.mean(LLF_V_u_hat_matrix[LLF_V_u_hat_matrix[:,0] == t, 1])\n",
    "    LLF_std_u_hat_year_mean[i, 1] = np.mean(np.sqrt(LLF_V_u_hat_matrix[LLF_V_u_hat_matrix[:,0] == t, 1]))\n",
    "\n",
    "mean_Gaussian_copula_JLMS_u_hat = np.mean(Gaussian_copula_JLMS_u_hat)\n",
    "mean_Gaussian_copula_JLMS_std_u_hat = np.mean(JLMS_std_u_hat_year_mean[:,1])\n",
    "mean_Gaussian_copula_NW_conditional_W_u_hat = np.mean(Gaussian_copula_NW_conditional_W_u_hat)\n",
    "mean_Gaussian_copula_NW_conditional_W_std_u_hat = np.mean(NW_conditional_W_std_u_hat_year_mean[:,1])\n",
    "mean_Gaussian_copula_LLF_conditional_W_u_hat = np.mean(Gaussian_copula_LLF_conditional_W_u_hat);\n",
    "mean_Gaussian_copula_LLF_conditional_W_std_u_hat = np.mean(LLF_std_u_hat_year_mean[:,1])\n",
    "\n",
    "cross_sectional_results_table = pd.DataFrame(np.concatenate([np.array([x for x in range(86, 99)]).reshape(-1,1), \n",
    "                                                JLMS_u_hat_year_mean[:, [1]], JLMS_std_u_hat_year_mean[:, [1]], \n",
    "                                                NW_conditional_W_u_hat_year_mean[:, [1]], NW_conditional_W_std_u_hat_year_mean[:, [1]], \n",
    "                                                LLF_u_hat_year_mean[:, [1]], LLF_std_u_hat_year_mean[:, [1]]], axis=1), \n",
    "                                             columns=['Year', 'JLMS Est.', 'JLMS Std Dev', 'NW Est.', 'NW Std Dev', 'LLF Est.', 'LLF Std Dev'])\n",
    "cross_sectional_averages = pd.DataFrame(np.array([mean_Gaussian_copula_JLMS_u_hat, \n",
    "                                     mean_Gaussian_copula_JLMS_std_u_hat, \n",
    "                                     mean_Gaussian_copula_NW_conditional_W_u_hat, \n",
    "                                     mean_Gaussian_copula_NW_conditional_W_std_u_hat, \n",
    "                                     mean_Gaussian_copula_LLF_conditional_W_u_hat, \n",
    "                                     mean_Gaussian_copula_LLF_conditional_W_std_u_hat]).reshape(1,-1), \n",
    "                                        columns=['JLMS Est.', 'JLMS Std Dev', 'NW Est.', 'NW Std Dev', 'LLF Est.', 'LLF Std Dev'], \n",
    "                                        index=['Average'])\n",
    "print('Cross-Sectional Models (APS16 Estimator)')\n",
    "display(cross_sectional_results_table)\n",
    "display(cross_sectional_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40cb6904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q0/9kb46g7n1_57dfk7vtdsv8t4w169ch/T/ipykernel_51156/3934490002.py:107: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panel Data Models (APS14 Estimator)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py:343: FutureWarning:\n",
      "\n",
      "In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>JLMS Est.</th>\n",
       "      <th>JLMS Std Dev</th>\n",
       "      <th>NW Est.</th>\n",
       "      <th>NW Std Dev</th>\n",
       "      <th>LLF Est.</th>\n",
       "      <th>LLF Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.0</td>\n",
       "      <td>1.311525</td>\n",
       "      <td>0.083724</td>\n",
       "      <td>1.067997</td>\n",
       "      <td>0.014492</td>\n",
       "      <td>0.524682</td>\n",
       "      <td>0.004997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.0</td>\n",
       "      <td>1.243104</td>\n",
       "      <td>0.084128</td>\n",
       "      <td>1.118123</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>0.508265</td>\n",
       "      <td>0.004214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.0</td>\n",
       "      <td>1.253664</td>\n",
       "      <td>0.084470</td>\n",
       "      <td>1.087808</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.502310</td>\n",
       "      <td>0.006762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.0</td>\n",
       "      <td>1.220227</td>\n",
       "      <td>0.084759</td>\n",
       "      <td>1.214301</td>\n",
       "      <td>0.007503</td>\n",
       "      <td>0.494355</td>\n",
       "      <td>0.003640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.0</td>\n",
       "      <td>1.236381</td>\n",
       "      <td>0.085002</td>\n",
       "      <td>1.439535</td>\n",
       "      <td>0.011857</td>\n",
       "      <td>0.494184</td>\n",
       "      <td>0.004057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>91.0</td>\n",
       "      <td>1.245825</td>\n",
       "      <td>0.085207</td>\n",
       "      <td>1.315694</td>\n",
       "      <td>0.011858</td>\n",
       "      <td>0.479602</td>\n",
       "      <td>0.005290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>92.0</td>\n",
       "      <td>1.265446</td>\n",
       "      <td>0.085379</td>\n",
       "      <td>1.270296</td>\n",
       "      <td>0.007702</td>\n",
       "      <td>0.477364</td>\n",
       "      <td>0.005021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93.0</td>\n",
       "      <td>1.275452</td>\n",
       "      <td>0.085523</td>\n",
       "      <td>1.341371</td>\n",
       "      <td>0.013708</td>\n",
       "      <td>0.443580</td>\n",
       "      <td>0.006030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>94.0</td>\n",
       "      <td>1.250490</td>\n",
       "      <td>0.085644</td>\n",
       "      <td>1.712312</td>\n",
       "      <td>0.009263</td>\n",
       "      <td>0.454200</td>\n",
       "      <td>0.007128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>95.0</td>\n",
       "      <td>1.275311</td>\n",
       "      <td>0.085745</td>\n",
       "      <td>0.786538</td>\n",
       "      <td>0.017145</td>\n",
       "      <td>0.461193</td>\n",
       "      <td>0.005018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>96.0</td>\n",
       "      <td>1.241606</td>\n",
       "      <td>0.085830</td>\n",
       "      <td>1.368083</td>\n",
       "      <td>0.015950</td>\n",
       "      <td>0.401119</td>\n",
       "      <td>0.008571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>97.0</td>\n",
       "      <td>1.190768</td>\n",
       "      <td>0.085902</td>\n",
       "      <td>0.077475</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>0.387911</td>\n",
       "      <td>0.007919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>98.0</td>\n",
       "      <td>1.221538</td>\n",
       "      <td>0.085961</td>\n",
       "      <td>0.922561</td>\n",
       "      <td>0.039065</td>\n",
       "      <td>0.434830</td>\n",
       "      <td>0.004277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrr}\n",
       "\\toprule\n",
       "{} &  Year &  JLMS Est. &  JLMS Std Dev &   NW Est. &  NW Std Dev &  LLF Est. &  LLF Std Dev \\\\\n",
       "\\midrule\n",
       "0  &  86.0 &   1.311525 &      0.083724 &  1.067997 &    0.014492 &  0.524682 &     0.004997 \\\\\n",
       "1  &  87.0 &   1.243104 &      0.084128 &  1.118123 &    0.010969 &  0.508265 &     0.004214 \\\\\n",
       "2  &  88.0 &   1.253664 &      0.084470 &  1.087808 &    0.009947 &  0.502310 &     0.006762 \\\\\n",
       "3  &  89.0 &   1.220227 &      0.084759 &  1.214301 &    0.007503 &  0.494355 &     0.003640 \\\\\n",
       "4  &  90.0 &   1.236381 &      0.085002 &  1.439535 &    0.011857 &  0.494184 &     0.004057 \\\\\n",
       "5  &  91.0 &   1.245825 &      0.085207 &  1.315694 &    0.011858 &  0.479602 &     0.005290 \\\\\n",
       "6  &  92.0 &   1.265446 &      0.085379 &  1.270296 &    0.007702 &  0.477364 &     0.005021 \\\\\n",
       "7  &  93.0 &   1.275452 &      0.085523 &  1.341371 &    0.013708 &  0.443580 &     0.006030 \\\\\n",
       "8  &  94.0 &   1.250490 &      0.085644 &  1.712312 &    0.009263 &  0.454200 &     0.007128 \\\\\n",
       "9  &  95.0 &   1.275311 &      0.085745 &  0.786538 &    0.017145 &  0.461193 &     0.005018 \\\\\n",
       "10 &  96.0 &   1.241606 &      0.085830 &  1.368083 &    0.015950 &  0.401119 &     0.008571 \\\\\n",
       "11 &  97.0 &   1.190768 &      0.085902 &  0.077475 &    0.004872 &  0.387911 &     0.007919 \\\\\n",
       "12 &  98.0 &   1.221538 &      0.085961 &  0.922561 &    0.039065 &  0.434830 &     0.004277 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "    Year  JLMS Est.  JLMS Std Dev   NW Est.  NW Std Dev  LLF Est.  LLF Std Dev\n",
       "0   86.0   1.311525      0.083724  1.067997    0.014492  0.524682     0.004997\n",
       "1   87.0   1.243104      0.084128  1.118123    0.010969  0.508265     0.004214\n",
       "2   88.0   1.253664      0.084470  1.087808    0.009947  0.502310     0.006762\n",
       "3   89.0   1.220227      0.084759  1.214301    0.007503  0.494355     0.003640\n",
       "4   90.0   1.236381      0.085002  1.439535    0.011857  0.494184     0.004057\n",
       "5   91.0   1.245825      0.085207  1.315694    0.011858  0.479602     0.005290\n",
       "6   92.0   1.265446      0.085379  1.270296    0.007702  0.477364     0.005021\n",
       "7   93.0   1.275452      0.085523  1.341371    0.013708  0.443580     0.006030\n",
       "8   94.0   1.250490      0.085644  1.712312    0.009263  0.454200     0.007128\n",
       "9   95.0   1.275311      0.085745  0.786538    0.017145  0.461193     0.005018\n",
       "10  96.0   1.241606      0.085830  1.368083    0.015950  0.401119     0.008571\n",
       "11  97.0   1.190768      0.085902  0.077475    0.004872  0.387911     0.007919\n",
       "12  98.0   1.221538      0.085961  0.922561    0.039065  0.434830     0.004277"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py:343: FutureWarning:\n",
      "\n",
      "In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JLMS Est.</th>\n",
       "      <th>JLMS Std Dev</th>\n",
       "      <th>NW Est.</th>\n",
       "      <th>NW Std Dev</th>\n",
       "      <th>LLF Est.</th>\n",
       "      <th>LLF Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>1.248564</td>\n",
       "      <td>0.085175</td>\n",
       "      <td>1.132469</td>\n",
       "      <td>0.01341</td>\n",
       "      <td>0.46643</td>\n",
       "      <td>0.00561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrr}\n",
       "\\toprule\n",
       "{} &  JLMS Est. &  JLMS Std Dev &   NW Est. &  NW Std Dev &  LLF Est. &  LLF Std Dev \\\\\n",
       "\\midrule\n",
       "Average &   1.248564 &      0.085175 &  1.132469 &     0.01341 &   0.46643 &      0.00561 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "         JLMS Est.  JLMS Std Dev   NW Est.  NW Std Dev  LLF Est.  LLF Std Dev\n",
       "Average   1.248564      0.085175  1.132469     0.01341   0.46643      0.00561"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Estimate Panel data models \n",
    "all_ = pd.concat([data[['Firm No', 'YEAR']], y, X], axis=1)\n",
    "N = 72\n",
    "T = len(all_['YEAR'].unique())\n",
    "panel_y = {}\n",
    "panel_X = {}\n",
    "for j, t in enumerate(range(86, 99)):\n",
    "    y__ = all_.loc[all_['YEAR'] == t, 'Output (MWhr)']\n",
    "    X__ = all_.loc[all_['YEAR'] == t, [0, 1, 'Capital ($1000)']]\n",
    "    panel_y[j] = y__.values\n",
    "    panel_X[j] = X__.values\n",
    "\n",
    "# Independent uniform random variables for FMSLE - assumes a Gaussian copula\n",
    "sampler = stats.qmc.Halton(d=T, \n",
    "                           scramble=True, \n",
    "                           seed=123)\n",
    "us_ = sampler.random(n=S)\n",
    "us_ = stats.norm.ppf(us_) # transform to standard normal\n",
    "FMSLE_us = {}\n",
    "for t in range(T):\n",
    "    us_Sxn = np.tile(us_[:, t-1][np.newaxis, :], (N, 1)).T\n",
    "    FMSLE_us[t] = us_Sxn\n",
    "\n",
    "initial_lalpha = -2.6\n",
    "initial_beta = np.array([0.5, 0.2, 0.2])\n",
    "initial_lbeta = np.log(initial_beta)\n",
    "initial_logit_beta = np.log(initial_beta/(1-initial_beta))\n",
    "initial_sigma2_v = 0.015\n",
    "initial_lsigma2_v = np.log(initial_sigma2_v)\n",
    "initial_sigma2_u = 0.15\n",
    "initial_lsigma2_u = np.log(initial_sigma2_u)\n",
    "\n",
    "initial_delta = np.array([initial_lsigma2_u, 0.2])\n",
    "eps_ = {}\n",
    "for t in range(T):\n",
    "    eps__array = np.zeros(N)\n",
    "    eps__ = panel_y[t] - initial_lalpha- panel_X[t]@initial_beta\n",
    "    eps__array[:len(eps__)] = eps__\n",
    "    eps_[t] = eps__array\n",
    "\n",
    "k = len(initial_beta) + 1\n",
    "initial_Rho = np.corrcoef(np.concatenate([eps.reshape(-1,1) for eps in eps_.values()], axis=1).T)\n",
    "initial_lRho = direct_mapping_mat(initial_Rho)\n",
    "theta0 = np.concatenate([np.array([initial_lalpha]), \n",
    "                         initial_logit_beta, \n",
    "                         initial_delta, \n",
    "                         np.array([initial_sigma2_v]), initial_lRho])\n",
    "          \n",
    "# Bounds to ensure sigma2v and sigma2u are >= 0\n",
    "bounds = [(None, None) for x in range(6)] + [\n",
    "    (1e-5, np.inf),\n",
    "] + [(None, None) for x in range(len(initial_lRho))]\n",
    "\n",
    "# Minimize the negative log-likelihood using numerical optimization.\n",
    "MLE_results = minimize(\n",
    "    fun=Loglikelihood_APS14_dynamic_panel_SFA_u_RS2007,\n",
    "    x0=theta0,\n",
    "    method=\"L-BFGS-B\",\n",
    "    tol = 1e-8,\n",
    "    options={\"ftol\": 1e-8, \"maxiter\": 1000, \"maxfun\": 35000, \"maxcor\": 500},\n",
    "    args=(panel_y, panel_X, N, T, k, S, FMSLE_us),\n",
    "    bounds=bounds,\n",
    ")\n",
    "\n",
    "APS14_theta = MLE_results.x\n",
    "APS14_theta[0] = np.exp(APS14_theta[0])\n",
    "APS14_theta[1:4] = 1/(1+np.exp(-APS14_theta[1:4])) # inverse logit transform of betas\n",
    "APS14_logMLE = MLE_results.fun * -1\n",
    "\n",
    "APS14_Rho = inverse_mapping_vec(APS14_theta[7:])\n",
    "np.savetxt(r'./panel_SFA_RS2007_electricty_application_gaussian_copula_correlation_matrix.csv', \n",
    "              APS14_Rho, \n",
    "              delimiter=',')\n",
    "\n",
    "# Simulated dependent U based upon estimated copula parameters\n",
    "APS14_U_hat = simulate_error_components(Rho=APS14_Rho, \n",
    "                                        n_inputs=T, \n",
    "                                        S_kernel=S_kernel, \n",
    "                                        seed=10)\n",
    "\n",
    "# JLMS scores\n",
    "(APS14_JLMS_u_hat, \n",
    " APS14_JLMS_V_u_hat) = Estimate_Jondrow1982_u_hat_panel_SFA_application_RS2007(alpha=APS14_theta[0], \n",
    "                                                                               beta=APS14_theta[1:4], \n",
    "                                                                               delta=APS14_theta[4:6], \n",
    "                                                                               sigma2_v=APS14_theta[6], \n",
    "                                                                               y=panel_y, \n",
    "                                                                               X=panel_X, \n",
    "                                                                               T=T, \n",
    "                                                                               N=N)\n",
    "APS14_JLMS_u_hat_year_mean = np.nanmean(APS14_JLMS_u_hat, axis=0)\n",
    "APS14_JLMS_V_u_hat_year_mean = np.nanmean(APS14_JLMS_V_u_hat, axis=0)\n",
    "APS14_JLMS_std_u_hat_year_mean = np.nanmean(np.sqrt(APS14_JLMS_V_u_hat), axis=0)\n",
    "\n",
    "# Copula Nadaraya Watson\n",
    "(APS14_NW_u_hat_conditional_eps, \n",
    " APS14_NW_V_u_hat_conditional_eps) = Estimate_NW_u_hat_conditional_eps_panel_SFA_RS2007(theta=APS14_theta, \n",
    "                                                                                        y=panel_y, \n",
    "                                                                                        X=panel_X, \n",
    "                                                                                        N=N, \n",
    "                                                                                        T=T, \n",
    "                                                                                        k=k, \n",
    "                                                                                        U_hat=APS14_U_hat, \n",
    "                                                                                        S_kernel=S_kernel)\n",
    "APS14_NW_u_hat_conditional_eps_year_mean = np.nanmean(APS14_NW_u_hat_conditional_eps, axis=0)\n",
    "APS14_NW_V_u_hat_conditional_eps_year_mean = np.nanmean(APS14_NW_V_u_hat_conditional_eps, axis=0)\n",
    "APS14_NW_std_u_hat_conditional_eps_year_mean = np.nanmean(np.sqrt(APS14_NW_V_u_hat_conditional_eps), axis=0)\n",
    "\n",
    "export_RS2007_electricity_SFA_panel_data(theta=APS14_theta, \n",
    "                                         y=panel_y, \n",
    "                                         X=panel_X, \n",
    "                                         N=N, \n",
    "                                         T=T, \n",
    "                                         U_hat=APS14_U_hat, \n",
    "                                         S_kernel=S_kernel)\n",
    "\n",
    "# Users should change the first directory to the path where R is installed\n",
    "    # Windows\n",
    "# subprocess.call(r'C:\\Program Files\\R\\R-4.2.2\\bin\\Rscript.exe ./train_LocalLinear_forest_panel_RS2007_electricty_application.R', \n",
    "#                 shell=True)\n",
    "    # Mac\n",
    "# subprocess.call([r'/usr/local/bin/Rscript', './train_LocalLinear_forest_panel_RS2007_electricty_application.R'])\n",
    "APS14_LLF_conditional_eps_u_hat = pd.read_csv(r'./RS2007_electricity_LLF_Gaussian_copula_u_hat.csv')\n",
    "APS14_LLF_conditional_eps_V_u_hat = pd.read_csv(r'./RS2007_electricity_LLF_Gaussian_copula_V_u_hat.csv')\n",
    "APS14_LLF_conditional_eps_u_hat = APS14_LLF_conditional_eps_u_hat.iloc[:-1, :]\n",
    "APS14_LLF_conditional_eps_V_u_hat = APS14_LLF_conditional_eps_V_u_hat.iloc[:-1, :]\n",
    "APS14_LLF_conditional_eps_u_hat_year_mean = np.nanmean(APS14_LLF_conditional_eps_u_hat, axis=0)\n",
    "APS14_LLF_conditional_eps_V_u_hat_year_mean = np.nanmean(APS14_LLF_conditional_eps_V_u_hat, axis=0)\n",
    "APS14_LLF_conditional_eps_std_u_hat_year_mean = np.nanmean(np.sqrt(APS14_LLF_conditional_eps_V_u_hat), axis=0)\n",
    "\n",
    "mean_Gaussian_copula_JLMS_u_hat = np.mean(APS14_JLMS_u_hat_year_mean)\n",
    "mean_Gaussian_copula_JLMS_std_u_hat = np.mean(APS14_JLMS_std_u_hat_year_mean)\n",
    "mean_Gaussian_copula_NW_conditional_W_u_hat = np.mean(APS14_NW_u_hat_conditional_eps_year_mean)\n",
    "mean_Gaussian_copula_NW_conditional_W_std_u_hat = np.mean(APS14_NW_std_u_hat_conditional_eps_year_mean)\n",
    "mean_Gaussian_copula_LLF_conditional_W_u_hat = np.mean(APS14_LLF_conditional_eps_u_hat_year_mean)\n",
    "mean_Gaussian_copula_LLF_conditional_W_std_u_hat = np.mean(APS14_LLF_conditional_eps_std_u_hat_year_mean)\n",
    "\n",
    "panel_results_table = pd.DataFrame(np.concatenate([np.array([x for x in range(86, 99)]).reshape(-1,1), \n",
    "                                                APS14_JLMS_u_hat_year_mean.reshape(-1,1), APS14_JLMS_std_u_hat_year_mean.reshape(-1,1), \n",
    "                                                APS14_NW_u_hat_conditional_eps_year_mean.reshape(-1,1), APS14_NW_std_u_hat_conditional_eps_year_mean.reshape(-1,1), \n",
    "                                                APS14_LLF_conditional_eps_u_hat_year_mean.reshape(-1,1), APS14_LLF_conditional_eps_std_u_hat_year_mean.reshape(-1,1)], axis=1), \n",
    "                                             columns=['Year', 'JLMS Est.', 'JLMS Std Dev', 'NW Est.', 'NW Std Dev', 'LLF Est.', 'LLF Std Dev'])\n",
    "panel_averages = pd.DataFrame(np.array([mean_Gaussian_copula_JLMS_u_hat, \n",
    "                                     mean_Gaussian_copula_JLMS_std_u_hat, \n",
    "                                     mean_Gaussian_copula_NW_conditional_W_u_hat, \n",
    "                                     mean_Gaussian_copula_NW_conditional_W_std_u_hat, \n",
    "                                     mean_Gaussian_copula_LLF_conditional_W_u_hat, \n",
    "                                     mean_Gaussian_copula_LLF_conditional_W_std_u_hat]).reshape(1,-1), \n",
    "                                        columns=['JLMS Est.', 'JLMS Std Dev', 'NW Est.', 'NW Std Dev', 'LLF Est.', 'LLF Std Dev'], \n",
    "                                        index=['Average'])\n",
    "print('Panel Data Models (APS14 Estimator)')\n",
    "display(panel_results_table)\n",
    "display(panel_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0aa78f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "text/plain": [
       "<Figure size 1650x1050 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/pdf": "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\n",
      "text/plain": [
       "<Figure size 1650x1050 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# density plots\n",
    "kde = stats.gaussian_kde(Gaussian_copula_JLMS_u_hat)\n",
    "Gaussian_copula_JLMS_TE_kernel_x = np.linspace(Gaussian_copula_JLMS_u_hat.min(), Gaussian_copula_JLMS_u_hat.max(), 100)\n",
    "Gaussian_copula_JLMS_TE_kernel = kde(Gaussian_copula_JLMS_TE_kernel_x)\n",
    "\n",
    "kde = stats.gaussian_kde(Gaussian_copula_NW_conditional_W_u_hat)\n",
    "Gaussian_copula_NW_TE_kernel_x = np.linspace(Gaussian_copula_JLMS_u_hat.min(), Gaussian_copula_JLMS_u_hat.max(), 100)\n",
    "Gaussian_copula_NW_TE_kernel = kde(Gaussian_copula_NW_TE_kernel_x)\n",
    "\n",
    "kde = stats.gaussian_kde(Gaussian_copula_LLF_conditional_W_u_hat)\n",
    "Gaussian_copula_LLF_TE_kernel_x = np.linspace(Gaussian_copula_JLMS_u_hat.min(), Gaussian_copula_JLMS_u_hat.max(), 100)\n",
    "Gaussian_copula_LLF_TE_kernel = kde(Gaussian_copula_LLF_TE_kernel_x)\n",
    "\n",
    "figure = plt.Figure(figsize=(10,10))\n",
    "plt.plot(Gaussian_copula_JLMS_TE_kernel_x, Gaussian_copula_JLMS_TE_kernel, \n",
    "         color='k', \n",
    "         ls='-', \n",
    "         label=r'JLMS $E[u_{i}|\\varepsilon_{i}]$')\n",
    "plt.plot(Gaussian_copula_NW_TE_kernel_x, \n",
    "         Gaussian_copula_NW_TE_kernel, \n",
    "         ls=':', \n",
    "         color='k', \n",
    "         label=r'NW $E[u_{i}|\\varepsilon_{i}, \\omega_{i2}, \\omega_{i3}]$')\n",
    "plt.plot(Gaussian_copula_LLF_TE_kernel_x, \n",
    "         Gaussian_copula_LLF_TE_kernel, \n",
    "         ls='-.', \n",
    "         color='k', \n",
    "         label=r'LLF $E[u_{i}|\\varepsilon_{i}, \\omega_{i2}, \\omega_{i3}]$')\n",
    "plt.xlabel(r'$u_{i}$', fontsize=14)\n",
    "plt.ylabel('Kernel Density', fontsize=14)\n",
    "plt.title('JLMS & APS16 Estimator', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "APS14_JLMS_u_hat_flat = APS14_JLMS_u_hat.flatten()\n",
    "APS14_JLMS_u_hat_flat = APS14_JLMS_u_hat_flat[~np.isnan(APS14_JLMS_u_hat_flat)]\n",
    "APS14_NW_u_hat_conditional_eps_flat = APS14_NW_u_hat_conditional_eps.flatten()\n",
    "APS14_NW_u_hat_conditional_eps_flat = APS14_NW_u_hat_conditional_eps_flat[~np.isnan(APS14_NW_u_hat_conditional_eps_flat)]\n",
    "APS14_LLF_conditional_eps_u_hat_flat = APS14_LLF_conditional_eps_u_hat.values.flatten()\n",
    "APS14_LLF_conditional_eps_u_hat_flat = APS14_LLF_conditional_eps_u_hat_flat[~np.isnan(APS14_LLF_conditional_eps_u_hat_flat)]\n",
    "\n",
    "kde = stats.gaussian_kde(APS14_JLMS_u_hat_flat)\n",
    "APS14_JLMS_TE_kernel_x = np.linspace(APS14_JLMS_u_hat_flat.min(), \n",
    "                                     APS14_JLMS_u_hat_flat.max(), 100)\n",
    "APS14_JLMS_TE_kernel = kde(APS14_JLMS_TE_kernel_x)\n",
    "\n",
    "kde = stats.gaussian_kde(APS14_NW_u_hat_conditional_eps_flat)\n",
    "APS14_NW_TE_kernel_x = np.linspace(APS14_NW_u_hat_conditional_eps_flat.min(), \n",
    "                                   APS14_NW_u_hat_conditional_eps_flat.max(), 100)\n",
    "APS14_NW_TE_kernel = kde(APS14_NW_TE_kernel_x)\n",
    "\n",
    "kde = stats.gaussian_kde(APS14_LLF_conditional_eps_u_hat_flat)\n",
    "APS14_LLF_TE_kernel_x = np.linspace(APS14_LLF_conditional_eps_u_hat_flat.min(), \n",
    "                                    APS14_LLF_conditional_eps_u_hat_flat.max(), 100)\n",
    "APS14_LLF_TE_kernel = kde(APS14_LLF_TE_kernel_x)\n",
    "\n",
    "figure = plt.Figure(figsize=(10,10))\n",
    "plt.plot(APS14_JLMS_TE_kernel_x, APS14_JLMS_TE_kernel, \n",
    "         color='k', \n",
    "         ls='-', \n",
    "         label=r'JLMS $E[u_{it}|\\varepsilon_{it}]$')\n",
    "plt.plot(APS14_NW_TE_kernel_x, \n",
    "         APS14_NW_TE_kernel, \n",
    "         ls=':', \n",
    "         color='k', \n",
    "         label=r'NW $E[u_{it}|\\varepsilon_{i1}, \\dots, \\varepsilon_{i,13}]$')\n",
    "plt.plot(APS14_LLF_TE_kernel_x, \n",
    "         APS14_LLF_TE_kernel, \n",
    "         ls='-.', \n",
    "         color='k', \n",
    "         label=r'LLF $E[u_{it}|\\varepsilon_{i1}, \\dots, \\varepsilon_{i,13}]$')\n",
    "plt.xlabel(r'$u_{i}$', fontsize=14)\n",
    "plt.ylabel('Kernel Density', fontsize=14)\n",
    "plt.title('JLMS & APS14 Estimator', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93959b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
