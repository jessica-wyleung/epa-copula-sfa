{
 "cells": [
  {
   "cell_type": "raw",
   "id": "18cb47f9",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  keep-ipynb: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5acfb5",
   "metadata": {},
   "source": [
    "# Chapter 7: Predicting inefficiency\n",
    "\n",
    "## Exercise 7.1: JLMS estimator of inefficiency of rice production\n",
    "This exercise uses the procedure outlined in Section 7.2 to estimate technical inefficiencies of Indonesian rice farms. The details of the data are provided in Exercise 4.2. The table reports predicted technical inefficiencies in each time period for ten farms, namely those that are at the 0.05,0.15,...,0.95 fractiles of the sample of inefficiencies for $t = 1$. The first two columns report the rank and the quantile of the selected farms. The third and fourth columns contain the sample variation and sample mean over t, respectively, of the inefficiencies, which are reported in columns 5 through 10. The last row of the tables reports column-wise averages over the ten farms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e10c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def JLMS_panel_technical_inefficiency_scores(theta, y, X):\n",
    "    N = 171\n",
    "    T = 6\n",
    "\n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:14]\n",
    "    sigma = theta[-2]\n",
    "    _lambda = theta[-1]\n",
    "\n",
    "    u_hat = np.zeros((N, T))\n",
    "    for t in range(T):\n",
    "\n",
    "        eps_t = y[t] - alpha - X[t] @ beta\n",
    "        b = (eps_t * _lambda) / sigma\n",
    "        u_hat[:, t] = ((sigma * _lambda) / (1 + _lambda**2)) * (\n",
    "            stats.norm.pdf(b) / (1 - stats.norm.cdf(b)) - b\n",
    "        )\n",
    "\n",
    "    return u_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6d06a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JLMS Scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jleu0010/Documents/GitHub/EPA-Copula-SFA-website/venv/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/var/folders/q0/9kb46g7n1_57dfk7vtdsv8t4w169ch/T/ipykernel_20238/1289086853.py:88: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  JLMS_technical_inefficiency_results.iloc[:, -6:] = JLMS_u_hat_for_table\n",
      "/Users/jleu0010/Documents/GitHub/EPA-Copula-SFA-website/venv/lib/python3.8/site-packages/IPython/core/formatters.py:344: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Fractile</th>\n",
       "      <th>Std(u)</th>\n",
       "      <th>Mean(u)</th>\n",
       "      <th>t=1</th>\n",
       "      <th>t=2</th>\n",
       "      <th>t=3</th>\n",
       "      <th>t=4</th>\n",
       "      <th>t=5</th>\n",
       "      <th>t=6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.199559</td>\n",
       "      <td>0.353626</td>\n",
       "      <td>0.155185</td>\n",
       "      <td>0.334495</td>\n",
       "      <td>0.751678</td>\n",
       "      <td>0.298441</td>\n",
       "      <td>0.168792</td>\n",
       "      <td>0.413164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.266284</td>\n",
       "      <td>0.389258</td>\n",
       "      <td>0.194313</td>\n",
       "      <td>0.247702</td>\n",
       "      <td>0.901049</td>\n",
       "      <td>0.577669</td>\n",
       "      <td>0.251172</td>\n",
       "      <td>0.163642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.089351</td>\n",
       "      <td>0.285298</td>\n",
       "      <td>0.227678</td>\n",
       "      <td>0.192656</td>\n",
       "      <td>0.460310</td>\n",
       "      <td>0.332479</td>\n",
       "      <td>0.232645</td>\n",
       "      <td>0.266019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.139124</td>\n",
       "      <td>0.389688</td>\n",
       "      <td>0.258642</td>\n",
       "      <td>0.210409</td>\n",
       "      <td>0.295907</td>\n",
       "      <td>0.535117</td>\n",
       "      <td>0.477681</td>\n",
       "      <td>0.560369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.200043</td>\n",
       "      <td>0.338445</td>\n",
       "      <td>0.283757</td>\n",
       "      <td>0.201320</td>\n",
       "      <td>0.559250</td>\n",
       "      <td>0.664111</td>\n",
       "      <td>0.178528</td>\n",
       "      <td>0.143705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>94</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.065599</td>\n",
       "      <td>0.362450</td>\n",
       "      <td>0.307076</td>\n",
       "      <td>0.346067</td>\n",
       "      <td>0.296354</td>\n",
       "      <td>0.380834</td>\n",
       "      <td>0.349237</td>\n",
       "      <td>0.495129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>111</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.108994</td>\n",
       "      <td>0.455132</td>\n",
       "      <td>0.336327</td>\n",
       "      <td>0.376101</td>\n",
       "      <td>0.393122</td>\n",
       "      <td>0.559206</td>\n",
       "      <td>0.642810</td>\n",
       "      <td>0.423228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.139395</td>\n",
       "      <td>0.307457</td>\n",
       "      <td>0.393067</td>\n",
       "      <td>0.551203</td>\n",
       "      <td>0.307025</td>\n",
       "      <td>0.291704</td>\n",
       "      <td>0.134145</td>\n",
       "      <td>0.167600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>145</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.293376</td>\n",
       "      <td>0.600202</td>\n",
       "      <td>0.466352</td>\n",
       "      <td>0.382522</td>\n",
       "      <td>1.180340</td>\n",
       "      <td>0.717295</td>\n",
       "      <td>0.284017</td>\n",
       "      <td>0.570689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>162</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.138623</td>\n",
       "      <td>0.552315</td>\n",
       "      <td>0.568924</td>\n",
       "      <td>0.719727</td>\n",
       "      <td>0.540897</td>\n",
       "      <td>0.716119</td>\n",
       "      <td>0.425638</td>\n",
       "      <td>0.342586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrr}\n",
       "\\toprule\n",
       "{} &  Rank &  Fractile &    Std(u) &   Mean(u) &       t=1 &       t=2 &       t=3 &       t=4 &       t=5 &       t=6 \\\\\n",
       "\\midrule\n",
       "0 &     9 &      0.05 &  0.199559 &  0.353626 &  0.155185 &  0.334495 &  0.751678 &  0.298441 &  0.168792 &  0.413164 \\\\\n",
       "1 &    26 &      0.15 &  0.266284 &  0.389258 &  0.194313 &  0.247702 &  0.901049 &  0.577669 &  0.251172 &  0.163642 \\\\\n",
       "2 &    43 &      0.25 &  0.089351 &  0.285298 &  0.227678 &  0.192656 &  0.460310 &  0.332479 &  0.232645 &  0.266019 \\\\\n",
       "3 &    60 &      0.35 &  0.139124 &  0.389688 &  0.258642 &  0.210409 &  0.295907 &  0.535117 &  0.477681 &  0.560369 \\\\\n",
       "4 &    77 &      0.45 &  0.200043 &  0.338445 &  0.283757 &  0.201320 &  0.559250 &  0.664111 &  0.178528 &  0.143705 \\\\\n",
       "5 &    94 &      0.55 &  0.065599 &  0.362450 &  0.307076 &  0.346067 &  0.296354 &  0.380834 &  0.349237 &  0.495129 \\\\\n",
       "6 &   111 &      0.65 &  0.108994 &  0.455132 &  0.336327 &  0.376101 &  0.393122 &  0.559206 &  0.642810 &  0.423228 \\\\\n",
       "7 &   128 &      0.75 &  0.139395 &  0.307457 &  0.393067 &  0.551203 &  0.307025 &  0.291704 &  0.134145 &  0.167600 \\\\\n",
       "8 &   145 &      0.85 &  0.293376 &  0.600202 &  0.466352 &  0.382522 &  1.180340 &  0.717295 &  0.284017 &  0.570689 \\\\\n",
       "9 &   162 &      0.95 &  0.138623 &  0.552315 &  0.568924 &  0.719727 &  0.540897 &  0.716119 &  0.425638 &  0.342586 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "   Rank  Fractile    Std(u)   Mean(u)       t=1       t=2       t=3       t=4  \\\n",
       "0     9      0.05  0.199559  0.353626  0.155185  0.334495  0.751678  0.298441   \n",
       "1    26      0.15  0.266284  0.389258  0.194313  0.247702  0.901049  0.577669   \n",
       "2    43      0.25  0.089351  0.285298  0.227678  0.192656  0.460310  0.332479   \n",
       "3    60      0.35  0.139124  0.389688  0.258642  0.210409  0.295907  0.535117   \n",
       "4    77      0.45  0.200043  0.338445  0.283757  0.201320  0.559250  0.664111   \n",
       "5    94      0.55  0.065599  0.362450  0.307076  0.346067  0.296354  0.380834   \n",
       "6   111      0.65  0.108994  0.455132  0.336327  0.376101  0.393122  0.559206   \n",
       "7   128      0.75  0.139395  0.307457  0.393067  0.551203  0.307025  0.291704   \n",
       "8   145      0.85  0.293376  0.600202  0.466352  0.382522  1.180340  0.717295   \n",
       "9   162      0.95  0.138623  0.552315  0.568924  0.719727  0.540897  0.716119   \n",
       "\n",
       "        t=5       t=6  \n",
       "0  0.168792  0.413164  \n",
       "1  0.251172  0.163642  \n",
       "2  0.232645  0.266019  \n",
       "3  0.477681  0.560369  \n",
       "4  0.178528  0.143705  \n",
       "5  0.349237  0.495129  \n",
       "6  0.642810  0.423228  \n",
       "7  0.134145  0.167600  \n",
       "8  0.284017  0.570689  \n",
       "9  0.425638  0.342586  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jleu0010/Documents/GitHub/EPA-Copula-SFA-website/venv/lib/python3.8/site-packages/IPython/core/formatters.py:344: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Std(u)</th>\n",
       "      <th>Mean(u)</th>\n",
       "      <th>t=1</th>\n",
       "      <th>t=2</th>\n",
       "      <th>t=3</th>\n",
       "      <th>t=4</th>\n",
       "      <th>t=5</th>\n",
       "      <th>t=6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.164035</td>\n",
       "      <td>0.403387</td>\n",
       "      <td>0.319132</td>\n",
       "      <td>0.35622</td>\n",
       "      <td>0.568593</td>\n",
       "      <td>0.507298</td>\n",
       "      <td>0.314467</td>\n",
       "      <td>0.354613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrr}\n",
       "\\toprule\n",
       "{} &    Std(u) &   Mean(u) &       t=1 &      t=2 &       t=3 &       t=4 &       t=5 &       t=6 \\\\\n",
       "\\midrule\n",
       "Average &  0.164035 &  0.403387 &  0.319132 &  0.35622 &  0.568593 &  0.507298 &  0.314467 &  0.354613 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "           Std(u)   Mean(u)       t=1      t=2       t=3       t=4       t=5  \\\n",
       "Average  0.164035  0.403387  0.319132  0.35622  0.568593  0.507298  0.314467   \n",
       "\n",
       "              t=6  \n",
       "Average  0.354613  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ricefarm = pd.read_csv(r\"ricefarm.csv\")\n",
    "\n",
    "ricefarm = ricefarm.iloc[:, [0, 1, 3, 5, 6, 7, 8, 14, 16]]\n",
    "\n",
    "# Create ID dummies\n",
    "dar = np.round(ricefarm[\"ID\"] / 100000).astype(int)\n",
    "id_dummies = pd.get_dummies(dar, prefix=\"DR\").astype(int)\n",
    "ricefarm = pd.concat([ricefarm, id_dummies.iloc[:, :-1]], axis=1)\n",
    "\n",
    "# Create rice variety dummies\n",
    "rice_dummies = pd.get_dummies(ricefarm.iloc[:, 2], prefix=\"VAR\").astype(int)\n",
    "ricefarm = pd.concat([ricefarm, rice_dummies.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Recode TSP as logs and zeros\n",
    "ricefarm[ricefarm.columns[5]] = np.where(\n",
    "    ricefarm.iloc[:, 5] > 0, np.log(ricefarm.iloc[:, 5]), 0\n",
    ")\n",
    "\n",
    "# Convert pesticide usage to a dummy\n",
    "ricefarm[ricefarm.columns[6]] = (ricefarm.iloc[:, 6] != 0).astype(int)\n",
    "\n",
    "# Reorder the data\n",
    "ricefarm = ricefarm.iloc[:, [8, 3, 4, 5, 7, 1, 6, 14, 15, 9, 10, 11, 12, 13]]\n",
    "\n",
    "y = np.log(ricefarm.iloc[:, 0].values)\n",
    "X = ricefarm.iloc[:, 1:].values\n",
    "\n",
    "# Log covariates\n",
    "X[:, 0:2] = np.log(X[:, 0:2])\n",
    "X[:, 3] = np.log(X[:, 3])\n",
    "X[:, 4] = np.log(X[:, 4])\n",
    "\n",
    "# Cross sections of  y and X\n",
    "N = 171\n",
    "T = 6\n",
    "y_t = []\n",
    "X_t = []\n",
    "for t in range(T):\n",
    "    if t == 0:\n",
    "        y_t.append(y[:N])\n",
    "        X_t.append(X[:N, :])\n",
    "    else:\n",
    "        y_t.append(y[t * N : (t + 1) * N])\n",
    "        X_t.append(X[t * N : (t + 1) * N, :])\n",
    "\n",
    "# Import the model parameters from exercise 5.8\n",
    "theta = np.loadtxt(\"exercise_4_8_theta_python.txt\", delimiter=\",\")\n",
    "\n",
    "# Estimation of technical inefficiencies\n",
    "# Get idx of percentile firms\n",
    "percetile_firm_idx = np.round(np.arange(0.05, 1, 0.1) * N).astype(int)\n",
    "\n",
    "# JLMS formula technical inefficiencies\n",
    "JLMS_u_hat = JLMS_panel_technical_inefficiency_scores(theta=theta, y=y_t, X=X_t)\n",
    "# Sort technical inefficiency scores by t=1 values\n",
    "sort_idx = JLMS_u_hat[:, 0].argsort()\n",
    "sorted_t1_JLMS_u_hat = JLMS_u_hat[sort_idx]\n",
    "# Mean technical inefficiency for each firms over all t\n",
    "JLMS_mean = np.mean(JLMS_u_hat, axis=1)\n",
    "sorted_t1_JLMS_mean = JLMS_mean[sort_idx]\n",
    "# Standard deviation of technical inefficiency for each firm over all t\n",
    "JLMS_std = np.std(JLMS_u_hat, axis=1)\n",
    "sorted_t1_JLMS_std = JLMS_std[sort_idx]\n",
    "\n",
    "JLMS_u_hat_for_table = sorted_t1_JLMS_u_hat[percetile_firm_idx-1, :]\n",
    "JLMS_std_u_hat_for_table = sorted_t1_JLMS_std[percetile_firm_idx-1]\n",
    "JLMS_mean_u_hat_for_table = sorted_t1_JLMS_mean[percetile_firm_idx-1]\n",
    "\n",
    "# Tabulate the technical inefficiency scores\n",
    "JLMS_technical_inefficiency_results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Rank\",\n",
    "        \"Fractile\",\n",
    "        \"Std(u)\",\n",
    "        \"Mean(u)\",\n",
    "        \"t=1\",\n",
    "        \"t=2\",\n",
    "        \"t=3\",\n",
    "        \"t=4\",\n",
    "        \"t=5\",\n",
    "        \"t=6\",\n",
    "    ]\n",
    ")\n",
    "JLMS_technical_inefficiency_results[\"Rank\"] = percetile_firm_idx\n",
    "JLMS_technical_inefficiency_results[\"Fractile\"] = np.arange(0.05, 1, 0.1)\n",
    "JLMS_technical_inefficiency_results[\"Std(u)\"] = JLMS_std_u_hat_for_table\n",
    "JLMS_technical_inefficiency_results[\"Mean(u)\"] = JLMS_mean_u_hat_for_table\n",
    "JLMS_technical_inefficiency_results.iloc[:, -6:] = JLMS_u_hat_for_table\n",
    "\n",
    "# Compute averages\n",
    "average_JLMS = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            np.mean(JLMS_std_u_hat_for_table).reshape(-1, 1).T,\n",
    "            np.mean(JLMS_mean_u_hat_for_table).reshape(-1, 1).T,\n",
    "            np.mean(JLMS_u_hat_for_table, axis=0).reshape(-1, 1).T,\n",
    "        ],\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"Std(u)\", \"Mean(u)\", \"t=1\", \"t=2\", \"t=3\", \"t=4\", \"t=5\", \"t=6\"],\n",
    "    index=[\"Average\"],\n",
    ")\n",
    "\n",
    "print(\"JLMS Scores\")\n",
    "display(JLMS_technical_inefficiency_results)\n",
    "display(average_JLMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8d8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea223e0",
   "metadata": {},
   "source": [
    "## Exercise 7.2: The APS estimator of inefficiency of rice production\n",
    "This exercise uses the procedure outlined in Section 7.4.2 to estimate technical inefficiencies of Indonesian rice farms. The predictions employ the Gaussian copula to draw $S = 10,000$ observations from the joint distribution of $u$ and $\\epsilon$. For the simulated sample, the predictions are based on the Nadaraya-Watson estimator which uses the Gaussian kernel with a common bandwidth parameter chosen by cross-validation.\n",
    "\n",
    "The table reports predicted inefficiencies in each time period for ten farms, namely those that are at the 0.05, 0.15, . . . , 0.95 fractiles of the sample of inefficiencies at $t = 1$. The first two columns report the rank and the quantile of the selected farms. The third and fourth columns contain the sample variation and sample mean over $t$, respectively, of the inefficiency estimates, which are reported in columns 5 through 10. The last row of the tables reports column-wise averages over the ten farms. (The results is somewhat different from the original results by Amsler et al. (2014), who used GAUSS and obtained a smaller average $\\hat{\\sigma}$ compared to JLMS.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a0822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.linalg import expm, logm, norm\n",
    "\n",
    "\n",
    "def JLMS_panel_technical_inefficiency_scores(theta, y, X):\n",
    "    N = 171\n",
    "    T = 6\n",
    "\n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:14]\n",
    "    sigma = theta[-2]\n",
    "    _lambda = theta[-1]\n",
    "\n",
    "    u_hat = np.zeros((N, T))\n",
    "    for t in range(T):\n",
    "\n",
    "        eps_t = y[t] - alpha - X[t] @ beta\n",
    "        b = (eps_t * _lambda) / sigma\n",
    "        u_hat[:, t] = ((sigma * _lambda) / (1 + _lambda**2)) * (\n",
    "            stats.norm.pdf(b) / (1 - stats.norm.cdf(b)) - b\n",
    "        )\n",
    "\n",
    "\n",
    "    return u_hat\n",
    "\n",
    "\n",
    "def inverse_mapping_vec(gamma, tol_value=1e-8):\n",
    "    C = []\n",
    "    iter_number = -1\n",
    "\n",
    "    # Check if input is of proper format: gamma is of suitable length\n",
    "    if not isinstance(gamma, (np.ndarray, list)):\n",
    "        raise ValueError\n",
    "    if isinstance(gamma, np.ndarray):\n",
    "        if gamma.ndim != 1:\n",
    "            raise ValueError\n",
    "    n = 0.5 * (1 + np.sqrt(1 + 8 * len(gamma)))\n",
    "    if not all([n.is_integer(), n > 1]):\n",
    "        raise ValueError\n",
    "\n",
    "    # Check if tolerance value belongs to a proper interval\n",
    "    # and change it to the default value otherwise\n",
    "    if not (0 < tol_value <= 1e-4):\n",
    "        tol_value = 1e-8\n",
    "        print(\"Warning: tolerance value has been changed to default\")\n",
    "\n",
    "    # Place elements from gamma into off-diagonal parts\n",
    "    # and put zeros on the main diagonal of [n x n] symmetric matrix A\n",
    "    n = int(n)\n",
    "    A = np.zeros(shape=(n, n))\n",
    "    A[np.triu_indices(n, 1)] = gamma\n",
    "    A = A + A.T\n",
    "\n",
    "    # Read properties of the input matrix\n",
    "    diag_vec = np.diag(A)  # get current diagonal\n",
    "    diag_ind = np.diag_indices_from(\n",
    "        A\n",
    "    )  # get row and column indices of diagonal elements\n",
    "\n",
    "    # Iterative algorithm to get the proper diagonal vector\n",
    "    dist = np.sqrt(n)\n",
    "    while dist > np.sqrt(n) * tol_value:\n",
    "        diag_delta = np.log(np.diag(expm(A)))\n",
    "        diag_vec = diag_vec - diag_delta\n",
    "        A[diag_ind] = diag_vec\n",
    "        dist = norm(diag_delta)\n",
    "        iter_number += 1\n",
    "\n",
    "    # Get a unique reciprocal correlation matrix\n",
    "    C = expm(A)\n",
    "    np.fill_diagonal(C, 1)\n",
    "\n",
    "    return C\n",
    "\n",
    "def NW_panel_technical_inefficiency_scores(theta, y, X, U_hat):\n",
    "    N = 171\n",
    "    T = 6\n",
    "\n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:14]\n",
    "    sigma = theta[-2]\n",
    "    _lambda = theta[-1]\n",
    "\n",
    "    sigma2u = ((_lambda * sigma) / (1 + _lambda)) ** 2\n",
    "    sigma2v = ((sigma) / (1 + _lambda)) ** 2\n",
    "\n",
    "    obs_eps = np.zeros((N, T))\n",
    "    for t in range(T):\n",
    "        obs_eps[:, t] = y[t] - alpha - X[t] @ beta\n",
    "\n",
    "    # Simulated half normal RVs from the estimated Gaussian copula\n",
    "    simulated_u = stats.halfnorm.ppf(\n",
    "        U_hat, loc=np.zeros(T), scale=np.array([np.sqrt(sigma2u) for x in range(T)])\n",
    "    )\n",
    "    # Simulated random noise for all T\n",
    "    simulated_v = stats.multivariate_normal.rvs(\n",
    "        size=10000, mean=np.zeros(T), cov=np.eye(T) * sigma2v, random_state=123\n",
    "    )\n",
    "    simulated_eps = simulated_v - simulated_u\n",
    "\n",
    "    # Rule of thumb bandwidth\n",
    "    h_eps = (\n",
    "        1.06\n",
    "        * 10000 ** (-1 / 5)\n",
    "        * (max(np.std(simulated_eps), stats.iqr(simulated_eps) / 1.34))\n",
    "    )\n",
    "\n",
    "    u_hat = np.zeros((N, T))\n",
    "    for i in range(N):\n",
    "        panel_i_kernel_regression_results = np.zeros(T)\n",
    "        eps_kernel = np.zeros((10000, T))\n",
    "        # Construct the kernel distances for all T time periods\n",
    "        for t in range(T):\n",
    "            eps_kernel[:, t] = stats.norm.pdf(\n",
    "                (simulated_eps[:, t] - obs_eps[i, t]) / h_eps\n",
    "            )\n",
    "        kernel_product = np.prod(eps_kernel, 1)\n",
    "        for j in range(T):\n",
    "            panel_i_kernel_regression_results[j] = np.sum(\n",
    "                kernel_product * simulated_u[:, j]\n",
    "            ) / np.sum(kernel_product)\n",
    "        u_hat[i, :] = panel_i_kernel_regression_results\n",
    "\n",
    "    return u_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db16b2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jleu0010/Documents/GitHub/EPA-Copula-SFA-website/venv/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nadaraya Watson Scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q0/9kb46g7n1_57dfk7vtdsv8t4w169ch/T/ipykernel_20238/1977337797.py:100: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  NW_technical_inefficiency_results.iloc[:, -6:] = NW_u_hat_for_table\n",
      "/Users/jleu0010/Documents/GitHub/EPA-Copula-SFA-website/venv/lib/python3.8/site-packages/IPython/core/formatters.py:344: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Fractile</th>\n",
       "      <th>Std(u)</th>\n",
       "      <th>Mean(u)</th>\n",
       "      <th>t=1</th>\n",
       "      <th>t=2</th>\n",
       "      <th>t=3</th>\n",
       "      <th>t=4</th>\n",
       "      <th>t=5</th>\n",
       "      <th>t=6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.245873</td>\n",
       "      <td>0.403594</td>\n",
       "      <td>0.024135</td>\n",
       "      <td>0.318582</td>\n",
       "      <td>0.738054</td>\n",
       "      <td>0.688861</td>\n",
       "      <td>0.270232</td>\n",
       "      <td>0.381701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.169122</td>\n",
       "      <td>0.239246</td>\n",
       "      <td>0.052832</td>\n",
       "      <td>0.124861</td>\n",
       "      <td>0.509873</td>\n",
       "      <td>0.430818</td>\n",
       "      <td>0.138350</td>\n",
       "      <td>0.178745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.172628</td>\n",
       "      <td>0.251123</td>\n",
       "      <td>0.096750</td>\n",
       "      <td>0.066012</td>\n",
       "      <td>0.585722</td>\n",
       "      <td>0.327563</td>\n",
       "      <td>0.202194</td>\n",
       "      <td>0.228496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.127765</td>\n",
       "      <td>0.243374</td>\n",
       "      <td>0.158353</td>\n",
       "      <td>0.491428</td>\n",
       "      <td>0.309240</td>\n",
       "      <td>0.098721</td>\n",
       "      <td>0.214911</td>\n",
       "      <td>0.187592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.187637</td>\n",
       "      <td>0.358608</td>\n",
       "      <td>0.195284</td>\n",
       "      <td>0.089106</td>\n",
       "      <td>0.250239</td>\n",
       "      <td>0.535277</td>\n",
       "      <td>0.505213</td>\n",
       "      <td>0.576530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>94</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.145152</td>\n",
       "      <td>0.196747</td>\n",
       "      <td>0.227607</td>\n",
       "      <td>0.125899</td>\n",
       "      <td>0.506787</td>\n",
       "      <td>0.104959</td>\n",
       "      <td>0.108918</td>\n",
       "      <td>0.106310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>111</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.100962</td>\n",
       "      <td>0.287877</td>\n",
       "      <td>0.290810</td>\n",
       "      <td>0.273255</td>\n",
       "      <td>0.249796</td>\n",
       "      <td>0.501644</td>\n",
       "      <td>0.218975</td>\n",
       "      <td>0.192783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.235708</td>\n",
       "      <td>0.332172</td>\n",
       "      <td>0.339868</td>\n",
       "      <td>0.141119</td>\n",
       "      <td>0.475475</td>\n",
       "      <td>0.772116</td>\n",
       "      <td>0.095174</td>\n",
       "      <td>0.169282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>145</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.225826</td>\n",
       "      <td>0.392722</td>\n",
       "      <td>0.466092</td>\n",
       "      <td>0.489747</td>\n",
       "      <td>0.082886</td>\n",
       "      <td>0.791562</td>\n",
       "      <td>0.298176</td>\n",
       "      <td>0.227871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>162</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.222817</td>\n",
       "      <td>0.544130</td>\n",
       "      <td>0.667975</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.315315</td>\n",
       "      <td>0.917145</td>\n",
       "      <td>0.307852</td>\n",
       "      <td>0.396496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrr}\n",
       "\\toprule\n",
       "{} &  Rank &  Fractile &    Std(u) &   Mean(u) &       t=1 &       t=2 &       t=3 &       t=4 &       t=5 &       t=6 \\\\\n",
       "\\midrule\n",
       "0 &     9 &      0.05 &  0.245873 &  0.403594 &  0.024135 &  0.318582 &  0.738054 &  0.688861 &  0.270232 &  0.381701 \\\\\n",
       "1 &    26 &      0.15 &  0.169122 &  0.239246 &  0.052832 &  0.124861 &  0.509873 &  0.430818 &  0.138350 &  0.178745 \\\\\n",
       "2 &    43 &      0.25 &  0.172628 &  0.251123 &  0.096750 &  0.066012 &  0.585722 &  0.327563 &  0.202194 &  0.228496 \\\\\n",
       "3 &    60 &      0.35 &  0.127765 &  0.243374 &  0.158353 &  0.491428 &  0.309240 &  0.098721 &  0.214911 &  0.187592 \\\\\n",
       "4 &    77 &      0.45 &  0.187637 &  0.358608 &  0.195284 &  0.089106 &  0.250239 &  0.535277 &  0.505213 &  0.576530 \\\\\n",
       "5 &    94 &      0.55 &  0.145152 &  0.196747 &  0.227607 &  0.125899 &  0.506787 &  0.104959 &  0.108918 &  0.106310 \\\\\n",
       "6 &   111 &      0.65 &  0.100962 &  0.287877 &  0.290810 &  0.273255 &  0.249796 &  0.501644 &  0.218975 &  0.192783 \\\\\n",
       "7 &   128 &      0.75 &  0.235708 &  0.332172 &  0.339868 &  0.141119 &  0.475475 &  0.772116 &  0.095174 &  0.169282 \\\\\n",
       "8 &   145 &      0.85 &  0.225826 &  0.392722 &  0.466092 &  0.489747 &  0.082886 &  0.791562 &  0.298176 &  0.227871 \\\\\n",
       "9 &   162 &      0.95 &  0.222817 &  0.544130 &  0.667975 &  0.660000 &  0.315315 &  0.917145 &  0.307852 &  0.396496 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "   Rank  Fractile    Std(u)   Mean(u)       t=1       t=2       t=3       t=4  \\\n",
       "0     9      0.05  0.245873  0.403594  0.024135  0.318582  0.738054  0.688861   \n",
       "1    26      0.15  0.169122  0.239246  0.052832  0.124861  0.509873  0.430818   \n",
       "2    43      0.25  0.172628  0.251123  0.096750  0.066012  0.585722  0.327563   \n",
       "3    60      0.35  0.127765  0.243374  0.158353  0.491428  0.309240  0.098721   \n",
       "4    77      0.45  0.187637  0.358608  0.195284  0.089106  0.250239  0.535277   \n",
       "5    94      0.55  0.145152  0.196747  0.227607  0.125899  0.506787  0.104959   \n",
       "6   111      0.65  0.100962  0.287877  0.290810  0.273255  0.249796  0.501644   \n",
       "7   128      0.75  0.235708  0.332172  0.339868  0.141119  0.475475  0.772116   \n",
       "8   145      0.85  0.225826  0.392722  0.466092  0.489747  0.082886  0.791562   \n",
       "9   162      0.95  0.222817  0.544130  0.667975  0.660000  0.315315  0.917145   \n",
       "\n",
       "        t=5       t=6  \n",
       "0  0.270232  0.381701  \n",
       "1  0.138350  0.178745  \n",
       "2  0.202194  0.228496  \n",
       "3  0.214911  0.187592  \n",
       "4  0.505213  0.576530  \n",
       "5  0.108918  0.106310  \n",
       "6  0.218975  0.192783  \n",
       "7  0.095174  0.169282  \n",
       "8  0.298176  0.227871  \n",
       "9  0.307852  0.396496  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jleu0010/Documents/GitHub/EPA-Copula-SFA-website/venv/lib/python3.8/site-packages/IPython/core/formatters.py:344: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Std(u)</th>\n",
       "      <th>Mean(u)</th>\n",
       "      <th>t=1</th>\n",
       "      <th>t=2</th>\n",
       "      <th>t=3</th>\n",
       "      <th>t=4</th>\n",
       "      <th>t=5</th>\n",
       "      <th>t=6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.183349</td>\n",
       "      <td>0.324959</td>\n",
       "      <td>0.251971</td>\n",
       "      <td>0.278001</td>\n",
       "      <td>0.402339</td>\n",
       "      <td>0.516867</td>\n",
       "      <td>0.235999</td>\n",
       "      <td>0.264581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrr}\n",
       "\\toprule\n",
       "{} &    Std(u) &   Mean(u) &       t=1 &       t=2 &       t=3 &       t=4 &       t=5 &       t=6 \\\\\n",
       "\\midrule\n",
       "Average &  0.183349 &  0.324959 &  0.251971 &  0.278001 &  0.402339 &  0.516867 &  0.235999 &  0.264581 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "           Std(u)   Mean(u)       t=1       t=2       t=3       t=4       t=5  \\\n",
       "Average  0.183349  0.324959  0.251971  0.278001  0.402339  0.516867  0.235999   \n",
       "\n",
       "              t=6  \n",
       "Average  0.264581  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ricefarm = pd.read_csv(r\"ricefarm.csv\")\n",
    "\n",
    "ricefarm = ricefarm.iloc[:, [0, 1, 3, 5, 6, 7, 8, 14, 16]]\n",
    "\n",
    "# Create ID dummies\n",
    "dar = np.round(ricefarm[\"ID\"] / 100000).astype(int)\n",
    "id_dummies = pd.get_dummies(dar, prefix=\"DR\").astype(int)\n",
    "ricefarm = pd.concat([ricefarm, id_dummies.iloc[:, :-1]], axis=1)\n",
    "\n",
    "# Create rice variety dummies\n",
    "rice_dummies = pd.get_dummies(ricefarm.iloc[:, 2], prefix=\"VAR\").astype(int)\n",
    "ricefarm = pd.concat([ricefarm, rice_dummies.iloc[:, 1:]], axis=1)\n",
    "\n",
    "# Recode TSP as logs and zeros\n",
    "ricefarm[ricefarm.columns[5]] = np.where(\n",
    "    ricefarm.iloc[:, 5] > 0, np.log(ricefarm.iloc[:, 5]), 0\n",
    ")\n",
    "\n",
    "# Convert pesticide usage to a dummy\n",
    "ricefarm[ricefarm.columns[6]] = (ricefarm.iloc[:, 6] != 0).astype(int)\n",
    "\n",
    "# Reorder the data\n",
    "ricefarm = ricefarm.iloc[:, [8, 3, 4, 5, 7, 1, 6, 14, 15, 9, 10, 11, 12, 13]]\n",
    "\n",
    "y = np.log(ricefarm.iloc[:, 0].values)\n",
    "X = ricefarm.iloc[:, 1:].values\n",
    "\n",
    "# Log covariates\n",
    "X[:, 0:2] = np.log(X[:, 0:2])\n",
    "X[:, 3] = np.log(X[:, 3])\n",
    "X[:, 4] = np.log(X[:, 4])\n",
    "\n",
    "# Cross sections of  y and X\n",
    "N = 171\n",
    "T = 6\n",
    "y_t = []\n",
    "X_t = []\n",
    "for t in range(T):\n",
    "    if t == 0:\n",
    "        y_t.append(y[:N])\n",
    "        X_t.append(X[:N, :])\n",
    "    else:\n",
    "        y_t.append(y[t * N : (t + 1) * N])\n",
    "        X_t.append(X[t * N : (t + 1) * N, :])\n",
    "        \n",
    "# Import the model parameters from exercise 5.8\n",
    "theta = np.loadtxt(\"exercise_4_8_theta_python.txt\", delimiter=\",\")\n",
    "gamma = np.loadtxt(\"exercise_4_8_gamma_python.txt\", delimiter=\",\")\n",
    "\n",
    "# Estimation of technical inefficiencies\n",
    "# Get idx of percentile firms\n",
    "percetile_firm_idx = np.round(np.arange(0.05, 1, 0.1) * N).astype(int)\n",
    "\n",
    "# NW technical inefficiency estimates\n",
    "Rho_hat = inverse_mapping_vec(gamma)\n",
    "# Draw T dependent unfirm RV's for each s_{1}, ... 10000\n",
    "Z = stats.multivariate_normal.rvs(\n",
    "    size=10000, mean=np.zeros(T), cov=np.eye(T), random_state=1234\n",
    ")\n",
    "A = np.linalg.cholesky(Rho_hat)\n",
    "U_hat = stats.norm.cdf(\n",
    "    Z @ A, loc=np.zeros(T), scale=np.ones(T)\n",
    ")  # Dependent uniform RVs from a gaussian copula\n",
    "NW_u_hat = NW_panel_technical_inefficiency_scores(\n",
    "    theta=theta, y=y_t, X=X_t, U_hat=U_hat\n",
    ")\n",
    "\n",
    "sort_idx = NW_u_hat[:, 0].argsort()\n",
    "# Sort technical inefficiency scores by t=1 values\n",
    "sorted_t1_NW_u_hat = NW_u_hat[sort_idx]\n",
    "# Mean technical inefficiency for each firms over all t\n",
    "NW_mean = np.mean(NW_u_hat, axis=1)\n",
    "sorted_t1_NW_mean = NW_mean[sort_idx]\n",
    "# Standard deviation of technical inefficiency for each firm over all t\n",
    "NW_std = np.std(NW_u_hat, axis=1)\n",
    "sorted_t1_NW_std = NW_std[sort_idx]\n",
    "\n",
    "NW_u_hat_for_table = sorted_t1_NW_u_hat[percetile_firm_idx-1, :]\n",
    "NW_std_u_hat_for_table = sorted_t1_NW_std[percetile_firm_idx-1]\n",
    "NW_mean_u_hat_for_table = sorted_t1_NW_mean[percetile_firm_idx-1]\n",
    "\n",
    "NW_technical_inefficiency_results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Rank\",\n",
    "        \"Fractile\",\n",
    "        \"Std(u)\",\n",
    "        \"Mean(u)\",\n",
    "        \"t=1\",\n",
    "        \"t=2\",\n",
    "        \"t=3\",\n",
    "        \"t=4\",\n",
    "        \"t=5\",\n",
    "        \"t=6\",\n",
    "    ]\n",
    ")\n",
    "NW_technical_inefficiency_results[\"Rank\"] = percetile_firm_idx\n",
    "NW_technical_inefficiency_results[\"Fractile\"] = np.arange(0.05, 1, 0.1)\n",
    "NW_technical_inefficiency_results[\"Std(u)\"] = NW_std_u_hat_for_table\n",
    "NW_technical_inefficiency_results[\"Mean(u)\"] = NW_mean_u_hat_for_table\n",
    "NW_technical_inefficiency_results.iloc[:, -6:] = NW_u_hat_for_table\n",
    "\n",
    "# Compute averages\n",
    "average_NW = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            np.mean(NW_std_u_hat_for_table).reshape(-1, 1).T,\n",
    "            np.mean(NW_mean_u_hat_for_table).reshape(-1, 1).T,\n",
    "            np.mean(NW_u_hat_for_table, axis=0).reshape(-1, 1).T,\n",
    "        ],\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"Std(u)\", \"Mean(u)\", \"t=1\", \"t=2\", \"t=3\", \"t=4\", \"t=5\", \"t=6\"],\n",
    "    index=[\"Average\"],\n",
    ")\n",
    "\n",
    "print(\"Nadaraya Watson Scores\")\n",
    "display(NW_technical_inefficiency_results)\n",
    "display(average_NW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d5e730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd8deb",
   "metadata": {},
   "source": [
    "## Exercise 7.3: Improved JLMS estimator for the 72 US utilities\n",
    "This exercise applies the predictors from Sections 7.4.2 and 7.6.1 to the US electricity generation dataset comprised of 72 firms over the period 1986-1999. The data file is steamelectric.xlsx. The output variable is net steam electric power in megawatt-hours and the values of the three inputs (fuel, labour and maintenance, and capital) are obtained by dividing respective expenses by relevant input prices; the data are cleaned following Amsler et al. (2021). The price of fuel aggregate is a Tornqvist price index of fuels. The aggregate price of labor and maintenance is a cost-share weighted price, where the price of labor is a company-wide average wage rate and the price of maintenance and other supplies is a price index of electrical supplies from the U.S. Bureau of Labor Statistics. The price of capital is the yield of the firm’s latest issue of long term debt adjusted for appreciation and depreciation. This data has recently been used by Amsler et al. (2021) and Lai and Kumbhakar (2019).\n",
    "\n",
    "The first table reports the average estimated technical inefficiencies by the year as well as the average of their estimated standard deviations, which can be viewed as measures of the unexplained variation in $u$. (We use a different random seed to Amsler et al. (2023) so our estimates are different from theirs.) Table 7.3 contains three predictors based on the panel model where the model parameters are estimated by MSLE. The JLMS estimator uses only the contemporaneous $\\epsilon_{it}$ in the conditioning set. The Nadaraya-Watson [NW] and Local Linear Forest [LLF] estimators use the expanded conditioning set and the APS14 approach to estimating the conditional expectation, described in Section 7.4.2. The second table contains three estimators based on the model with endogeneity where the parameters of the model are estimated by MSLE. For this part, we ignore the panel nature of the dataset (by viewing it as a pooled cross-section, i.e. by assuming independence over t as well as over i) and focus on the presence of the two types of inefficiency. In this case, JLMS is the estimator that does not use the $\\omega_{j}’s while NW and LLF include them in the conditioning set using the APS16 approach described in Section 7.6.1.\n",
    "\n",
    "In the second table the average technical inefficiency score over all firms and years is similar across estimators, regardless of whether or not we condition on the allocative inefficiency terms. Over all years, the average standard deviation of technical inefficiency scores from the Local Linear Forest is considerably smaller than those from the JLMS or Nadaraya-Watson estimators. In Table 7.3, the mean technical inefficiency scores are always larger for all estimators and years in the data. Again, we find that the average technical inefficiency score over all firms and years is similar for all estimators. However, there are noticeable differences in the mean technical inefficiency score of estimators between years, for example in years 86, 87, 91 and 98. Finally, we note that the mean standard deviations for both non-parametric estimators are remarkably smaller than those associated with the JLMS estimator, suggesting that we explain a much larger portion of the unconditional variation in u. The technical inefficiency scores computed from the Local Linear Forest estimator are always associated with the smallest standard deviations.\n",
    "\n",
    "Note: Users should uncomment the line calling the subprocess and change the file path to their own R executable location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a89237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.linalg import expm, logm, norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def direct_mapping_mat(C):\n",
    "    gamma = []\n",
    "\n",
    "    try:\n",
    "        # Check if input is of proper format: C is 2D np.array of suitable dimensions\n",
    "        # and is positive-definite correlation matrix\n",
    "        if not isinstance(C, np.ndarray):\n",
    "            raise ValueError\n",
    "        if C.ndim != 2:\n",
    "            raise ValueError\n",
    "        if C.shape[0] != C.shape[1]:\n",
    "            raise ValueError\n",
    "        if not all(\n",
    "            [\n",
    "                np.all(np.abs(np.diag(C) - np.ones(C.shape[0])) < 1e-8),\n",
    "                np.all(np.linalg.eigvals(C) > 0),\n",
    "                np.all(np.abs(C - C.T) < 1e-8),\n",
    "            ]\n",
    "        ):\n",
    "            raise ValueError\n",
    "\n",
    "        # Apply matrix log-transformation to C and get off-diagonal elements\n",
    "        A = logm(C)\n",
    "        gamma = A[np.triu_indices(C.shape[0], 1)]\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Error: input is of a wrong format\")\n",
    "\n",
    "    return gamma\n",
    "\n",
    "\n",
    "def inverse_mapping_vec(gamma, tol_value=1e-8):\n",
    "    C = []\n",
    "    iter_number = -1\n",
    "\n",
    "    # Check if input is of proper format: gamma is of suitable length\n",
    "    if not isinstance(gamma, (np.ndarray, list)):\n",
    "        raise ValueError\n",
    "    if isinstance(gamma, np.ndarray):\n",
    "        if gamma.ndim != 1:\n",
    "            raise ValueError\n",
    "    n = 0.5 * (1 + np.sqrt(1 + 8 * len(gamma)))\n",
    "    if not all([n.is_integer(), n > 1]):\n",
    "        raise ValueError\n",
    "\n",
    "    # Check if tolerance value belongs to a proper interval\n",
    "    # and change it to the default value otherwise\n",
    "    if not (0 < tol_value <= 1e-4):\n",
    "        tol_value = 1e-8\n",
    "        print(\"Warning: tolerance value has been changed to default\")\n",
    "\n",
    "    # Place elements from gamma into off-diagonal parts\n",
    "    # and put zeros on the main diagonal of [n x n] symmetric matrix A\n",
    "    n = int(n)\n",
    "    A = np.zeros(shape=(n, n))\n",
    "    A[np.triu_indices(n, 1)] = gamma\n",
    "    A = A + A.T\n",
    "\n",
    "    # Read properties of the input matrix\n",
    "    diag_vec = np.diag(A)  # get current diagonal\n",
    "    diag_ind = np.diag_indices_from(\n",
    "        A\n",
    "    )  # get row and column indices of diagonal elements\n",
    "\n",
    "    # Iterative algorithm to get the proper diagonal vector\n",
    "    dist = np.sqrt(n)\n",
    "    while dist > np.sqrt(n) * tol_value:\n",
    "        diag_delta = np.log(np.diag(expm(A)))\n",
    "        diag_vec = diag_vec - diag_delta\n",
    "        A[diag_ind] = diag_vec\n",
    "        dist = norm(diag_delta)\n",
    "        iter_number += 1\n",
    "\n",
    "    # Get a unique reciprocal correlation matrix\n",
    "    C = expm(A)\n",
    "    np.fill_diagonal(C, 1)\n",
    "\n",
    "    return C\n",
    "\n",
    "def Loglikelihood_Gaussian_copula_cross_sectional_application_SFA(theta, y, X, P, us_Sxn, n_inputs, S):\n",
    "    \n",
    "    n = len(y)\n",
    "    \n",
    "    alpha = np.exp(theta[0])\n",
    "    beta = np.exp(theta[1:n_inputs+1])\n",
    "    sigma2_v = theta[1+n_inputs]\n",
    "    sigma2_u = theta[2+n_inputs]\n",
    "    sigma2_w = np.exp(theta[3+n_inputs:(3+n_inputs)+(n_inputs-1)])\n",
    "    mu_W = theta[(3+n_inputs)+(n_inputs-2)+1:(3+n_inputs)+(n_inputs-2)+(n_inputs)]\n",
    "\n",
    "    rhos_log_form = theta[-3:]\n",
    "    \n",
    "    Rho_hat = inverse_mapping_vec(rhos_log_form)\n",
    "\n",
    "    # Cobb-Douglas production function\n",
    "    eps = y - np.log(alpha) - X@beta \n",
    "    W = (np.tile(X[:, 0].reshape(-1, 1), (1, n_inputs-1)) - X[:,1:]) - (P[:,1:] - np.tile(P[:, 0].reshape(-1, 1), (1, n_inputs-1)) +(np.log(beta[0]) - np.log(beta[1:])))\n",
    "    # Marginal density of allocative inefficiency terms\n",
    "    Den_W = stats.norm.pdf(W, np.tile(mu_W, (n, 1)), np.tile(np.sqrt(sigma2_w), (n, 1)))\n",
    "    CDF_W = stats.norm.cdf(W, np.tile(mu_W, (n, 1)), np.tile(np.sqrt(sigma2_w), (n, 1)))\n",
    "\n",
    "    eps_Sxn = np.repeat(eps.reshape(-1,1), S, axis=1).T        \n",
    "    us_Sxn_scaled = np.sqrt(sigma2_u)*us_Sxn\n",
    "    CdfUs = 2*(stats.norm.cdf(np.sqrt(sigma2_u)*us_Sxn, 0, np.sqrt(sigma2_u)) -0.5)\n",
    "    eps_plus_us = eps_Sxn + us_Sxn_scaled\n",
    "    den_eps_plus_us = stats.norm.pdf(eps_plus_us, 0, np.sqrt(sigma2_v))\n",
    "\n",
    "    #Evaluate the integral via simulation (to integrate out u from eps+u)\n",
    "    simulated_copula_pdfs = np.zeros((S,n))\n",
    "    CDF_W_rep = {}\n",
    "    for i in range(n_inputs-1):\n",
    "        CDF_W_rep[i] = np.repeat(CDF_W[:, i].reshape(-1,1), S, axis=1).T        \n",
    "        \n",
    "    for j in range(n):\n",
    "        CDF_w_j = np.zeros((S, n_inputs-1))\n",
    "        for i in range(n_inputs-1):\n",
    "            CDF_w_j[:,i] = CDF_W_rep[i][:,j]\n",
    "        U = np.concatenate([stats.norm.ppf(CdfUs[:, j]).reshape(-1,1),\n",
    "                            stats.norm.ppf(CDF_w_j)], \n",
    "                           axis=1)\n",
    "        c123 = stats.multivariate_normal.pdf(U, \n",
    "                                             mean = np.array([0, 0, 0]), \n",
    "                                             cov=Rho_hat)/ np.prod(stats.norm.pdf(U), axis=1)\n",
    "        simulated_copula_pdfs[:,j] = c123\n",
    "\n",
    "    Integral = np.mean(simulated_copula_pdfs*den_eps_plus_us, \n",
    "                       axis=0) #Evaluation of the integral over S simulated samples. Column-wise mean.\n",
    "    # Joint desnity. product of marginal density of w_{i}, i = 1, ..., n_inputs and the joint density f(\\epsilon, w)\n",
    "    prod_Den_W = np.prod(Den_W, 1)\n",
    "    DenAll = prod_Den_W*Integral;\n",
    "    DenAll[DenAll < 1e-6] = 1e-6\n",
    "    r = np.log(np.sum(beta))\n",
    "    logDen = np.log(DenAll) + r\n",
    "    logL = -np.sum(logDen)\n",
    "    \n",
    "    return logL\n",
    "    \n",
    "def estimate_Jondrow1982_u_hat(theta, n_inputs, n_corr_terms, y, X):\n",
    "    \n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:n_inputs+1]\n",
    "    sigma2_v = theta[1+n_inputs]\n",
    "    sigma2_u = theta[2+n_inputs]\n",
    "    \n",
    "    obs_eps = y - np.log(alpha) - X@beta\n",
    "    _lambda = np.sqrt(sigma2_u/sigma2_v)\n",
    "    sigma = np.sqrt(sigma2_u+sigma2_v)\n",
    "    sig_star = np.sqrt(sigma2_u*sigma2_v/(sigma**2))\n",
    "    u_hat = sig_star*(((stats.norm.pdf(_lambda*obs_eps/sigma))/(1-stats.norm.cdf(_lambda*obs_eps/sigma))) - ((_lambda*obs_eps)/sigma))\n",
    "    V_u_hat = sig_star**2*(1+stats.norm.pdf(_lambda*obs_eps/sigma)/(1-stats.norm.cdf(_lambda*obs_eps/sigma))*_lambda*obs_eps/sigma-(stats.norm.pdf(_lambda*obs_eps/sigma)/(1-stats.norm.cdf(_lambda*obs_eps/sigma)))**2)\n",
    "    \n",
    "    return u_hat, V_u_hat\n",
    "\n",
    "def Estimate_Jondrow1982_u_hat_panel_SFA_application_RS2007(alpha, beta, delta, sigma2_v, y, X, T, N):\n",
    "    \n",
    "    obs_eps = {}\n",
    "    u_hat = np.zeros((N, T))\n",
    "    V_u_hat = np.zeros((N, T))\n",
    "    for t in range(T):\n",
    "        sigma2_u = np.exp(delta[0] + delta[1]*t)\n",
    "        _lambda = np.sqrt(sigma2_u)/np.sqrt(sigma2_v)\n",
    "        sigma = np.sqrt(sigma2_u+sigma2_v)\n",
    "        sig_star = np.sqrt(sigma2_u*sigma2_v/(sigma**2))\n",
    "    \n",
    "        u_hat_ = np.full(N, np.nan)\n",
    "        V_u_hat_ = np.full(N, np.nan)\n",
    "        obs_eps[t] = y[t] - np.log(alpha) - X[t]@beta # composed errors from the production function equation (i.e residuals from the production function)\n",
    "        b = (obs_eps[t]*_lambda)/sigma\n",
    "        u_hat_tmp = ((sigma*_lambda)/(1 + _lambda**2))*(stats.norm.pdf(b)/(1 - stats.norm.cdf(b)) - b)\n",
    "        V_u_hat_tmp = sig_star**2*(1+stats.norm.pdf(b)/(1-stats.norm.cdf(b))*b-(stats.norm.pdf(b)/(1-stats.norm.cdf(b)))**2)\n",
    "\n",
    "        u_hat_[:len(u_hat_tmp)] = u_hat_tmp\n",
    "        V_u_hat_[:len(V_u_hat_tmp)] = V_u_hat_tmp\n",
    "        u_hat[:, t] = u_hat_\n",
    "        V_u_hat[:, t] = V_u_hat_\n",
    "        \n",
    "    return u_hat, V_u_hat\n",
    "\n",
    "def simulate_error_components(Rho, n_inputs, S_kernel, seed):\n",
    "    \n",
    "    chol_of_rho = np.linalg.cholesky(Rho)\n",
    "    Z = stats.multivariate_normal.rvs(mean=np.zeros(n_inputs), \n",
    "                                      cov=np.eye(n_inputs), \n",
    "                                      size=S_kernel, \n",
    "                                      random_state=seed)\n",
    "    X = chol_of_rho@Z.T\n",
    "    U = stats.norm.cdf(X)\n",
    "     \n",
    "    return U.T\n",
    "\n",
    "def Estimate_NW_u_hat_conditional_eps_panel_SFA_RS2007(theta, y, X, N, T, k, U_hat, S_kernel):\n",
    "    \n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:n_inputs+1]\n",
    "    delta = theta[4:6]\n",
    "    sigma2_v = theta[6]\n",
    "  \n",
    "    # Observed variables\n",
    "    obs_eps = {}\n",
    "    for t in range(T):\n",
    "        eps__ = np.full(N, np.nan)\n",
    "        tmp_eps = y[t] - np.log(alpha) - X[t]@beta\n",
    "        eps__[:len(tmp_eps)] = tmp_eps\n",
    "        obs_eps[t] = eps__\n",
    "\n",
    "    # Simulated variables\n",
    "    simulated_v = stats.multivariate_normal.rvs(mean=np.zeros(T), \n",
    "                                                cov=np.eye(T)*sigma2_v, \n",
    "                                                size=S_kernel) #simulate random noise for all T panels \n",
    "    simulated_u = np.zeros((S_kernel, T))\n",
    "    simulated_eps = np.zeros((S_kernel, T))\n",
    "    for t in range(T):\n",
    "        sigma2_u = np.exp(delta[0] + delta[1]*t)\n",
    "        simulated_u[:, t] = np.sqrt(sigma2_u)*stats.norm.ppf((U_hat[:,t]+1)/2)\n",
    "        simulated_eps[:,t] = simulated_v[:,t] - simulated_u[:,t]\n",
    "\n",
    "    # Bandwidth information for each conditioning variable\n",
    "    h_eps = np.zeros(T)\n",
    "    for t in range(T):\n",
    "        h_eps[t] = 1.06*S_kernel**(-1/5)*(max(np.std(simulated_eps[:,t]), stats.iqr(simulated_eps[:,t])/1.34))\n",
    "\n",
    "    # kernel estimates for E[u_{t}|eps_{1}, ..., eps_{T}]\n",
    "    # V[u|eps, w1, w2] = E[u^2|w2, w3, eps] - (E[u|w2, w3, eps])^2\n",
    "    u_hat = np.zeros((N, T))\n",
    "    u_hat2 = np.zeros((N, T))\n",
    "    all_eps = np.concatenate([x.reshape(-1,1) for x in obs_eps.values()], axis=1)\n",
    "    for i in range(N): \n",
    "        obs_eps_i = all_eps[i, :]\n",
    "\n",
    "        panel_i_kernel_regression_results = np.zeros(T)\n",
    "        panel_i_kernel_regression_results2 = np.zeros(T)\n",
    "        eps_kernel = np.zeros((S_kernel, T))\n",
    "        # Construct the kernel distances for all T time periods\n",
    "        for t in range(T):\n",
    "            eps_kernel[:,t] = stats.norm.pdf((simulated_eps[:,t] - obs_eps[t][i])/h_eps[t])\n",
    "\n",
    "        out = eps_kernel[:,np.all(~np.isnan(eps_kernel), axis=0)]\n",
    "        kernel_product = np.prod(out, 1)\n",
    "        for j in range(T): # NW for each t = 1, .., T observation in each panel i\n",
    "            if not np.isnan(obs_eps_i[j]):\n",
    "                panel_i_kernel_regression_results[j] = np.sum(kernel_product*simulated_u[:, j])/np.sum(kernel_product)\n",
    "                panel_i_kernel_regression_results2[j] = np.sum(kernel_product*simulated_u[:,j]**2)/np.sum(kernel_product)\n",
    "            else:\n",
    "                panel_i_kernel_regression_results[j] = np.nan\n",
    "                panel_i_kernel_regression_results2[j] = np.nan\n",
    "\n",
    "        u_hat[i, :] = panel_i_kernel_regression_results\n",
    "        u_hat2[i,  :] = panel_i_kernel_regression_results2\n",
    "\n",
    "    V_u_hat = u_hat2 - (u_hat**2)\n",
    "    \n",
    "    return u_hat, V_u_hat\n",
    "\n",
    "def Estimate_NW_u_hat_conditional_W_cross_sectional_application(theta, n_inputs, n_corr_terms, y, X, P, U_hat, S_kernel):\n",
    "    \n",
    "    n = len(y)\n",
    "\n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:n_inputs+1]\n",
    "    sigma2_v = theta[1+n_inputs]\n",
    "    sigma2_u = theta[2+n_inputs]\n",
    "    sigma2_w = np.exp(theta[3+n_inputs:(3+n_inputs)+(n_inputs-1)])\n",
    "    mu_W = theta[(3+n_inputs)+(n_inputs-2)+1:(3+n_inputs)+(n_inputs-2)+(n_inputs)]\n",
    "\n",
    "    # Observed variables\n",
    "    obs_eps = y - np.log(alpha) - X@beta\n",
    "    W = (np.tile(X[:, 0].reshape(-1, 1), (1, n_inputs-1)) - X[:,1:]) - (P[:,1:] - np.tile(P[:, 0].reshape(-1, 1), (1, n_inputs-1)) +(np.log(beta[0]) - np.log(beta[1:])))\n",
    "    rep_obs_eps = np.repeat(obs_eps.reshape(-1,1), S_kernel, axis=1).T        \n",
    "    rep_obs_W = {}\n",
    "    for i in range(n_inputs-1):\n",
    "        rep_obs_W[i] = np.repeat(W[:, i].reshape(-1,1), S_kernel, axis=1).T        \n",
    "\n",
    "    # Simulated variables\n",
    "    simulated_v = stats.norm.rvs(loc=0, scale=np.sqrt(sigma2_v), size=S_kernel)\n",
    "    simulated_u = np.sqrt(sigma2_u)*stats.norm.ppf((U_hat[:, 0]+1)/2) # Simulated half normal rvs\n",
    "    simulated_W = np.zeros((S_kernel, n_inputs-1))\n",
    "    for i in range(n_inputs-1):\n",
    "        simulated_W[:,i] = stats.norm.ppf(U_hat[:,i+1], mu_W[i], np.sqrt(sigma2_w[i]))\n",
    "    simulated_eps = simulated_v - simulated_u\n",
    "\n",
    "    # Bandwidth information for each conditioning variable\n",
    "    h_eps = 1.06*S_kernel**(-1/5)*(max(np.std(simulated_eps), stats.iqr(simulated_eps)/1.34))\n",
    "    h_W = np.zeros(n_inputs-1)\n",
    "    for i in range(n_inputs-1):\n",
    "        h_W[i] = 1.06*S_kernel**(-1/5)*(max(np.std(simulated_W[:,i]), stats.iqr(simulated_W[:,i])/1.34))\n",
    "    h = np.concatenate([np.array([h_eps]), h_W])\n",
    "\n",
    "    # Kernel estimates for E[u|eps, w1, w2]\n",
    "    # V[u|eps, w1, w2] = E[u^2|w2, w3, eps] - (E[u|w2, w3, eps])^2\n",
    "    kernel_regression_results1 = np.zeros(n)\n",
    "    kernel_regression_results2 = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        eps_kernel = stats.norm.pdf((simulated_eps - rep_obs_eps[:,i])/h[0])\n",
    "        W_kernel = np.zeros((S_kernel,n_inputs-1))\n",
    "        for j in range(n_inputs-1):\n",
    "            W_kernel[:,j] = stats.norm.pdf((simulated_W[:,j] - rep_obs_W[j][:,i])/h[j+1])\n",
    "\n",
    "        W_kernel_prod = np.prod(W_kernel, 1)\n",
    "        kernel_product = eps_kernel*W_kernel_prod\n",
    "        kernel_regression_results1[i] = np.sum(kernel_product*simulated_u)/np.sum(kernel_product)\n",
    "        kernel_regression_results2[i] = np.sum(kernel_product*(simulated_u**2))/np.sum(kernel_product)\n",
    "\n",
    "    u_hat = kernel_regression_results1\n",
    "    V_u_hat = kernel_regression_results2 - (u_hat**2)\n",
    "    \n",
    "    return u_hat, V_u_hat\n",
    "\n",
    "def Loglikelihood_APS14_dynamic_panel_SFA_u_RS2007(theta, y, X, N, T, k, S, FMSLE_us):\n",
    "\n",
    "    if np.any(np.isnan(theta)):\n",
    "        logDen = np.ones((n, 1))*-1e8\n",
    "        logL = -np.sum(logDen)\n",
    "    else:\n",
    "        rhos = theta[7:]\n",
    "        Rho = inverse_mapping_vec(rhos) # Gaussian copula correlation matrix \n",
    "       \n",
    "        alpha = np.exp(theta[0])\n",
    "        beta = 1/(1+np.exp(-theta[1:4])) # Inverse logit transform of betas\n",
    "        delta = theta[4:6]\n",
    "        sigma2_v = theta[6]\n",
    "      \n",
    "        eps = {}\n",
    "        for t in range(T):\n",
    "            eps__ = np.full(N, np.nan)\n",
    "            tmp_eps = y[t] - np.log(alpha) - X[t]@beta\n",
    "            eps__[:len(tmp_eps)] = tmp_eps\n",
    "            eps[t] = eps__\n",
    "            \n",
    "        if not np.all(np.linalg.eigvals(Rho) > 0):\n",
    "            logDen = np.ones((n, 1))*-1e8\n",
    "            logL = -np.sum(logDen)\n",
    "        else:\n",
    "            # Evaluate the integral via simulated MLE (FMSLE)\n",
    "            A = np.linalg.cholesky(Rho)\n",
    "            all_eps = np.concatenate([_eps.reshape(-1,1) for _eps in eps.values()], axis=1)\n",
    "            FMSLE_densities = np.zeros(N)\n",
    "            for i in range(N):\n",
    "                eps_i = all_eps[i, :]\n",
    "                n_NaNs = len(eps_i[np.isnan(eps_i)])\n",
    "                eps_i = eps_i[~np.isnan(eps_i)] # remove any NaN from an unbalanced panel\n",
    "                rep_eps_i = np.tile(eps_i, (S, 1))\n",
    "                simulated_us_i = np.zeros((S, T))\n",
    "                for t in range(T):\n",
    "                   simulated_us_i[:, t] = FMSLE_us[t][:, i]\n",
    "        \n",
    "                # Transform simulated values to half-normal RV's\n",
    "                CDF_u_i = stats.norm.cdf(simulated_us_i@A, np.zeros((S, T)), np.ones((S, T)))\n",
    "                sigma2_u_hat = np.exp(delta[0] + delta[1]*np.arange(1, T+1, 1))\n",
    "                u_i = stats.halfnorm.ppf(CDF_u_i, \n",
    "                                         np.zeros((S, T)), \n",
    "                                         np.tile(np.ones((1, T)) * np.sqrt(sigma2_u_hat), (S, 1)))\n",
    "                \n",
    "                # adjust for possible unbalanced panel\n",
    "                u_i = u_i[:,:T-n_NaNs]\n",
    "                # Joint density\n",
    "                den_i = np.mean(stats.multivariate_normal.pdf(rep_eps_i + u_i, \n",
    "                                                              mean=np.zeros(T-n_NaNs), \n",
    "                                                          cov=np.eye(T-n_NaNs)*sigma2_v)) # eq 1 pg. 510 section 5.1 APS14\n",
    "                if den_i < 1e-10:\n",
    "                    den_i = 1e-10\n",
    "                FMSLE_densities[i] = den_i\n",
    "        \n",
    "            logL = -np.sum(np.log(FMSLE_densities))\n",
    "        \n",
    "    return logL\n",
    "\n",
    "def export_simulation_data_RS2007_electricity_application(theta, n_inputs, y, X, P, U_hat, S_kernel):\n",
    "    \n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:n_inputs+1]\n",
    "    sigma2_v = theta[1+n_inputs]\n",
    "    sigma2_u = theta[2+n_inputs]\n",
    "    sigma2_w = np.exp(theta[3+n_inputs:(3+n_inputs)+(n_inputs-1)])\n",
    "    mu_W = theta[(3+n_inputs)+(n_inputs-2)+1:(3+n_inputs)+(n_inputs-2)+(n_inputs)]\n",
    "\n",
    "    # Observed variables\n",
    "    obs_eps = y - np.log(alpha) - X@beta\n",
    "    W = (np.tile(X[:, 0].reshape(-1, 1), (1, n_inputs-1)) - X[:,1:]) - (P[:,1:] - np.tile(P[:, 0].reshape(-1, 1), (1, n_inputs-1)) +(np.log(beta[0]) - np.log(beta[1:])))\n",
    "    rep_obs_W = {}\n",
    "    for i in range(n_inputs-1):\n",
    "        rep_obs_W[i] = np.repeat(W[:, i].reshape(-1,1), S_kernel, axis=1).T        \n",
    "\n",
    "    # Simulated variables\n",
    "    simulated_v = stats.norm.rvs(loc=0, scale=np.sqrt(sigma2_v), size=S_kernel)\n",
    "    simulated_u = np.sqrt(sigma2_u)*stats.norm.ppf((U_hat[:, 0]+1)/2) # Simulated half normal rvs\n",
    "    simulated_W = np.zeros((S_kernel, n_inputs-1))\n",
    "    for i in range(n_inputs-1):\n",
    "        simulated_W[:,i] = stats.norm.ppf(U_hat[:,i+1], mu_W[i], np.sqrt(sigma2_w[i]))\n",
    "    simulated_eps = simulated_v - simulated_u\n",
    "    \n",
    "    # Export\n",
    "    NN_train_eps_W = pd.DataFrame(np.concatenate([simulated_eps.reshape(-1,1), simulated_W], \n",
    "                                                 axis=1), \n",
    "                                  columns=['train_simulated_eps']+[f'train_simulated_w{i+1}' for i in range(n_inputs-1)])\n",
    "    NN_train_u = pd.DataFrame(simulated_u, \n",
    "                              columns=['train_simulated_u'])\n",
    "    NN_test_eps_W = pd.DataFrame(np.concatenate([obs_eps.reshape(-1,1), W], \n",
    "                                                 axis=1), \n",
    "                                  columns=['test_simulated_eps']+[f'test_simulated_w{i+1}' for i in range(n_inputs-1)])\n",
    "\n",
    "    NN_train_eps_W.to_csv(r'./cross_sectional_SFA_RS2007_electricity_application_NN_train_eps_W.csv', \n",
    "                          index=False)\n",
    "    NN_train_u.to_csv(r'./cross_sectional_SFA_RS2007_electricity_application_NN_train_u.csv', \n",
    "                          index=False)\n",
    "    NN_test_eps_W.to_csv(r'./cross_sectional_SFA_RS2007_electricity_application_NN_test_eps_W.csv', \n",
    "                          index=False)\n",
    "\n",
    "def export_RS2007_electricity_SFA_panel_data(theta, y, X, N, T, U_hat, S_kernel):\n",
    "    \n",
    "    alpha = theta[0]\n",
    "    beta = theta[1:4]\n",
    "    delta = theta[4:6]\n",
    "    sigma2_v = theta[6]\n",
    "  \n",
    "    #Observed variables\n",
    "    obs_eps = {}\n",
    "    for t in range(T):\n",
    "        eps__ = np.full(N, np.nan)\n",
    "        tmp_eps = y[t] - np.log(alpha) - X[t]@beta\n",
    "        eps__[:len(tmp_eps)] = tmp_eps\n",
    "        obs_eps[t] = eps__\n",
    "\n",
    "    obs_eps = np.concatenate([x.reshape(-1, 1) for x in obs_eps.values()], axis=1)\n",
    "    \n",
    "    # Simulated variables\n",
    "    simulated_v = stats.multivariate_normal.rvs(np.zeros(T), np.eye(T)*sigma2_v, S_kernel) # simulate random noise for all T panels \n",
    "    simulated_u = np.zeros((S_kernel, T))\n",
    "    simulated_eps = np.zeros((S_kernel, T))\n",
    "    for t in range(T):\n",
    "        sigma2_u = np.exp(delta[0] + delta[1]*t)\n",
    "        simulated_u[:, t] = np.sqrt(sigma2_u)*stats.norm.ppf((U_hat[:, t]+1)/2) # simulated half normal rvs\n",
    "        simulated_eps[:, t] = simulated_v[:, t] - simulated_u[:, t]\n",
    "\n",
    "    NN_train_eps = pd.DataFrame(simulated_eps, \n",
    "                                columns=[f'train_eps_{t}' for t in range(T)])\n",
    "\n",
    "    NN_train_u = pd.DataFrame(simulated_u, \n",
    "                              columns=[f'train_u_{t}' for t in range(T)])\n",
    "\n",
    "    NN_test_eps = pd.DataFrame(obs_eps, \n",
    "                               columns=[f'test_eps_{t}' for t in range(T)])\n",
    "    \n",
    "    NN_train_eps.to_csv(r'./panel_SFA_RS2007_electricty_application_NN_train_eps.csv', \n",
    "                          index=False)\n",
    "    NN_train_u.to_csv(r'./panel_SFA_RS2007_electricty_application_NN_train_u.csv', \n",
    "                          index=False)\n",
    "    NN_test_eps.to_csv(r'./panel_SFA_RS2007_electricty_application_NN_test_eps.csv', \n",
    "                          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd47e2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Sectional Models (APS16 Estimator)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jleu0010/Documents/GitHub/EPA-Copula-SFA-website/venv/lib/python3.8/site-packages/IPython/core/formatters.py:344: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>JLMS Est.</th>\n",
       "      <th>JLMS Std Dev</th>\n",
       "      <th>NW Est.</th>\n",
       "      <th>NW Std Dev</th>\n",
       "      <th>LLF Est.</th>\n",
       "      <th>LLF Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.0</td>\n",
       "      <td>0.460616</td>\n",
       "      <td>0.113611</td>\n",
       "      <td>0.458124</td>\n",
       "      <td>0.119535</td>\n",
       "      <td>0.459684</td>\n",
       "      <td>0.006971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.0</td>\n",
       "      <td>0.395511</td>\n",
       "      <td>0.110433</td>\n",
       "      <td>0.402088</td>\n",
       "      <td>0.116462</td>\n",
       "      <td>0.399679</td>\n",
       "      <td>0.006790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.0</td>\n",
       "      <td>0.389046</td>\n",
       "      <td>0.110389</td>\n",
       "      <td>0.392217</td>\n",
       "      <td>0.115092</td>\n",
       "      <td>0.395435</td>\n",
       "      <td>0.006410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.0</td>\n",
       "      <td>0.342816</td>\n",
       "      <td>0.107974</td>\n",
       "      <td>0.346136</td>\n",
       "      <td>0.111587</td>\n",
       "      <td>0.348193</td>\n",
       "      <td>0.006073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.376541</td>\n",
       "      <td>0.108117</td>\n",
       "      <td>0.378553</td>\n",
       "      <td>0.113662</td>\n",
       "      <td>0.382351</td>\n",
       "      <td>0.006412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>91.0</td>\n",
       "      <td>0.382511</td>\n",
       "      <td>0.108342</td>\n",
       "      <td>0.386262</td>\n",
       "      <td>0.113862</td>\n",
       "      <td>0.389021</td>\n",
       "      <td>0.006524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>92.0</td>\n",
       "      <td>0.404540</td>\n",
       "      <td>0.107777</td>\n",
       "      <td>0.402222</td>\n",
       "      <td>0.110581</td>\n",
       "      <td>0.408993</td>\n",
       "      <td>0.005997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93.0</td>\n",
       "      <td>0.413886</td>\n",
       "      <td>0.108060</td>\n",
       "      <td>0.409236</td>\n",
       "      <td>0.110864</td>\n",
       "      <td>0.417195</td>\n",
       "      <td>0.006726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>94.0</td>\n",
       "      <td>0.391928</td>\n",
       "      <td>0.105610</td>\n",
       "      <td>0.393589</td>\n",
       "      <td>0.109069</td>\n",
       "      <td>0.400428</td>\n",
       "      <td>0.006062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>95.0</td>\n",
       "      <td>0.406917</td>\n",
       "      <td>0.106064</td>\n",
       "      <td>0.412260</td>\n",
       "      <td>0.108425</td>\n",
       "      <td>0.415814</td>\n",
       "      <td>0.006120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>96.0</td>\n",
       "      <td>0.379603</td>\n",
       "      <td>0.102596</td>\n",
       "      <td>0.380035</td>\n",
       "      <td>0.104425</td>\n",
       "      <td>0.387660</td>\n",
       "      <td>0.007358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>97.0</td>\n",
       "      <td>0.348042</td>\n",
       "      <td>0.099227</td>\n",
       "      <td>0.352486</td>\n",
       "      <td>0.104273</td>\n",
       "      <td>0.356639</td>\n",
       "      <td>0.005932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>98.0</td>\n",
       "      <td>0.357750</td>\n",
       "      <td>0.099781</td>\n",
       "      <td>0.365190</td>\n",
       "      <td>0.102970</td>\n",
       "      <td>0.364057</td>\n",
       "      <td>0.006379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrr}\n",
       "\\toprule\n",
       "{} &  Year &  JLMS Est. &  JLMS Std Dev &   NW Est. &  NW Std Dev &  LLF Est. &  LLF Std Dev \\\\\n",
       "\\midrule\n",
       "0  &  86.0 &   0.460616 &      0.113611 &  0.458124 &    0.119535 &  0.459684 &     0.006971 \\\\\n",
       "1  &  87.0 &   0.395511 &      0.110433 &  0.402088 &    0.116462 &  0.399679 &     0.006790 \\\\\n",
       "2  &  88.0 &   0.389046 &      0.110389 &  0.392217 &    0.115092 &  0.395435 &     0.006410 \\\\\n",
       "3  &  89.0 &   0.342816 &      0.107974 &  0.346136 &    0.111587 &  0.348193 &     0.006073 \\\\\n",
       "4  &  90.0 &   0.376541 &      0.108117 &  0.378553 &    0.113662 &  0.382351 &     0.006412 \\\\\n",
       "5  &  91.0 &   0.382511 &      0.108342 &  0.386262 &    0.113862 &  0.389021 &     0.006524 \\\\\n",
       "6  &  92.0 &   0.404540 &      0.107777 &  0.402222 &    0.110581 &  0.408993 &     0.005997 \\\\\n",
       "7  &  93.0 &   0.413886 &      0.108060 &  0.409236 &    0.110864 &  0.417195 &     0.006726 \\\\\n",
       "8  &  94.0 &   0.391928 &      0.105610 &  0.393589 &    0.109069 &  0.400428 &     0.006062 \\\\\n",
       "9  &  95.0 &   0.406917 &      0.106064 &  0.412260 &    0.108425 &  0.415814 &     0.006120 \\\\\n",
       "10 &  96.0 &   0.379603 &      0.102596 &  0.380035 &    0.104425 &  0.387660 &     0.007358 \\\\\n",
       "11 &  97.0 &   0.348042 &      0.099227 &  0.352486 &    0.104273 &  0.356639 &     0.005932 \\\\\n",
       "12 &  98.0 &   0.357750 &      0.099781 &  0.365190 &    0.102970 &  0.364057 &     0.006379 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "    Year  JLMS Est.  JLMS Std Dev   NW Est.  NW Std Dev  LLF Est.  LLF Std Dev\n",
       "0   86.0   0.460616      0.113611  0.458124    0.119535  0.459684     0.006971\n",
       "1   87.0   0.395511      0.110433  0.402088    0.116462  0.399679     0.006790\n",
       "2   88.0   0.389046      0.110389  0.392217    0.115092  0.395435     0.006410\n",
       "3   89.0   0.342816      0.107974  0.346136    0.111587  0.348193     0.006073\n",
       "4   90.0   0.376541      0.108117  0.378553    0.113662  0.382351     0.006412\n",
       "5   91.0   0.382511      0.108342  0.386262    0.113862  0.389021     0.006524\n",
       "6   92.0   0.404540      0.107777  0.402222    0.110581  0.408993     0.005997\n",
       "7   93.0   0.413886      0.108060  0.409236    0.110864  0.417195     0.006726\n",
       "8   94.0   0.391928      0.105610  0.393589    0.109069  0.400428     0.006062\n",
       "9   95.0   0.406917      0.106064  0.412260    0.108425  0.415814     0.006120\n",
       "10  96.0   0.379603      0.102596  0.380035    0.104425  0.387660     0.007358\n",
       "11  97.0   0.348042      0.099227  0.352486    0.104273  0.356639     0.005932\n",
       "12  98.0   0.357750      0.099781  0.365190    0.102970  0.364057     0.006379"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jleu0010/Documents/GitHub/EPA-Copula-SFA-website/venv/lib/python3.8/site-packages/IPython/core/formatters.py:344: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JLMS Est.</th>\n",
       "      <th>JLMS Std Dev</th>\n",
       "      <th>NW Est.</th>\n",
       "      <th>NW Std Dev</th>\n",
       "      <th>LLF Est.</th>\n",
       "      <th>LLF Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.388515</td>\n",
       "      <td>0.106768</td>\n",
       "      <td>0.390714</td>\n",
       "      <td>0.110831</td>\n",
       "      <td>0.394315</td>\n",
       "      <td>0.006443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrr}\n",
       "\\toprule\n",
       "{} &  JLMS Est. &  JLMS Std Dev &   NW Est. &  NW Std Dev &  LLF Est. &  LLF Std Dev \\\\\n",
       "\\midrule\n",
       "Average &   0.388515 &      0.106768 &  0.390714 &    0.110831 &  0.394315 &     0.006443 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "         JLMS Est.  JLMS Std Dev   NW Est.  NW Std Dev  LLF Est.  LLF Std Dev\n",
       "Average   0.388515      0.106768  0.390714    0.110831  0.394315     0.006443"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_excel('steamelectric.xlsx')\n",
    "\n",
    "data['fuel_price_index'] = np.nan\n",
    "data.iloc[:-1, -1] = data.iloc[:-1, 2].values/data.iloc[1:,2].values\n",
    "data['LM_price_index'] = np.nan\n",
    "data.iloc[:-1, -1] = data.iloc[:-1, 3].values/data.iloc[1:,3].values\n",
    "\n",
    "data.iloc[13:1007:14, :] = np.nan #drop last fuel price index for each plant\n",
    "data = data.dropna()\n",
    "\n",
    "X1 = (data['Fuel Costs ($1000)']/data['fuel_price_index'])*1000*1e-6; #fuel costs over price index\n",
    "X2 = data['Operating Costs ($1000)']/data['LM_price_index']*1000*1e-6; #LM costs over price index\n",
    "X3 = data['Capital ($1000)']/1000; # capital\n",
    "\n",
    "X = np.log(pd.concat([X1, X2, X3], axis=1))\n",
    "P = np.log(data[['fuel_price_index', 'LM_price_index', 'User Cost of Capital']])\n",
    "y = np.log(data['Output (MWhr)']*1e-6) # output ml MWpH \n",
    "\n",
    "# Estimate cross sectional models\n",
    "n = len(y)\n",
    "S = 500 # Number of Halton draws used in maximum simulated likelihood\n",
    "S_kernel = 10000; # Number of simulated draws for evaluation of the conditional expectation\n",
    "n_inputs = 3\n",
    "n_corr_terms = ((n_inputs)^2-(n_inputs))/2 # Number of off diagonal lower triangular correlation/covariance terms for Gaussian copula correlation matrix. \n",
    "\n",
    "initial_lalpha = -2.6\n",
    "initial_beta = np.array([0.5, 0.3, 0.3])\n",
    "initial_lbeta = np.log(initial_beta)\n",
    "initial_logit_beta = np.log(initial_beta/(1-initial_beta))\n",
    "initial_sigma2_v = 0.015\n",
    "initial_lsigma2_v = np.log(initial_sigma2_v)\n",
    "initial_sigma2_u = 0.15\n",
    "initial_lsigma2_u = np.log(initial_sigma2_u)\n",
    "initial_mu_W = np.array([0.5, 0])\n",
    "\n",
    "sampler = stats.qmc.Halton(d=1, \n",
    "                           scramble=True, \n",
    "                           seed=123)\n",
    "sample = sampler.random(n=n)\n",
    "us_ = stats.norm.ppf((sample+1)/2, 0, 1)\n",
    "us_Sxn = np.reshape(np.repeat(us_[:, np.newaxis], S, axis=1), (S, n))\n",
    "\n",
    "# Gaussian Copula\n",
    "initial_Sigma = np.array([[0.2, 0.09], \n",
    "                          [0.09, 0.2]])\n",
    "initial_sigma2_w = np.diag(initial_Sigma)\n",
    "initial_lsigma2_w = np.log(initial_sigma2_w)\n",
    "\n",
    "eps_ = y - initial_lalpha - X@initial_beta\n",
    "W_ = (np.tile(X.iloc[:, 0].values.reshape(-1, 1), (1, n_inputs-1)) - X.iloc[:,1:].values) - (P.iloc[:,1:].values - np.tile(P.iloc[:, 0].values.reshape(-1, 1), (1, n_inputs-1)) +(np.log(initial_beta[0]) - np.log(initial_beta[1:])))\n",
    "initial_Rho = np.corrcoef(np.concatenate([eps_.values.reshape(-1,1), W_], axis=1).T)\n",
    "initial_lRho = direct_mapping_mat(initial_Rho)\n",
    "\n",
    "Gaussian_copula_theta0 = np.concatenate([np.array([initial_lalpha]), initial_lbeta, np.array([initial_sigma2_v, initial_sigma2_u]), \n",
    "                                         initial_lsigma2_w, initial_mu_W, initial_lRho])\n",
    "\n",
    "# Bounds to ensure sigma2v and sigma2u are >= 0\n",
    "bounds = [(None, None) for x in range(4)] + [\n",
    "    (1e-5, np.inf),\n",
    "    (1e-5, np.inf),\n",
    "] + [(None, None) for x in range(7)]\n",
    "\n",
    "# Minimize the negative log-likelihood using numerical optimization.\n",
    "MLE_results = minimize(\n",
    "    fun=Loglikelihood_Gaussian_copula_cross_sectional_application_SFA,\n",
    "    x0=Gaussian_copula_theta0,\n",
    "    method=\"L-BFGS-B\",\n",
    "    tol = 1e-6,\n",
    "    options={\"ftol\": 1e-6, \"maxiter\": 1000, \"maxfun\": 6*1000},\n",
    "    args=(y.values, X.values, P.values, us_Sxn, n_inputs, S),\n",
    "    bounds=bounds,\n",
    ")\n",
    "\n",
    "Gaussian_copula_theta = MLE_results.x\n",
    "logMLE = MLE_results.fun * -1  # Log-likelihood at the solution\n",
    "\n",
    "# Transform parameters\n",
    "Gaussian_copula_theta[:4] = np.exp(Gaussian_copula_theta[:4]) # Transform production system coefficients\n",
    "Gaussian_copula_theta[6:8] = np.exp(Gaussian_copula_theta[6:8])\n",
    "rhos_log_form = Gaussian_copula_theta[-3:]\n",
    "Rho = inverse_mapping_vec(rhos_log_form)\n",
    "Gaussian_copula_theta[-3:] = direct_mapping_mat(initial_Rho)\n",
    "Rho_lower_trianglular = Rho[np.tril_indices(Rho.shape[1], 1)]\n",
    "\n",
    "np.savetxt(r'./cross_sectional_SFA_RS2007_electricity_application_gaussian_copula_correlation_matrix.csv', \n",
    "              Rho, \n",
    "              delimiter=',')\n",
    "\n",
    "# JLMS technical inefficiency scores\n",
    "(Gaussian_copula_JLMS_u_hat, \n",
    " Gaussian_copula_JLMS_V_u_hat) = estimate_Jondrow1982_u_hat(theta=Gaussian_copula_theta, \n",
    "                                                            n_inputs=n_inputs, \n",
    "                           n_corr_terms=n_corr_terms, \n",
    "                           y=y.values, \n",
    "                           X=X.values)\n",
    "JLMS_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_JLMS_u_hat.reshape(-1,1)], axis=1)\n",
    "JLMS_V_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_JLMS_V_u_hat.reshape(-1,1)], axis=1)\n",
    "JLMS_u_hat_year_mean = np.zeros((13, 2))\n",
    "JLMS_V_u_hat_year_mean = np.zeros((13, 2))\n",
    "JLMS_std_u_hat_year_mean = np.zeros((13, 2))\n",
    "for i, t in enumerate(range(86, 99)):\n",
    "    JLMS_u_hat_year_mean[i, 0] = t\n",
    "    JLMS_V_u_hat_year_mean[i, 0] = t\n",
    "    JLMS_std_u_hat_year_mean[i, 0] = t\n",
    "    JLMS_u_hat_year_mean[i, 1] = np.mean(JLMS_u_hat_matrix[JLMS_u_hat_matrix[:,0] == t, 1])\n",
    "    JLMS_V_u_hat_year_mean[i, 1] = np.mean(JLMS_V_u_hat_matrix[JLMS_V_u_hat_matrix[:,0] == t, 1])\n",
    "    JLMS_std_u_hat_year_mean[i, 1:] = np.mean(np.sqrt(JLMS_V_u_hat_matrix[JLMS_V_u_hat_matrix[:,0] == t, 1]))\n",
    "\n",
    "# Copula Nadaraya Watson Scores\n",
    "Gaussian_copula_U_hat = simulate_error_components(Rho=Rho,\n",
    "                                                                      n_inputs=n_inputs, \n",
    "                                                                      S_kernel=S_kernel, \n",
    "                                                                      seed=1234)\n",
    "(Gaussian_copula_NW_conditional_W_u_hat, \n",
    " Gaussian_copula_NW_conditional_W_V_u_hat) = Estimate_NW_u_hat_conditional_W_cross_sectional_application(theta=Gaussian_copula_theta, \n",
    "                                                                                                         n_inputs=n_inputs, \n",
    "                                                                                                         n_corr_terms=n_corr_terms, \n",
    "                                                                                                         y=y.values, \n",
    "                                                                                                         X=X.values, \n",
    "                                                                                                         P=P.values, \n",
    "                                                                                                 U_hat=Gaussian_copula_U_hat, \n",
    "                                                                                                         S_kernel=S_kernel)\n",
    "NW_conditional_W_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_NW_conditional_W_u_hat.reshape(-1,1)], axis=1)\n",
    "NW_conditional_W_V_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_NW_conditional_W_V_u_hat.reshape(-1,1)], axis=1)\n",
    "NW_conditional_W_u_hat_year_mean = np.zeros((13, 2))\n",
    "NW_conditional_W_V_u_hat_year_mean = np.zeros((13, 2))\n",
    "NW_conditional_W_std_u_hat_year_mean = np.zeros((13, 2))\n",
    "for i, t in enumerate(range(86, 99)):\n",
    "    NW_conditional_W_u_hat_year_mean[i, 0] = t\n",
    "    NW_conditional_W_V_u_hat_year_mean[i, 0] = t\n",
    "    NW_conditional_W_std_u_hat_year_mean[i, 0] = t\n",
    "    NW_conditional_W_u_hat_year_mean[i, 1] = np.mean(NW_conditional_W_u_hat_matrix[NW_conditional_W_u_hat_matrix[:,0] == t, 1])\n",
    "    NW_conditional_W_V_u_hat_year_mean[i, 1] = np.mean(NW_conditional_W_V_u_hat_matrix[NW_conditional_W_V_u_hat_matrix[:,0] == t, 1])\n",
    "    NW_conditional_W_std_u_hat_year_mean[i, 1] = np.mean(np.sqrt(NW_conditional_W_V_u_hat_matrix[NW_conditional_W_V_u_hat_matrix[:,0] == t, 1]))\n",
    "\n",
    "# Export simulated training data and compute LLF u hat\n",
    "export_simulation_data_RS2007_electricity_application(theta=Gaussian_copula_theta, \n",
    "                                                      n_inputs=n_inputs, \n",
    "                                                      y=y.values, \n",
    "                                                      X=X.values, \n",
    "                                                      P=P.values, \n",
    "                                                      U_hat=Gaussian_copula_U_hat, \n",
    "                                                      S_kernel=S_kernel)\n",
    "\n",
    "# Users should change the first directory to the path where R is installed - use the code depending on your OS\n",
    "# Windows\n",
    "# subprocess.call([r'C:\\Program Files\\R\\R-4.2.2\\bin\\Rscript.exe ./train_LocalLinear_forest_cross_sectional_RS2007_electricty_application.R'], \n",
    "#                 shell=True)\n",
    "# Mac\n",
    "# subprocess.call([r'/usr/local/bin/Rscript', './train_LocalLinear_forest_cross_sectional_RS2007_electricty_application.R'])\n",
    "Gaussian_copula_LLF_results = pd.read_csv(r'./LLF_Gaussian_copula_u_hat.csv')\n",
    "Gaussian_copula_LLF_conditional_W_u_hat = Gaussian_copula_LLF_results.iloc[:,0].values\n",
    "Gaussian_copula_LLF_conditional_W_V_u_hat = Gaussian_copula_LLF_results.iloc[:,1].values\n",
    "\n",
    "LLF_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_LLF_conditional_W_u_hat.reshape(-1,1)], axis=1)\n",
    "LLF_V_u_hat_matrix = np.concatenate([data.loc[:, ['YEAR']].values, Gaussian_copula_LLF_conditional_W_V_u_hat.reshape(-1,1)], axis=1)\n",
    "LLF_u_hat_year_mean = np.zeros((13, 2))\n",
    "LLF_V_u_hat_year_mean = np.zeros((13, 2))\n",
    "LLF_std_u_hat_year_mean = np.zeros((13, 2))\n",
    "for i, t in enumerate(range(86, 99)):\n",
    "    LLF_u_hat_year_mean[i, 0] = t\n",
    "    LLF_V_u_hat_year_mean[i, 0] = t\n",
    "    LLF_std_u_hat_year_mean[i, 0] = t\n",
    "    LLF_u_hat_year_mean[i, 1] = np.mean(LLF_u_hat_matrix[LLF_u_hat_matrix[:,0] == t, 1])\n",
    "    LLF_V_u_hat_year_mean[i, 1] = np.mean(LLF_V_u_hat_matrix[LLF_V_u_hat_matrix[:,0] == t, 1])\n",
    "    LLF_std_u_hat_year_mean[i, 1] = np.mean(np.sqrt(LLF_V_u_hat_matrix[LLF_V_u_hat_matrix[:,0] == t, 1]))\n",
    "\n",
    "mean_Gaussian_copula_JLMS_u_hat = np.mean(Gaussian_copula_JLMS_u_hat)\n",
    "mean_Gaussian_copula_JLMS_std_u_hat = np.mean(JLMS_std_u_hat_year_mean[:,1])\n",
    "mean_Gaussian_copula_NW_conditional_W_u_hat = np.mean(Gaussian_copula_NW_conditional_W_u_hat)\n",
    "mean_Gaussian_copula_NW_conditional_W_std_u_hat = np.mean(NW_conditional_W_std_u_hat_year_mean[:,1])\n",
    "mean_Gaussian_copula_LLF_conditional_W_u_hat = np.mean(Gaussian_copula_LLF_conditional_W_u_hat);\n",
    "mean_Gaussian_copula_LLF_conditional_W_std_u_hat = np.mean(LLF_std_u_hat_year_mean[:,1])\n",
    "\n",
    "cross_sectional_results_table = pd.DataFrame(np.concatenate([np.array([x for x in range(86, 99)]).reshape(-1,1), \n",
    "                                                JLMS_u_hat_year_mean[:, [1]], JLMS_std_u_hat_year_mean[:, [1]], \n",
    "                                                NW_conditional_W_u_hat_year_mean[:, [1]], NW_conditional_W_std_u_hat_year_mean[:, [1]], \n",
    "                                                LLF_u_hat_year_mean[:, [1]], LLF_std_u_hat_year_mean[:, [1]]], axis=1), \n",
    "                                             columns=['Year', 'JLMS Est.', 'JLMS Std Dev', 'NW Est.', 'NW Std Dev', 'LLF Est.', 'LLF Std Dev'])\n",
    "cross_sectional_averages = pd.DataFrame(np.array([mean_Gaussian_copula_JLMS_u_hat, \n",
    "                                     mean_Gaussian_copula_JLMS_std_u_hat, \n",
    "                                     mean_Gaussian_copula_NW_conditional_W_u_hat, \n",
    "                                     mean_Gaussian_copula_NW_conditional_W_std_u_hat, \n",
    "                                     mean_Gaussian_copula_LLF_conditional_W_u_hat, \n",
    "                                     mean_Gaussian_copula_LLF_conditional_W_std_u_hat]).reshape(1,-1), \n",
    "                                        columns=['JLMS Est.', 'JLMS Std Dev', 'NW Est.', 'NW Std Dev', 'LLF Est.', 'LLF Std Dev'], \n",
    "                                        index=['Average'])\n",
    "print('Cross-Sectional Models (APS16 Estimator)')\n",
    "display(cross_sectional_results_table)\n",
    "display(cross_sectional_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c38a69f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panel Data Models (APS14 Estimator)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jleu0010/Documents/GitHub/EPA-Copula-SFA-website/venv/lib/python3.8/site-packages/IPython/core/formatters.py:344: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>JLMS Est.</th>\n",
       "      <th>JLMS Std Dev</th>\n",
       "      <th>NW Est.</th>\n",
       "      <th>NW Std Dev</th>\n",
       "      <th>LLF Est.</th>\n",
       "      <th>LLF Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.0</td>\n",
       "      <td>0.511809</td>\n",
       "      <td>0.088466</td>\n",
       "      <td>0.414498</td>\n",
       "      <td>0.024621</td>\n",
       "      <td>0.424587</td>\n",
       "      <td>0.004156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.0</td>\n",
       "      <td>0.404085</td>\n",
       "      <td>0.087483</td>\n",
       "      <td>0.426012</td>\n",
       "      <td>0.025372</td>\n",
       "      <td>0.432794</td>\n",
       "      <td>0.004808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.0</td>\n",
       "      <td>0.438372</td>\n",
       "      <td>0.088059</td>\n",
       "      <td>0.423147</td>\n",
       "      <td>0.024784</td>\n",
       "      <td>0.429139</td>\n",
       "      <td>0.003546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.0</td>\n",
       "      <td>0.416660</td>\n",
       "      <td>0.088250</td>\n",
       "      <td>0.428015</td>\n",
       "      <td>0.027402</td>\n",
       "      <td>0.434090</td>\n",
       "      <td>0.004797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.404666</td>\n",
       "      <td>0.086727</td>\n",
       "      <td>0.452374</td>\n",
       "      <td>0.023379</td>\n",
       "      <td>0.460818</td>\n",
       "      <td>0.002694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>91.0</td>\n",
       "      <td>0.409640</td>\n",
       "      <td>0.087197</td>\n",
       "      <td>0.463900</td>\n",
       "      <td>0.021413</td>\n",
       "      <td>0.470895</td>\n",
       "      <td>0.003679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>92.0</td>\n",
       "      <td>0.407005</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>0.372220</td>\n",
       "      <td>0.035573</td>\n",
       "      <td>0.388921</td>\n",
       "      <td>0.007091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93.0</td>\n",
       "      <td>0.412600</td>\n",
       "      <td>0.086621</td>\n",
       "      <td>0.415717</td>\n",
       "      <td>0.036747</td>\n",
       "      <td>0.432685</td>\n",
       "      <td>0.007462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>94.0</td>\n",
       "      <td>0.385891</td>\n",
       "      <td>0.085183</td>\n",
       "      <td>0.389727</td>\n",
       "      <td>0.031838</td>\n",
       "      <td>0.410006</td>\n",
       "      <td>0.003713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>95.0</td>\n",
       "      <td>0.414157</td>\n",
       "      <td>0.086411</td>\n",
       "      <td>0.391118</td>\n",
       "      <td>0.034243</td>\n",
       "      <td>0.401456</td>\n",
       "      <td>0.005797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>96.0</td>\n",
       "      <td>0.374778</td>\n",
       "      <td>0.082006</td>\n",
       "      <td>0.352669</td>\n",
       "      <td>0.034403</td>\n",
       "      <td>0.368819</td>\n",
       "      <td>0.005441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>97.0</td>\n",
       "      <td>0.332448</td>\n",
       "      <td>0.077521</td>\n",
       "      <td>0.307613</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.331670</td>\n",
       "      <td>0.007950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>98.0</td>\n",
       "      <td>0.381271</td>\n",
       "      <td>0.083361</td>\n",
       "      <td>0.390921</td>\n",
       "      <td>0.053746</td>\n",
       "      <td>0.386111</td>\n",
       "      <td>0.004549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrr}\n",
       "\\toprule\n",
       "{} &  Year &  JLMS Est. &  JLMS Std Dev &   NW Est. &  NW Std Dev &  LLF Est. &  LLF Std Dev \\\\\n",
       "\\midrule\n",
       "0  &  86.0 &   0.511809 &      0.088466 &  0.414498 &    0.024621 &  0.424587 &     0.004156 \\\\\n",
       "1  &  87.0 &   0.404085 &      0.087483 &  0.426012 &    0.025372 &  0.432794 &     0.004808 \\\\\n",
       "2  &  88.0 &   0.438372 &      0.088059 &  0.423147 &    0.024784 &  0.429139 &     0.003546 \\\\\n",
       "3  &  89.0 &   0.416660 &      0.088250 &  0.428015 &    0.027402 &  0.434090 &     0.004797 \\\\\n",
       "4  &  90.0 &   0.404666 &      0.086727 &  0.452374 &    0.023379 &  0.460818 &     0.002694 \\\\\n",
       "5  &  91.0 &   0.409640 &      0.087197 &  0.463900 &    0.021413 &  0.470895 &     0.003679 \\\\\n",
       "6  &  92.0 &   0.407005 &      0.086893 &  0.372220 &    0.035573 &  0.388921 &     0.007091 \\\\\n",
       "7  &  93.0 &   0.412600 &      0.086621 &  0.415717 &    0.036747 &  0.432685 &     0.007462 \\\\\n",
       "8  &  94.0 &   0.385891 &      0.085183 &  0.389727 &    0.031838 &  0.410006 &     0.003713 \\\\\n",
       "9  &  95.0 &   0.414157 &      0.086411 &  0.391118 &    0.034243 &  0.401456 &     0.005797 \\\\\n",
       "10 &  96.0 &   0.374778 &      0.082006 &  0.352669 &    0.034403 &  0.368819 &     0.005441 \\\\\n",
       "11 &  97.0 &   0.332448 &      0.077521 &  0.307613 &    0.035156 &  0.331670 &     0.007950 \\\\\n",
       "12 &  98.0 &   0.381271 &      0.083361 &  0.390921 &    0.053746 &  0.386111 &     0.004549 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "    Year  JLMS Est.  JLMS Std Dev   NW Est.  NW Std Dev  LLF Est.  LLF Std Dev\n",
       "0   86.0   0.511809      0.088466  0.414498    0.024621  0.424587     0.004156\n",
       "1   87.0   0.404085      0.087483  0.426012    0.025372  0.432794     0.004808\n",
       "2   88.0   0.438372      0.088059  0.423147    0.024784  0.429139     0.003546\n",
       "3   89.0   0.416660      0.088250  0.428015    0.027402  0.434090     0.004797\n",
       "4   90.0   0.404666      0.086727  0.452374    0.023379  0.460818     0.002694\n",
       "5   91.0   0.409640      0.087197  0.463900    0.021413  0.470895     0.003679\n",
       "6   92.0   0.407005      0.086893  0.372220    0.035573  0.388921     0.007091\n",
       "7   93.0   0.412600      0.086621  0.415717    0.036747  0.432685     0.007462\n",
       "8   94.0   0.385891      0.085183  0.389727    0.031838  0.410006     0.003713\n",
       "9   95.0   0.414157      0.086411  0.391118    0.034243  0.401456     0.005797\n",
       "10  96.0   0.374778      0.082006  0.352669    0.034403  0.368819     0.005441\n",
       "11  97.0   0.332448      0.077521  0.307613    0.035156  0.331670     0.007950\n",
       "12  98.0   0.381271      0.083361  0.390921    0.053746  0.386111     0.004549"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jleu0010/Documents/GitHub/EPA-Copula-SFA-website/venv/lib/python3.8/site-packages/IPython/core/formatters.py:344: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JLMS Est.</th>\n",
       "      <th>JLMS Std Dev</th>\n",
       "      <th>NW Est.</th>\n",
       "      <th>NW Std Dev</th>\n",
       "      <th>LLF Est.</th>\n",
       "      <th>LLF Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.407183</td>\n",
       "      <td>0.085706</td>\n",
       "      <td>0.402148</td>\n",
       "      <td>0.031437</td>\n",
       "      <td>0.41323</td>\n",
       "      <td>0.005053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrr}\n",
       "\\toprule\n",
       "{} &  JLMS Est. &  JLMS Std Dev &   NW Est. &  NW Std Dev &  LLF Est. &  LLF Std Dev \\\\\n",
       "\\midrule\n",
       "Average &   0.407183 &      0.085706 &  0.402148 &    0.031437 &   0.41323 &     0.005053 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "         JLMS Est.  JLMS Std Dev   NW Est.  NW Std Dev  LLF Est.  LLF Std Dev\n",
       "Average   0.407183      0.085706  0.402148    0.031437   0.41323     0.005053"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Estimate Panel data models \n",
    "all_ = pd.concat([data[['Firm No', 'YEAR']], y, X], axis=1)\n",
    "N = 72\n",
    "T = len(all_['YEAR'].unique())\n",
    "panel_y = {}\n",
    "panel_X = {}\n",
    "for j, t in enumerate(range(86, 99)):\n",
    "    y__ = all_.loc[all_['YEAR'] == t, 'Output (MWhr)']\n",
    "    X__ = all_.loc[all_['YEAR'] == t, [0, 1, 'Capital ($1000)']]\n",
    "    panel_y[j] = y__.values\n",
    "    panel_X[j] = X__.values\n",
    "\n",
    "# Independent uniform random variables for FMSLE - assumes a Gaussian copula\n",
    "sampler = stats.qmc.Halton(d=T, \n",
    "                           scramble=True, \n",
    "                           seed=123)\n",
    "us_ = sampler.random(n=S)\n",
    "us_ = stats.norm.ppf(us_) # transform to standard normal\n",
    "FMSLE_us = {}\n",
    "for t in range(T):\n",
    "    us_Sxn = np.tile(us_[:, t-1][np.newaxis, :], (N, 1)).T\n",
    "    FMSLE_us[t] = us_Sxn\n",
    "\n",
    "initial_lalpha = -2.6\n",
    "initial_beta = np.array([0.5, 0.2, 0.2])\n",
    "initial_lbeta = np.log(initial_beta)\n",
    "initial_logit_beta = np.log(initial_beta/(1-initial_beta))\n",
    "initial_sigma2_v = 0.015\n",
    "initial_lsigma2_v = np.log(initial_sigma2_v)\n",
    "initial_sigma2_u = 0.15\n",
    "initial_lsigma2_u = np.log(initial_sigma2_u)\n",
    "\n",
    "initial_delta = np.array([initial_lsigma2_u, 0.2])\n",
    "eps_ = {}\n",
    "for t in range(T):\n",
    "    eps__array = np.zeros(N)\n",
    "    eps__ = panel_y[t] - initial_lalpha- panel_X[t]@initial_beta\n",
    "    eps__array[:len(eps__)] = eps__\n",
    "    eps_[t] = eps__array\n",
    "\n",
    "k = len(initial_beta) + 1\n",
    "initial_Rho = np.corrcoef(np.concatenate([eps.reshape(-1,1) for eps in eps_.values()], axis=1).T)\n",
    "initial_lRho = direct_mapping_mat(initial_Rho)\n",
    "theta0 = np.concatenate([np.array([initial_lalpha]), \n",
    "                         initial_logit_beta, \n",
    "                         initial_delta, \n",
    "                         np.array([initial_sigma2_v]), initial_lRho])\n",
    "          \n",
    "# Bounds to ensure sigma2v and sigma2u are >= 0\n",
    "bounds = [(None, None) for x in range(6)] + [\n",
    "    (1e-5, np.inf),\n",
    "] + [(None, None) for x in range(len(initial_lRho))]\n",
    "\n",
    "# Minimize the negative log-likelihood using numerical optimization.\n",
    "MLE_results = minimize(\n",
    "    fun=Loglikelihood_APS14_dynamic_panel_SFA_u_RS2007,\n",
    "    x0=theta0,\n",
    "    method=\"L-BFGS-B\",\n",
    "    tol = 1e-8,\n",
    "    options={\"ftol\": 1e-8, \"maxiter\": 1000, \"maxfun\": 35000, \"maxcor\": 500},\n",
    "    args=(panel_y, panel_X, N, T, k, S, FMSLE_us),\n",
    "    bounds=bounds,\n",
    ")\n",
    "\n",
    "APS14_theta = MLE_results.x\n",
    "APS14_theta[0] = np.exp(APS14_theta[0])\n",
    "APS14_theta[1:4] = 1/(1+np.exp(-APS14_theta[1:4])) # inverse logit transform of betas\n",
    "APS14_logMLE = MLE_results.fun * -1\n",
    "\n",
    "APS14_Rho = inverse_mapping_vec(APS14_theta[7:])\n",
    "np.savetxt(r'./panel_SFA_RS2007_electricty_application_gaussian_copula_correlation_matrix.csv', \n",
    "              APS14_Rho, \n",
    "              delimiter=',')\n",
    "\n",
    "# Simulated dependent U based upon estimated copula parameters\n",
    "APS14_U_hat = simulate_error_components(Rho=APS14_Rho, \n",
    "                                        n_inputs=T, \n",
    "                                        S_kernel=S_kernel, \n",
    "                                        seed=10)\n",
    "\n",
    "# JLMS scores\n",
    "(APS14_JLMS_u_hat, \n",
    " APS14_JLMS_V_u_hat) = Estimate_Jondrow1982_u_hat_panel_SFA_application_RS2007(alpha=APS14_theta[0], \n",
    "                                                                               beta=APS14_theta[1:4], \n",
    "                                                                               delta=APS14_theta[4:6], \n",
    "                                                                               sigma2_v=APS14_theta[6], \n",
    "                                                                               y=panel_y, \n",
    "                                                                               X=panel_X, \n",
    "                                                                               T=T, \n",
    "                                                                               N=N)\n",
    "APS14_JLMS_u_hat_year_mean = np.nanmean(APS14_JLMS_u_hat, axis=0)\n",
    "APS14_JLMS_V_u_hat_year_mean = np.nanmean(APS14_JLMS_V_u_hat, axis=0)\n",
    "APS14_JLMS_std_u_hat_year_mean = np.nanmean(np.sqrt(APS14_JLMS_V_u_hat), axis=0)\n",
    "\n",
    "# Copula Nadaraya Watson\n",
    "(APS14_NW_u_hat_conditional_eps, \n",
    " APS14_NW_V_u_hat_conditional_eps) = Estimate_NW_u_hat_conditional_eps_panel_SFA_RS2007(theta=APS14_theta, \n",
    "                                                                                        y=panel_y, \n",
    "                                                                                        X=panel_X, \n",
    "                                                                                        N=N, \n",
    "                                                                                        T=T, \n",
    "                                                                                        k=k, \n",
    "                                                                                        U_hat=APS14_U_hat, \n",
    "                                                                                        S_kernel=S_kernel)\n",
    "APS14_NW_u_hat_conditional_eps_year_mean = np.nanmean(APS14_NW_u_hat_conditional_eps, axis=0)\n",
    "APS14_NW_V_u_hat_conditional_eps_year_mean = np.nanmean(APS14_NW_V_u_hat_conditional_eps, axis=0)\n",
    "APS14_NW_std_u_hat_conditional_eps_year_mean = np.nanmean(np.sqrt(APS14_NW_V_u_hat_conditional_eps), axis=0)\n",
    "\n",
    "export_RS2007_electricity_SFA_panel_data(theta=APS14_theta, \n",
    "                                         y=panel_y, \n",
    "                                         X=panel_X, \n",
    "                                         N=N, \n",
    "                                         T=T, \n",
    "                                         U_hat=APS14_U_hat, \n",
    "                                         S_kernel=S_kernel)\n",
    "\n",
    "# Users should change the first directory to the path where R is installed\n",
    "    # Windows\n",
    "# subprocess.call(r'C:\\Program Files\\R\\R-4.2.2\\bin\\Rscript.exe ./train_LocalLinear_forest_panel_RS2007_electricty_application.R', \n",
    "#                 shell=True)\n",
    "    # Mac\n",
    "# subprocess.call([r'/usr/local/bin/Rscript', './train_LocalLinear_forest_panel_RS2007_electricty_application.R'])\n",
    "APS14_LLF_conditional_eps_u_hat = pd.read_csv(r'./RS2007_electricity_LLF_Gaussian_copula_u_hat.csv')\n",
    "APS14_LLF_conditional_eps_V_u_hat = pd.read_csv(r'./RS2007_electricity_LLF_Gaussian_copula_V_u_hat.csv')\n",
    "APS14_LLF_conditional_eps_u_hat = APS14_LLF_conditional_eps_u_hat.iloc[:-1, :]\n",
    "APS14_LLF_conditional_eps_V_u_hat = APS14_LLF_conditional_eps_V_u_hat.iloc[:-1, :]\n",
    "APS14_LLF_conditional_eps_u_hat_year_mean = np.nanmean(APS14_LLF_conditional_eps_u_hat, axis=0)\n",
    "APS14_LLF_conditional_eps_V_u_hat_year_mean = np.nanmean(APS14_LLF_conditional_eps_V_u_hat, axis=0)\n",
    "APS14_LLF_conditional_eps_std_u_hat_year_mean = np.nanmean(np.sqrt(APS14_LLF_conditional_eps_V_u_hat), axis=0)\n",
    "\n",
    "mean_Gaussian_copula_JLMS_u_hat = np.mean(APS14_JLMS_u_hat_year_mean)\n",
    "mean_Gaussian_copula_JLMS_std_u_hat = np.mean(APS14_JLMS_std_u_hat_year_mean)\n",
    "mean_Gaussian_copula_NW_conditional_W_u_hat = np.mean(APS14_NW_u_hat_conditional_eps_year_mean)\n",
    "mean_Gaussian_copula_NW_conditional_W_std_u_hat = np.mean(APS14_NW_std_u_hat_conditional_eps_year_mean)\n",
    "mean_Gaussian_copula_LLF_conditional_W_u_hat = np.mean(APS14_LLF_conditional_eps_u_hat_year_mean)\n",
    "mean_Gaussian_copula_LLF_conditional_W_std_u_hat = np.mean(APS14_LLF_conditional_eps_std_u_hat_year_mean)\n",
    "\n",
    "panel_results_table = pd.DataFrame(np.concatenate([np.array([x for x in range(86, 99)]).reshape(-1,1), \n",
    "                                                APS14_JLMS_u_hat_year_mean.reshape(-1,1), APS14_JLMS_std_u_hat_year_mean.reshape(-1,1), \n",
    "                                                APS14_NW_u_hat_conditional_eps_year_mean.reshape(-1,1), APS14_NW_std_u_hat_conditional_eps_year_mean.reshape(-1,1), \n",
    "                                                APS14_LLF_conditional_eps_u_hat_year_mean.reshape(-1,1), APS14_LLF_conditional_eps_std_u_hat_year_mean.reshape(-1,1)], axis=1), \n",
    "                                             columns=['Year', 'JLMS Est.', 'JLMS Std Dev', 'NW Est.', 'NW Std Dev', 'LLF Est.', 'LLF Std Dev'])\n",
    "panel_averages = pd.DataFrame(np.array([mean_Gaussian_copula_JLMS_u_hat, \n",
    "                                     mean_Gaussian_copula_JLMS_std_u_hat, \n",
    "                                     mean_Gaussian_copula_NW_conditional_W_u_hat, \n",
    "                                     mean_Gaussian_copula_NW_conditional_W_std_u_hat, \n",
    "                                     mean_Gaussian_copula_LLF_conditional_W_u_hat, \n",
    "                                     mean_Gaussian_copula_LLF_conditional_W_std_u_hat]).reshape(1,-1), \n",
    "                                        columns=['JLMS Est.', 'JLMS Std Dev', 'NW Est.', 'NW Std Dev', 'LLF Est.', 'LLF Std Dev'], \n",
    "                                        index=['Average'])\n",
    "print('Panel Data Models (APS14 Estimator)')\n",
    "display(panel_results_table)\n",
    "display(panel_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e850641b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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",
      "text/plain": [
       "<Figure size 1650x1050 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/pdf": "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",
      "text/plain": [
       "<Figure size 1650x1050 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# density plots\n",
    "kde = stats.gaussian_kde(Gaussian_copula_JLMS_u_hat)\n",
    "Gaussian_copula_JLMS_TE_kernel_x = np.linspace(Gaussian_copula_JLMS_u_hat.min(), Gaussian_copula_JLMS_u_hat.max(), 100)\n",
    "Gaussian_copula_JLMS_TE_kernel = kde(Gaussian_copula_JLMS_TE_kernel_x)\n",
    "\n",
    "kde = stats.gaussian_kde(Gaussian_copula_NW_conditional_W_u_hat)\n",
    "Gaussian_copula_NW_TE_kernel_x = np.linspace(Gaussian_copula_JLMS_u_hat.min(), Gaussian_copula_JLMS_u_hat.max(), 100)\n",
    "Gaussian_copula_NW_TE_kernel = kde(Gaussian_copula_NW_TE_kernel_x)\n",
    "\n",
    "kde = stats.gaussian_kde(Gaussian_copula_LLF_conditional_W_u_hat)\n",
    "Gaussian_copula_LLF_TE_kernel_x = np.linspace(Gaussian_copula_JLMS_u_hat.min(), Gaussian_copula_JLMS_u_hat.max(), 100)\n",
    "Gaussian_copula_LLF_TE_kernel = kde(Gaussian_copula_LLF_TE_kernel_x)\n",
    "\n",
    "figure = plt.Figure(figsize=(10,10))\n",
    "plt.plot(Gaussian_copula_JLMS_TE_kernel_x, Gaussian_copula_JLMS_TE_kernel, \n",
    "         color='k', \n",
    "         ls='-', \n",
    "         label=r'JLMS $E[u_{i}|\\varepsilon_{i}]$')\n",
    "plt.plot(Gaussian_copula_NW_TE_kernel_x, \n",
    "         Gaussian_copula_NW_TE_kernel, \n",
    "         ls=':', \n",
    "         color='k', \n",
    "         label=r'NW $E[u_{i}|\\varepsilon_{i}, \\omega_{i2}, \\omega_{i3}]$')\n",
    "plt.plot(Gaussian_copula_LLF_TE_kernel_x, \n",
    "         Gaussian_copula_LLF_TE_kernel, \n",
    "         ls='-.', \n",
    "         color='k', \n",
    "         label=r'LLF $E[u_{i}|\\varepsilon_{i}, \\omega_{i2}, \\omega_{i3}]$')\n",
    "plt.xlabel(r'$u_{i}$', fontsize=14)\n",
    "plt.ylabel('Kernel Density', fontsize=14)\n",
    "plt.title('JLMS & APS16 Estimator', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "APS14_JLMS_u_hat_flat = APS14_JLMS_u_hat.flatten()\n",
    "APS14_JLMS_u_hat_flat = APS14_JLMS_u_hat_flat[~np.isnan(APS14_JLMS_u_hat_flat)]\n",
    "APS14_NW_u_hat_conditional_eps_flat = APS14_NW_u_hat_conditional_eps.flatten()\n",
    "APS14_NW_u_hat_conditional_eps_flat = APS14_NW_u_hat_conditional_eps_flat[~np.isnan(APS14_NW_u_hat_conditional_eps_flat)]\n",
    "APS14_LLF_conditional_eps_u_hat_flat = APS14_LLF_conditional_eps_u_hat.values.flatten()\n",
    "APS14_LLF_conditional_eps_u_hat_flat = APS14_LLF_conditional_eps_u_hat_flat[~np.isnan(APS14_LLF_conditional_eps_u_hat_flat)]\n",
    "\n",
    "kde = stats.gaussian_kde(APS14_JLMS_u_hat_flat)\n",
    "APS14_JLMS_TE_kernel_x = np.linspace(APS14_JLMS_u_hat_flat.min(), \n",
    "                                     APS14_JLMS_u_hat_flat.max(), 100)\n",
    "APS14_JLMS_TE_kernel = kde(APS14_JLMS_TE_kernel_x)\n",
    "\n",
    "kde = stats.gaussian_kde(APS14_NW_u_hat_conditional_eps_flat)\n",
    "APS14_NW_TE_kernel_x = np.linspace(APS14_NW_u_hat_conditional_eps_flat.min(), \n",
    "                                   APS14_NW_u_hat_conditional_eps_flat.max(), 100)\n",
    "APS14_NW_TE_kernel = kde(APS14_NW_TE_kernel_x)\n",
    "\n",
    "kde = stats.gaussian_kde(APS14_LLF_conditional_eps_u_hat_flat)\n",
    "APS14_LLF_TE_kernel_x = np.linspace(APS14_LLF_conditional_eps_u_hat_flat.min(), \n",
    "                                    APS14_LLF_conditional_eps_u_hat_flat.max(), 100)\n",
    "APS14_LLF_TE_kernel = kde(APS14_LLF_TE_kernel_x)\n",
    "\n",
    "figure = plt.Figure(figsize=(10,10))\n",
    "plt.plot(APS14_JLMS_TE_kernel_x, APS14_JLMS_TE_kernel, \n",
    "         color='k', \n",
    "         ls='-', \n",
    "         label=r'JLMS $E[u_{it}|\\varepsilon_{it}]$')\n",
    "plt.plot(APS14_NW_TE_kernel_x, \n",
    "         APS14_NW_TE_kernel, \n",
    "         ls=':', \n",
    "         color='k', \n",
    "         label=r'NW $E[u_{it}|\\varepsilon_{i1}, \\dots, \\varepsilon_{i,13}]$')\n",
    "plt.plot(APS14_LLF_TE_kernel_x, \n",
    "         APS14_LLF_TE_kernel, \n",
    "         ls='-.', \n",
    "         color='k', \n",
    "         label=r'LLF $E[u_{it}|\\varepsilon_{i1}, \\dots, \\varepsilon_{i,13}]$')\n",
    "plt.xlabel(r'$u_{i}$', fontsize=14)\n",
    "plt.ylabel('Kernel Density', fontsize=14)\n",
    "plt.title('JLMS & APS14 Estimator', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "251c3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all previous function and variable definitions before the next exercise\n",
    "%reset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
